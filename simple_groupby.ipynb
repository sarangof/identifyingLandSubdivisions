{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_NAME = 'sara'\n",
    "\n",
    "AWS_PROFILE = 'cities'\n",
    "\n",
    "'''\n",
    "# List of cities to process\n",
    "cities = [\"Belo Horizonte\", \"Campinas\"]#, \"Bogota\", \"Nairobi\", \"Bamako\", \n",
    "        #\"Lagos\", \"Accra\", \"Abidjan\", \"Mogadishu\", \"Cape Town\", \n",
    "        #\"Maputo\", \"Luanda\"]\n",
    "\n",
    "test_cities = [\"Belo Horizonte\"]\n",
    "#cities = test_cities\n",
    "\n",
    "cities = [city.replace(' ', '_') for city in cities]\n",
    "\n",
    "search_buffer_files = fs.ls(SEARCH_BUFFER_PATH)\n",
    "\n",
    "cities \n",
    "\n",
    "number_of_cities = len(cities)\n",
    "\n",
    "print(f'City count: {number_of_cities}')\n",
    "'''\n",
    "grid_size = 200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = \"s3://wri-cities-sandbox/identifyingLandSubdivisions/data\"\n",
    "INPUT_PATH = f'{MAIN_PATH}/input'\n",
    "CITY_INFO_PATH = f'{INPUT_PATH}/city_info'\n",
    "EXTENTS_PATH = f'{CITY_INFO_PATH}/extents'\n",
    "BUILDINGS_PATH = f'{INPUT_PATH}/buildings'\n",
    "ROADS_PATH = f'{INPUT_PATH}/roads'\n",
    "INTERSECTIONS_PATH = f'{INPUT_PATH}/intersections'\n",
    "GRIDS_PATH = f'{INPUT_PATH}/city_info/grids'\n",
    "SEARCH_BUFFER_PATH = f'{INPUT_PATH}/city_info/search_buffers'\n",
    "OUTPUT_PATH = f'{MAIN_PATH}/output'\n",
    "OUTPUT_PATH_CSV = f'{OUTPUT_PATH}/csv'\n",
    "OUTPUT_PATH_RASTER = f'{OUTPUT_PATH}/raster'\n",
    "OUTPUT_PATH_PNG = f'{OUTPUT_PATH}/png'\n",
    "OUTPUT_PATH_RAW = f'{OUTPUT_PATH}/raw_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'MYSFZND4GEX547S0',\n",
       "  'HostId': 'GbTWpi/hS2/dol8CkwkvIB7qcSE7uqij4KYCx9A54fwctE9tluy82dbndBiYgkHKCogILTQVdCQ=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'GbTWpi/hS2/dol8CkwkvIB7qcSE7uqij4KYCx9A54fwctE9tluy82dbndBiYgkHKCogILTQVdCQ=',\n",
       "   'x-amz-request-id': 'MYSFZND4GEX547S0',\n",
       "   'date': 'Tue, 18 Mar 2025 19:18:12 GMT',\n",
       "   'content-type': 'application/xml',\n",
       "   'transfer-encoding': 'chunked',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'Buckets': [{'Name': 'aft-sandbox-540362055257',\n",
       "   'CreationDate': datetime.datetime(2022, 9, 13, 15, 12, 20, tzinfo=tzutc())},\n",
       "  {'Name': 'amplify-citiesindicatorsapi-dev-10508-deployment',\n",
       "   'CreationDate': datetime.datetime(2023, 8, 30, 5, 5, 13, tzinfo=tzutc())},\n",
       "  {'Name': 'cities-dev-sandbox',\n",
       "   'CreationDate': datetime.datetime(2025, 2, 7, 23, 18, 12, tzinfo=tzutc())},\n",
       "  {'Name': 'cities-heat',\n",
       "   'CreationDate': datetime.datetime(2023, 6, 1, 13, 22, 1, tzinfo=tzutc())},\n",
       "  {'Name': 'era5-brazil',\n",
       "   'CreationDate': datetime.datetime(2025, 2, 15, 19, 51, 14, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-athena-us-west-2',\n",
       "   'CreationDate': datetime.datetime(2024, 1, 12, 18, 45, 11, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-climate-hazards',\n",
       "   'CreationDate': datetime.datetime(2024, 1, 3, 16, 57, 31, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-data-api',\n",
       "   'CreationDate': datetime.datetime(2024, 7, 16, 8, 53, 31, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-heat',\n",
       "   'CreationDate': datetime.datetime(2024, 3, 25, 15, 46, 55, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-indicators',\n",
       "   'CreationDate': datetime.datetime(2024, 5, 13, 15, 50, 58, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-sandbox',\n",
       "   'CreationDate': datetime.datetime(2024, 7, 27, 0, 51, 38, tzinfo=tzutc())}],\n",
       " 'Owner': {'DisplayName': 'aws-cities',\n",
       "  'ID': 'df12253943982d72f60594f06c2cacf9a1ee3a9e738c1649c9fb96e5127f1a5c'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check s3 connection using AWS_PROFILE=CitiesUserPermissionSet profile \n",
    "import boto3\n",
    "\n",
    "session = boto3.Session(profile_name=AWS_PROFILE)\n",
    "s3 = session.client('s3')\n",
    "\n",
    "# export CitiesUserPermissionSet profile to use in the next cells\n",
    "import os\n",
    "os.environ['AWS_PROFILE'] = AWS_PROFILE\n",
    "\n",
    "\n",
    "s3.list_buckets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-18 14:18:14,715][INFO    ][coiled] Fetching latest package priorities...\n",
      "[2025-03-18 14:18:14,716][INFO    ][coiled.package_sync] Resolving your local subdivisions2 Python environment...\n",
      "[2025-03-18 14:18:15,627][INFO    ][coiled.package_sync] Scanning 442 conda packages...\n",
      "[2025-03-18 14:18:15,635][INFO    ][coiled.package_sync] Scanning 273 python packages...\n",
      "[2025-03-18 14:18:16,585][INFO    ][coiled] Running pip check...\n",
      "[2025-03-18 14:18:18,030][INFO    ][coiled] Validating environment...\n",
      "[2025-03-18 14:18:20,544][INFO    ][coiled] Creating wheel for ~/Documents/Identifying Land Subdivisions/identifyingLandSubdivisions...\n",
      "[2025-03-18 14:18:20,779][WARNING ][coiled.package_sync] Package - libopenvino-intel-cpu-plugin, libopenvino-intel-cpu-plugin~=2025.0.0 has no install candidate for Python 3.12 linux-aarch64 on conda-forge\n",
      "[2025-03-18 14:18:20,781][INFO    ][coiled] Uploading coiled_local_identifyingLandSubdivisions...\n",
      "[2025-03-18 14:18:21,787][INFO    ][coiled] Requesting package sync build...\n",
      "[2025-03-18 14:18:22,722][INFO    ][coiled] Creating Cluster (name: ils-sara, https://cloud.coiled.io/clusters/800720?account=wri-cities-data ). This usually takes 1-2 minutes...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started a new Dask client on Coiled. Dashboard is available at https://cluster-qacba.dask.host/uwciyM-f5-ASkirr/status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in callback None()\n",
      "handle: <Handle cancelled>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 1367, in _do_ssl_handshake\n",
      "    self.socket.do_handshake()\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/ssl.py\", line 1319, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "ssl.SSLEOFError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1010)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 202, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 691, in _handle_events\n",
      "    self._handle_read()\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 1427, in _handle_read\n",
      "    self._do_ssl_handshake()\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 1376, in _do_ssl_handshake\n",
      "    return self.close(exc_info=err)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 606, in close\n",
      "    self._signal_closed()\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 636, in _signal_closed\n",
      "    self._ssl_connect_future.exception()\n",
      "asyncio.exceptions.CancelledError\n",
      "Exception in callback None()\n",
      "handle: <Handle cancelled>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 1367, in _do_ssl_handshake\n",
      "    self.socket.do_handshake()\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/ssl.py\", line 1319, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "ssl.SSLEOFError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1010)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 202, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 691, in _handle_events\n",
      "    self._handle_read()\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 1427, in _handle_read\n",
      "    self._do_ssl_handshake()\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 1376, in _do_ssl_handshake\n",
      "    return self.close(exc_info=err)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 606, in close\n",
      "    self._signal_closed()\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 636, in _signal_closed\n",
      "    self._ssl_connect_future.exception()\n",
      "asyncio.exceptions.CancelledError\n",
      "Exception in callback None()\n",
      "handle: <Handle cancelled>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 1367, in _do_ssl_handshake\n",
      "    self.socket.do_handshake()\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/ssl.py\", line 1319, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "ssl.SSLEOFError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1010)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 202, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 691, in _handle_events\n",
      "    self._handle_read()\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 1427, in _handle_read\n",
      "    self._do_ssl_handshake()\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 1376, in _do_ssl_handshake\n",
      "    return self.close(exc_info=err)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 606, in close\n",
      "    self._signal_closed()\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 636, in _signal_closed\n",
      "    self._ssl_connect_future.exception()\n",
      "asyncio.exceptions.CancelledError\n",
      "2025-03-18 14:57:01,430 - distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client\n"
     ]
    }
   ],
   "source": [
    "import coiled\n",
    "\n",
    "cluster = coiled.Cluster(\n",
    "    workspace=\"wri-cities-data\",\n",
    "    name=f'ils-{YOUR_NAME}',\n",
    "    region=\"us-west-2\",\n",
    "    arm=True,\n",
    "    worker_vm_types=\"r8g.xlarge\",\n",
    "    spot_policy=\"spot\",\n",
    "    n_workers=8,\n",
    "    package_sync_ignore=[\"pyspark\", \"pypandoc\"]\n",
    ")\n",
    "client = cluster.get_client()\n",
    "\n",
    "print(f\"Started a new Dask client on Coiled. Dashboard is available at {client.dashboard_link}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import s3fs\n",
    "import fsspec\n",
    "import traceback\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "fs = s3fs.S3FileSystem(anon=False)\n",
    "search_buffer_files = fs.ls(SEARCH_BUFFER_PATH)\n",
    "\n",
    "cities = [x.split('/')[-1] for x in search_buffer_files]\n",
    "len(cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_geopandas as dgpd\n",
    "from dask import delayed, compute, visualize\n",
    "from dask.diagnostics import ProgressBar\n",
    "%autoreload\n",
    "from citywide_calculation import get_utm_crs\n",
    "from metrics_calculation import calculate_minimum_distance_to_roads_option_B\n",
    "from shapely.geometry import MultiLineString, LineString\n",
    "\n",
    "@delayed\n",
    "def get_epsg(city_name):\n",
    "    search_buffer = f'{SEARCH_BUFFER_PATH}/{city_name}/{city_name}_search_buffer.geoparquet'\n",
    "    extent = dgpd.read_parquet(search_buffer)\n",
    "    geometry = extent.geometry[0].compute()\n",
    "    epsg = get_utm_crs(geometry)\n",
    "    print(f'{city_name} EPSG: {epsg}')\n",
    "    return epsg\n",
    "\n",
    "@delayed\n",
    "def load_dataset(path, epsg=None):\n",
    "    \"\"\"Load a single parquet dataset\"\"\"\n",
    "    dataset = dgpd.read_parquet(path, npartitions=2)\n",
    "    if epsg:\n",
    "        dataset = dataset.set_crs(\"EPSG:4326\", allow_override=True) \n",
    "        dataset = dataset.to_crs(epsg=epsg)\n",
    "    return dataset\n",
    "\n",
    "@delayed\n",
    "def row_count(dgdf):\n",
    "    \"\"\"Count the rows in a dataframe\"\"\"\n",
    "    row_count = dgdf.map_partitions(len).compute().sum()\n",
    "\n",
    "    return row_count\n",
    "\n",
    "\n",
    "def test_math(input):\n",
    "    return input + input\n",
    "\n",
    "%autoreload\n",
    "from metrics_groupby import metrics\n",
    "\n",
    "@delayed\n",
    "def metrics(city_name,YOUR_NAME,grid_size):\n",
    "    grid_cell_count = 0\n",
    "    paths = {\n",
    "        'grid': f'{GRIDS_PATH}/{city_name}/{city_name}_{str(grid_size)}m_grid.geoparquet',\n",
    "        'buildings': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}.geoparquet',\n",
    "        'roads': f'{ROADS_PATH}/{city_name}/{city_name}_OSM_roads.geoparquet',\n",
    "        'intersections': f'{INTERSECTIONS_PATH}/{city_name}/{city_name}_OSM_intersections.geoparquet'\n",
    "    }\n",
    "    # Get EPSG\n",
    "    epsg = get_epsg(city_name)\n",
    "    # Load grid\n",
    "    grid = load_dataset(paths['grid'], epsg=epsg).compute()\n",
    "    if 'geom' in grid.columns:\n",
    "        grid = grid.drop(columns=['geom'])\n",
    "    grid['cell_area'] = grid.geometry.area\n",
    "\n",
    "    cells = grid.index.size\n",
    "    grid_cell_count += cells.compute()\n",
    "\n",
    "    # Load buildings and perform relevant calculations on it\n",
    "    buildings = load_dataset(paths['buildings'], epsg=epsg).compute()\n",
    "    buildings['area'] = buildings.geometry.area\n",
    "    joined_buildings = dgpd.sjoin(buildings, grid, predicate='within')  \n",
    "    counts_buildings = joined_buildings.groupby('index_right').size()\n",
    "    grid['n_buildings'] = grid.index.map(counts_buildings).fillna(0).astype(int)\n",
    "    built_area_buildings = joined_buildings.groupby('index_right')['area'].sum()\n",
    "    grid['built_area'] = grid.index.map(built_area_buildings).fillna(0).astype(float)\n",
    "\n",
    "    #total_buildings = row_count(buildings).compute()\n",
    "    #print(total_buildings)\n",
    "    # Load roads\n",
    "    roads = load_dataset(paths['roads'], epsg=epsg).compute()\n",
    "    #road_union = roads.unary_union.compute()\n",
    "    # Load intersections\n",
    "    intersections = load_dataset(paths['intersections'], epsg=epsg).compute()\n",
    "\n",
    "\n",
    "    print(type(intersections))\n",
    "    intersections_3plus = intersections[intersections.street_count >= 3]\n",
    "    print(type(intersections_3plus))\n",
    "    intersections_4way = intersections[intersections.street_count == 4]\n",
    "    print(type(intersections_4way))\n",
    "\n",
    "    grid['cell_area_km2'] = grid['cell_area']/1000000.\n",
    "    roads_grid_joined = dgpd.sjoin(roads, grid, predicate='within')\n",
    "    road_length_km = roads_grid_joined.groupby('index_right')['length'].sum()/1000.\n",
    "    grid['road_length'] = grid.index.map(road_length_km).fillna(0).astype(float)\n",
    "\n",
    "    \n",
    "\n",
    "    '''\n",
    "    buildings['distance_to_road'] = buildings['geometry'].map_partitions(\n",
    "        lambda partition: partition.apply(lambda geom: calculate_minimum_distance_to_roads_option_B(geom, road_union)),\n",
    "        meta=('distance_to_road', 'float64')\n",
    "    )\n",
    "    \n",
    "    buildings_within_10m_of_buildings = buildings[buildings.distance_to_road <= 20]\n",
    "    joined_buildings_within_10m_of_buildings = dgpd.sjoin(buildings_within_10m_of_buildings, grid, predicate='within')\n",
    "    counts_buildings_within_10m_of_buildings = joined_buildings_within_10m_of_buildings.groupby('index_right').size()\n",
    "    grid['n_buildings_within_10m_of_roads'] = grid.index.map(counts_buildings_within_10m_of_buildings).fillna(0).astype(int)\n",
    "    joined_buildings_distance_to_road = dgpd.sjoin(buildings[['geometry', 'distance_to_road']], grid, predicate='within')\n",
    "    # NOT SURE OF THIS YET\n",
    "    averaged_buildings_distance_to_road = joined_buildings_distance_to_road.groupby('index_right')['distance_to_road'].mean()\n",
    "    grid['average_distance_to_closest_roads'] = grid.index.map(averaged_buildings_distance_to_road).fillna(0).astype(int)\n",
    "    '''\n",
    "    joined_intersections_3plus = dgpd.sjoin(intersections_3plus, grid, predicate='within')\n",
    "    counts_intersections_3plus = joined_intersections_3plus.groupby('index_right').size()\n",
    "    grid['intersections_3plus'] = grid.index.map(counts_intersections_3plus).fillna(0).astype(int)\n",
    "\n",
    "    joined_intersections_4way = dgpd.sjoin(intersections_4way, grid, predicate='within')\n",
    "    counts_intersections_4way = joined_intersections_4way.groupby('index_right').size()\n",
    "    grid['intersections_4way'] = grid.index.map(counts_intersections_4way).fillna(0).astype(int)\n",
    "    \n",
    "    '''\n",
    "    grid['m1'] = grid['n_buildings_within_10m_of_roads'] / grid['n_buildings']\n",
    "    grid['m2'] = grid['average_distance_to_closest_roads']\n",
    "    '''\n",
    "\n",
    "    grid['m3'] = grid['road_length']/grid['cell_area_km2']\n",
    "    grid['m4'] = grid['intersections_4way'] / grid['intersections_3plus']\n",
    "\n",
    "    #grid['m5'] =  (1000.**2)*(grid['intersections_4way']/grid['cell_area']) #make sure this is equivalent to the meter calculation\n",
    "\n",
    "\n",
    "    grid['m11'] = 1.0*grid['n_buildings'] / grid['cell_area'] # Building density\n",
    "    grid['m12'] = grid['built_area'] / grid['cell_area'] # Built area share\n",
    "    grid['m13'] = grid['built_area'] / grid['n_buildings'] # Average building area\n",
    "\n",
    "    path = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{str(grid_size)}m_grid_{YOUR_NAME}.geoparquet'\n",
    "    grid.to_parquet(path)\n",
    "    return grid_cell_count, path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nairobi EPSG: 32737\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      2\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# Start the timer\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmetrics\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNairobi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYOUR_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrid_size\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# End the timer\u001b[39;00m\n\u001b[1;32m      7\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/base.py:374\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    351\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 374\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/base.py:662\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    659\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 662\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/threading.py:355\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 355\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()  # Start the timer\n",
    "\n",
    "metrics('Nairobi', YOUR_NAME, grid_size=grid_size).compute()\n",
    "\n",
    "end_time = time.time()  # End the timer\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Tasks completed in {elapsed_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nairobi EPSG: 32737\n",
      "Nairobi EPSG: 32737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py:4729: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  warnings.warn(meta_warning(meta))\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py:4729: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nairobi EPSG: 32737\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     grid_calculations\u001b[38;5;241m.\u001b[39mappend(grid_calc)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#visualize(*grid_calulations)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m calculated_grids \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgrid_calculations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m calculated_grids\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTasks completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/base.py:662\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    659\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 662\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/threading.py:355\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 355\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nairobi EPSG: 32737\n",
      "<class 'dask_geopandas.expr.GeoDataFrame'>\n",
      "<class 'dask_geopandas.expr.GeoDataFrame'>\n",
      "<class 'dask_geopandas.expr.GeoDataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py:4729: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  warnings.warn(meta_warning(meta))\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py:4729: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  warnings.warn(meta_warning(meta))\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py:4729: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "test_cities = ['Nairobi','Belo_Horizonte']\n",
    "\n",
    "# Create delayed tasks for counting\n",
    "grid_calculations = []\n",
    "\n",
    "for city_name in test_cities:\n",
    "    grid_calc = metrics(city_name, YOUR_NAME, grid_size=grid_size)\n",
    "    grid_calculations.append(grid_calc)\n",
    "\n",
    "#visualize(*grid_calulations)\n",
    "calculated_grids = compute(*grid_calculations)\n",
    "calculated_grids\n",
    "\n",
    "print(f\"Tasks completed in {elapsed_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total grid cells: 462160\n"
     ]
    }
   ],
   "source": [
    "# Sum the total number of grid cells\n",
    "total_grid_cells = sum([grid_cells for grid_cells, path in calculated_grids])\n",
    "print(f'Total grid cells: {total_grid_cells}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subdivisions2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
