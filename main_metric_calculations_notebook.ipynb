{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_NAME = 'sara'\n",
    "\n",
    "AWS_PROFILE = 'cities'\n",
    "\n",
    "'''\n",
    "# List of cities to process\n",
    "cities = [\"Belo Horizonte\", \"Campinas\"]#, \"Bogota\", \"Nairobi\", \"Bamako\", \n",
    "        #\"Lagos\", \"Accra\", \"Abidjan\", \"Mogadishu\", \"Cape Town\", \n",
    "        #\"Maputo\", \"Luanda\"]\n",
    "\n",
    "test_cities = [\"Belo Horizonte\"]\n",
    "#cities = test_cities\n",
    "\n",
    "cities = [city.replace(' ', '_') for city in cities]\n",
    "\n",
    "search_buffer_files = fs.ls(SEARCH_BUFFER_PATH)\n",
    "\n",
    "cities \n",
    "\n",
    "number_of_cities = len(cities)\n",
    "\n",
    "print(f'City count: {number_of_cities}')\n",
    "'''\n",
    "grid_size = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = \"s3://wri-cities-sandbox/identifyingLandSubdivisions/data\"\n",
    "INPUT_PATH = f'{MAIN_PATH}/input'\n",
    "CITY_INFO_PATH = f'{INPUT_PATH}/city_info'\n",
    "EXTENTS_PATH = f'{CITY_INFO_PATH}/extents'\n",
    "BUILDINGS_PATH = f'{INPUT_PATH}/buildings'\n",
    "BLOCKS_PATH = f'{INPUT_PATH}/blocks'\n",
    "ROADS_PATH = f'{INPUT_PATH}/roads'\n",
    "INTERSECTIONS_PATH = f'{INPUT_PATH}/intersections'\n",
    "GRIDS_PATH = f'{INPUT_PATH}/city_info/grids'\n",
    "SEARCH_BUFFER_PATH = f'{INPUT_PATH}/city_info/search_buffers'\n",
    "OUTPUT_PATH = f'{MAIN_PATH}/output'\n",
    "OUTPUT_PATH_CSV = f'{OUTPUT_PATH}/csv'\n",
    "OUTPUT_PATH_RASTER = f'{OUTPUT_PATH}/raster'\n",
    "OUTPUT_PATH_PNG = f'{OUTPUT_PATH}/png'\n",
    "OUTPUT_PATH_RAW = f'{OUTPUT_PATH}/raw_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '142RKXC9FW3PDC9F',\n",
       "  'HostId': '4CCqls1QXeS0gYDT2zfvJBbEg0AVOTwSoiUOS2+K+TR+ENLcZkI4kFyhMjrWGK6mF0IvUZj5m1mtohejtx7dEMXazF19L7Kl',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': '4CCqls1QXeS0gYDT2zfvJBbEg0AVOTwSoiUOS2+K+TR+ENLcZkI4kFyhMjrWGK6mF0IvUZj5m1mtohejtx7dEMXazF19L7Kl',\n",
       "   'x-amz-request-id': '142RKXC9FW3PDC9F',\n",
       "   'date': 'Thu, 24 Apr 2025 17:44:33 GMT',\n",
       "   'content-type': 'application/xml',\n",
       "   'transfer-encoding': 'chunked',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'Buckets': [{'Name': 'aft-sandbox-540362055257',\n",
       "   'CreationDate': datetime.datetime(2022, 9, 13, 15, 12, 20, tzinfo=tzutc())},\n",
       "  {'Name': 'amplify-citiesindicatorsapi-dev-10508-deployment',\n",
       "   'CreationDate': datetime.datetime(2023, 8, 30, 5, 5, 13, tzinfo=tzutc())},\n",
       "  {'Name': 'cities-heat',\n",
       "   'CreationDate': datetime.datetime(2023, 6, 1, 13, 22, 1, tzinfo=tzutc())},\n",
       "  {'Name': 'cities-test-sandbox',\n",
       "   'CreationDate': datetime.datetime(2025, 4, 18, 19, 10, 49, tzinfo=tzutc())},\n",
       "  {'Name': 'era5-brazil',\n",
       "   'CreationDate': datetime.datetime(2025, 2, 15, 19, 51, 14, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-athena-us-west-2',\n",
       "   'CreationDate': datetime.datetime(2024, 1, 12, 18, 45, 11, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-climate-hazards',\n",
       "   'CreationDate': datetime.datetime(2024, 1, 3, 16, 57, 31, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-data-api',\n",
       "   'CreationDate': datetime.datetime(2024, 7, 16, 8, 53, 31, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-heat',\n",
       "   'CreationDate': datetime.datetime(2024, 3, 25, 15, 46, 55, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-indicators',\n",
       "   'CreationDate': datetime.datetime(2024, 5, 13, 15, 50, 58, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-sandbox',\n",
       "   'CreationDate': datetime.datetime(2024, 7, 27, 0, 51, 38, tzinfo=tzutc())}],\n",
       " 'Owner': {'DisplayName': 'aws-cities',\n",
       "  'ID': 'df12253943982d72f60594f06c2cacf9a1ee3a9e738c1649c9fb96e5127f1a5c'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check s3 connection using AWS_PROFILE=CitiesUserPermissionSet profile \n",
    "import boto3\n",
    "\n",
    "session = boto3.Session(profile_name=AWS_PROFILE)\n",
    "s3 = session.client('s3')\n",
    "\n",
    "# export CitiesUserPermissionSet profile to use in the next cells\n",
    "import os\n",
    "os.environ['AWS_PROFILE'] = AWS_PROFILE\n",
    "\n",
    "\n",
    "s3.list_buckets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-24 12:44:36,407][INFO    ][coiled] Fetching latest package priorities...\n",
      "[2025-04-24 12:44:36,408][INFO    ][coiled.package_sync] Resolving your local subdivisions2 Python environment...\n",
      "[2025-04-24 12:44:37,378][INFO    ][coiled.package_sync] Scanning 444 conda packages...\n",
      "[2025-04-24 12:44:37,389][INFO    ][coiled.package_sync] Scanning 259 python packages...\n",
      "[2025-04-24 12:44:39,069][INFO    ][coiled] Running pip check...\n",
      "[2025-04-24 12:44:41,119][INFO    ][coiled] Validating environment...\n",
      "[2025-04-24 12:44:43,933][INFO    ][coiled] Creating wheel for ~/Documents/Identifying Land Subdivisions/identifyingLandSubdivisions...\n",
      "[2025-04-24 12:44:44,380][WARNING ][coiled.package_sync] Package - libopenvino-intel-cpu-plugin, libopenvino-intel-cpu-plugin~=2025.0.0 has no install candidate for Python 3.12 linux-aarch64 on conda-forge\n",
      "[2025-04-24 12:44:44,382][INFO    ][coiled] Uploading coiled_local_identifyingLandSubdivisions...\n",
      "[2025-04-24 12:44:45,343][INFO    ][coiled] Requesting package sync build...\n",
      "[2025-04-24 12:44:46,306][INFO    ][coiled] Creating Cluster (name: ils-sara, https://cloud.coiled.io/clusters/850713?account=wri-cities-data ). This usually takes 1-2 minutes...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started a new Dask client on Coiled. Dashboard is available at https://cluster-syvrn.dask.host/DauPWWfNSiNnSF4z/status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1965' coro=<Client._gather.<locals>.wait() done, defined at /Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/distributed/client.py:2394> exception=AllExit()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/distributed/client.py\", line 2403, in wait\n",
      "    raise AllExit()\n",
      "distributed.client.AllExit\n"
     ]
    }
   ],
   "source": [
    "import coiled\n",
    "\n",
    "cluster = coiled.Cluster(\n",
    "    workspace=\"wri-cities-data\",\n",
    "    name=f'ils-{YOUR_NAME}',\n",
    "    region=\"us-west-2\",\n",
    "    arm=True,\n",
    "    worker_vm_types=\"r8g.xlarge\",\n",
    "    spot_policy=\"spot\",\n",
    "    n_workers=4,\n",
    "    package_sync_ignore=[\"pyspark\", \"pypandoc\"]\n",
    ")\n",
    "client = cluster.get_client()\n",
    "\n",
    "print(f\"Started a new Dask client on Coiled. Dashboard is available at {client.dashboard_link}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_geopandas as dgpd\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from dask import delayed, compute, visualize\n",
    "import geopandas as gpd\n",
    "from dask.diagnostics import ProgressBar\n",
    "from shapely.geometry import MultiLineString, LineString, Point\n",
    "from shapely.ops import polygonize, nearest_points\n",
    "#from shapely.geometry import Polygon, LineString, Point, MultiPolygon, MultiLineString, GeometryCollection\n",
    "from scipy.optimize import fminbound, minimize\n",
    "from metrics_groupby import metrics\n",
    "\n",
    "from pre_processing import *\n",
    "from auxiliary_functions import *\n",
    "from standardize_metrics import *\n",
    "\n",
    "YOUR_NAME = 'sara'\n",
    "grid_size = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks completed in 904.61 seconds.\n"
     ]
    }
   ],
   "source": [
    "from pre_processing import *\n",
    "\n",
    "import time\n",
    "start_time = time.time()  # Start the timer\n",
    "\n",
    "%autoreload\n",
    "#from main import *\n",
    "\n",
    "\n",
    "def preprocess_all_cities(city_list):\n",
    "    #delayed_jobs = [delayed(calculate_building_distances_to_roads)(city) for city in city_list]\n",
    "    #delayed_jobs.append([delayed(produce_blocks)(city) for city in city_list])\n",
    "    delayed_jobs = []\n",
    "    delayed_jobs.append([calculate_building_distances_to_roads(city) for city in city_list])\n",
    "    delayed_jobs.append([produce_blocks(city,YOUR_NAME,grid_size) for city in city_list])\n",
    "    results = compute(*delayed_jobs)\n",
    "    return results\n",
    "\n",
    "preprocess_all_cities([\"Belo_Horizonte\", \"Medellin\"]) \n",
    "#[\"Abidjan\", \"Accra\", \"Nairobi\", \"Bamako\", \"Belo_Horizonte\", \"Bogota\", \"Campinas\", \"Cape_Town\", \"Lagos\",\"Luanda\", \"Nairobi\", \"Medellin\"])\n",
    "\n",
    "end_time = time.time()  # End the timer\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Tasks completed in {elapsed_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks completed in 133.54 seconds.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pre_processing import *\n",
    "\n",
    "import time\n",
    "start_time = time.time()  # Start the timer\n",
    "\n",
    "\n",
    "city_list = [\"Belo_Horizonte\", \"Medellin\"]\n",
    "#[\"Abidjan\", \"Accra\", \"Nairobi\", \"Bamako\", \"Belo_Horizonte\", \"Bogota\", \"Campinas\", \"Cape_Town\", \"Lagos\",\"Luanda\", \"Nairobi\", \"Medellin\"])\n",
    "\n",
    "delayed_jobs = []\n",
    "delayed_jobs.append([produce_azimuths(city, YOUR_NAME, grid_size) for city in city_list])\n",
    "results = compute(*delayed_jobs)\n",
    "\n",
    "end_time = time.time()  # End the timer\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Tasks completed in {elapsed_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def building_and_intersection_metrics(city_name):\n",
    "    grid_cell_count = 0\n",
    "    paths = {\n",
    "        'grid': f'{GRIDS_PATH}/{city_name}/{city_name}_{str(grid_size)}m_grid.geoparquet',\n",
    "        'buildings': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}.geoparquet',\n",
    "        'roads': f'{ROADS_PATH}/{city_name}/{city_name}_OSM_roads.geoparquet',\n",
    "        'intersections': f'{INTERSECTIONS_PATH}/{city_name}/{city_name}_OSM_intersections.geoparquet'\n",
    "    }\n",
    "    # Get EPSG\n",
    "    epsg = get_epsg(city_name).compute()\n",
    "    # Load grid\n",
    "    grid = load_dataset(paths['grid'], epsg=epsg)#.compute()\n",
    "    if 'geom' in grid.columns:\n",
    "        grid = grid.drop(columns=['geom'])\n",
    "    grid['cell_area'] = grid.geometry.area\n",
    "\n",
    "    cells = grid.index.size\n",
    "    grid_cell_count += cells\n",
    "\n",
    "    # Load buildings and perform relevant calculations on it\n",
    "    buildings = load_dataset(paths['buildings'], epsg=epsg)#.compute()\n",
    "    buildings['area'] = buildings.geometry.area\n",
    "    joined_buildings = dgpd.sjoin(buildings, grid, predicate='intersects')  \n",
    "    counts_buildings = joined_buildings.groupby('index_right').size()\n",
    "    grid['n_buildings'] = grid.index.map(counts_buildings.astype(int))#.fillna(0.).astype(int)\n",
    "    built_area_buildings = joined_buildings.groupby('index_right')['area'].sum()\n",
    "    grid['built_area'] = grid.index.map(built_area_buildings).astype(float)#.fillna(0.).astype(float)\n",
    "\n",
    "    #total_buildings = row_count(buildings).compute()\n",
    "    #print(total_buildings)\n",
    "    # Load roads\n",
    "    roads = load_dataset(paths['roads'], epsg=epsg)#.compute()\n",
    "    included_road_types = ['trunk','motorway','primary','secondary','tertiary','primary_link','secondary_link','tertiary_link','trunk_link','motorway_link','residential','unclassified','road','living_street']\n",
    "\n",
    "    def highway_filter(highway_value):\n",
    "        # If highway_value is missing, return False\n",
    "        if pd.isna(highway_value):\n",
    "            return False\n",
    "        # Split the string by commas, and strip any whitespace from each part\n",
    "        types = [part.strip() for part in highway_value.split(',')]\n",
    "        # Return True if any of the types is in our included list\n",
    "        return any(t in included_road_types for t in types)\n",
    "\n",
    "    # Now filter the roads GeoDataFrame:\n",
    "    roads = roads[roads['highway'].apply(highway_filter)]\n",
    "    #roads['highway'][roads['highway'].isin(included_road_types)]\n",
    "    \n",
    "    #road_union = roads.unary_union.compute()\n",
    "    #roads = roads.compute()\n",
    "\n",
    "    # Load intersections\n",
    "    intersections = load_dataset(paths['intersections'], epsg=epsg)#.compute()\n",
    "\n",
    "    all_valid_intersections = intersections[intersections.street_count >= 2]\n",
    "    intersections_3plus = intersections[intersections.street_count >= 3]\n",
    "    intersections_4way = intersections[intersections.street_count == 4]\n",
    "\n",
    "    grid['cell_area_km2'] = grid['cell_area']/1000000.\n",
    "\n",
    "\n",
    "    roads_grid_joined = dgpd.sjoin(roads, grid, predicate='intersects')\n",
    "\n",
    "    roads_grid_joined = roads_grid_joined.merge(\n",
    "        grid[['geometry']],\n",
    "        left_on='index_right',\n",
    "        right_index=True,\n",
    "        suffixes=('', '_grid')\n",
    "    )\n",
    "\n",
    "    # Compute the intersection (i.e. the \"cookie-cutter\") road segment within the grid cell for each row:\n",
    "    def compute_intersection_length(row):\n",
    "        try:\n",
    "            inter_geom = row['geometry'].intersection(row['geometry_grid'])\n",
    "            return inter_geom.length\n",
    "        except Exception:\n",
    "            return 0.0\n",
    "\n",
    "    roads_grid_joined['road_length_in_cell'] = roads_grid_joined.apply(compute_intersection_length, axis=1)\n",
    "\n",
    "    # Now sum the intersection lengths by grid cell index:\n",
    "    road_length_km = roads_grid_joined.groupby('index_right')['road_length_in_cell'].sum() / 1000.    \n",
    "\n",
    "    #road_length_km = roads_grid_joined.groupby('index_right')['length'].sum()/1000.\n",
    "    grid['road_length'] = grid.index.map(road_length_km).astype(float)#.fillna(0.).astype(float)\n",
    "    grid['has_roads'] = grid['road_length'].fillna(0) > 0\n",
    "\n",
    "    joined_all_valid_intersections = dgpd.sjoin(all_valid_intersections, grid, predicate='intersects')\n",
    "    counts_all_valid_intersections = joined_all_valid_intersections.groupby('index_right').size()\n",
    "    grid['n_intersections'] = grid.index.map(counts_all_valid_intersections).astype(float)\n",
    "    grid['has_intersections'] = grid['n_intersections'].fillna(0)> 0\n",
    "\n",
    "    joined_intersections_3plus = dgpd.sjoin(intersections_3plus, grid, predicate='intersects')\n",
    "    counts_intersections_3plus = joined_intersections_3plus.groupby('index_right').size()\n",
    "    grid['intersections_3plus'] = grid.index.map(counts_intersections_3plus).astype(float)#.fillna(0.).astype(float)\n",
    "\n",
    "    joined_intersections_4way = dgpd.sjoin(intersections_4way, grid, predicate='intersects')\n",
    "    counts_intersections_4way = joined_intersections_4way.groupby('index_right').size()\n",
    "    grid['intersections_4way'] = grid.index.map(counts_intersections_4way).astype(float)#.fillna(0.).astype(float) # OJO: NEED TO CHANGE NA\n",
    "\n",
    "\n",
    "    grid['m3_raw'] = grid['road_length']/grid['cell_area_km2']\n",
    "    grid['m3_raw'] = grid['m3_raw']#.fillna(0.)\n",
    "    grid['m3_std'] = grid['m3_raw'].map_partitions(standardize_metric_3, meta=('m3', 'float64'))\n",
    "\n",
    "    grid['m4_raw'] = grid['intersections_4way'] / grid['intersections_3plus']\n",
    "    grid['m4_raw'] = grid['m4_raw'].mask((grid['m4_raw'].isna()) & (grid['road_length'] > 0), 0.)\n",
    "    avg_m4 = grid['m4_raw'].mean().compute()\n",
    "    grid['m4_raw'] = grid['m4_raw']#.fillna(avg_m4)\n",
    "    grid['m4_std'] = grid['m4_raw'].map_partitions(standardize_metric_4, meta=('m4', 'float64'))\n",
    "\n",
    "\n",
    "    grid['m5_raw'] =  (1000.**2)*(grid['n_intersections']/grid['cell_area']) #make sure this is equivalent to the meter calculation\n",
    "    grid['m5_raw'] = grid['m5_raw']#.fillna(0.)\n",
    "    # wherever has_roads is True AND m5_raw is NA, set it to 0\n",
    "    grid['m5_raw'] = grid['m5_raw'].mask(\n",
    "        grid['has_roads'] & grid['m5_raw'].isna(),\n",
    "        0.\n",
    "    )\n",
    "    grid['m5_std'] = grid['m5_raw'].map_partitions(standardize_metric_5, meta=('m5', 'float64'))\n",
    "\n",
    "    \n",
    "    grid['m11_raw'] = 1.0*grid['n_buildings'] / grid['cell_area_km2'] # Building density\n",
    "    grid['m11_raw'] = grid['m11_raw']#.fillna(0.)\n",
    "    grid['m11_std'] = grid['m11_raw'].map_partitions(standardize_metric_11, meta=('m11', 'float64'))\n",
    "\n",
    "    grid['m12_raw'] = grid['built_area'] / grid['cell_area'] # Built area share\n",
    "    grid['m12_raw'] = grid['m12_raw']#.fillna(0.)\n",
    "    grid['m12_std'] = grid['m12_raw'].map_partitions(standardize_metric_12, meta=('m12', 'float64'))\n",
    "\n",
    "    grid['m13_raw'] = grid['built_area'] / grid['n_buildings'] # Average building area\n",
    "    grid['m13_raw'] = grid['m13_raw']#.fillna(0.)\n",
    "    grid['m13_std'] = grid['m13_raw'].map_partitions(standardize_metric_13, meta=('m13', 'float64'))\n",
    "\n",
    "    path = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{str(grid_size)}m_metrics_3_4_5_11_12_13_grid_{YOUR_NAME}.geoparquet'\n",
    "\n",
    "    if 'geom' in grid.columns:\n",
    "        grid = grid.drop(columns='geom')\n",
    "\n",
    "    grid.to_parquet(path)\n",
    "    return grid_cell_count, path\n",
    "\n",
    "\n",
    "@delayed\n",
    "def building_distance_metrics(city_name):\n",
    "\n",
    "     grid_cell_count = 0\n",
    "     paths = {\n",
    "         'grid': f'{GRIDS_PATH}/{city_name}/{city_name}_{str(grid_size)}m_grid.geoparquet',\n",
    "         'buildings': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}.geoparquet',\n",
    "         'buildings_with_distances': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}_with_distances.geoparquet',\n",
    "         'roads': f'{ROADS_PATH}/{city_name}/{city_name}_OSM_roads.geoparquet',\n",
    "         'intersections': f'{INTERSECTIONS_PATH}/{city_name}/{city_name}_OSM_intersections.geoparquet'\n",
    "     }\n",
    "     # Get EPSG\n",
    "     epsg = get_epsg(city_name).compute()\n",
    "     # Load grid\n",
    "     grid = load_dataset(paths['grid'], epsg=epsg)#.compute()\n",
    "     if 'geom' in grid.columns:\n",
    "         grid = grid.drop(columns=['geom'])\n",
    "     \n",
    "     buildings = load_dataset(paths['buildings_with_distances'], epsg=epsg)#.compute()\n",
    "     buildings['distance_to_nearest_road'] = buildings['distance_to_nearest_road'].astype(float)\n",
    "     buildings['area'] = buildings.geometry.area\n",
    "     joined_buildings = dgpd.sjoin(buildings, grid, predicate='intersects')  \n",
    "     counts_buildings = joined_buildings.groupby('index_right').size()\n",
    "     grid['n_buildings'] = grid.index.map(counts_buildings).fillna(0).astype(int)\n",
    "     grid['has_buildings'] = grid['n_buildings']    > 0\n",
    "     average_distance = joined_buildings.groupby('index_right')['distance_to_nearest_road'].mean()\n",
    "     grid['average_distance_nearest_building'] = grid.index.map(average_distance).astype(float)#fillna(np.mean(average_distance)).astype(float)\n",
    "    \n",
    "    \n",
    "     buildings_closer_than_20m = buildings[buildings['distance_to_nearest_road'] <= 20]\n",
    "     joined_buildings_closer_than_20m = dgpd.sjoin(buildings_closer_than_20m, grid, predicate='intersects') \n",
    "     n_buildings_closer_than_20m = joined_buildings_closer_than_20m.groupby('index_right').size()\n",
    "     grid['n_buildings_closer_than_20m'] = grid.index.map(n_buildings_closer_than_20m).astype(float)#.fillna(np.mean(n_buildings_closer_than_20m))\n",
    "     grid = grid.assign(\n",
    "    n_buildings_closer_than_20m = grid['n_buildings_closer_than_20m'].mask(\n",
    "        (grid['n_buildings'] > 0) & (grid['n_buildings_closer_than_20m'].isna()),\n",
    "        0))\n",
    "     grid = grid.assign(\n",
    "         m1_raw = grid['n_buildings_closer_than_20m'] / grid['n_buildings']\n",
    "         )\n",
    "     grid['m1_raw'] = grid['m1_raw']#.fillna(grid['m1_raw'].mean())#grid['n_buildings_closer_than_20m'] / grid['n_buildings']\n",
    "     grid['m1_std'] = grid['m1_raw'].map_partitions(standardize_metric_1, meta=('m1', 'float64'))\n",
    "     grid['m2_raw'] = grid['average_distance_nearest_building']\n",
    "     grid['m2_raw'] = grid['m2_raw']#.fillna(grid['m2_raw'].mean())\n",
    "     grid['m2_std'] = grid['m2_raw'].map_partitions(standardize_metric_2, meta=('m2', 'float64'))\n",
    "     \n",
    "     path = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{str(grid_size)}m_grid_{YOUR_NAME}_metrics_1_2.geoparquet'\n",
    "    \n",
    "     if 'geom' in grid.columns:\n",
    "         grid = grid.drop(columns='geom')\n",
    "    \n",
    "     grid.to_parquet(path)\n",
    "\n",
    "\n",
    "\n",
    "@delayed\n",
    "def compute_m6_m7_m8(city_name):\n",
    "    \"\"\"\n",
    "    Computes:\n",
    "    - M6: KL divergence (building orientation)\n",
    "    - M7: Average block width\n",
    "    - M8: Building density ratio (inner vs. outer buffer)\n",
    "    \"\"\"\n",
    "\n",
    "    epsilon = 0.001\n",
    "    paths = {\n",
    "        'grid': f'{GRIDS_PATH}/{city_name}/{city_name}_{str(grid_size)}m_grid.geoparquet',\n",
    "        'blocks': f'{BLOCKS_PATH}/{city_name}/{city_name}_blocks_{YOUR_NAME}.geoparquet',\n",
    "        'buildings_with_distances': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}_with_distances.geoparquet',\n",
    "        'buildings_with_distances_azimuths': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}_with_distances_and_azimuths.geoparquet',\n",
    "        'buildings_to_blocks':f'{BLOCKS_PATH}/{city_name}/{city_name}_buildings_to_blocks_{YOUR_NAME}.geoparquet'\n",
    "    }\n",
    "\n",
    "    epsg = get_epsg(city_name).compute()\n",
    "    grid = load_dataset(paths['grid'], epsg=epsg)\n",
    "    blocks = load_dataset(paths['blocks'], epsg=epsg).persist()\n",
    "    buildings = load_dataset(paths['buildings_with_distances_azimuths'], epsg=epsg).persist()\n",
    "    buildings['azimuth'] = buildings['azimuth'].map_partitions(pd.to_numeric, errors='coerce')\n",
    "\n",
    "\n",
    "    if 'geom' in grid.columns:\n",
    "        grid = grid.drop(columns=['geom'])\n",
    "    \n",
    "    blocks['block_id'] = blocks.index\n",
    "    blocks['epsilon_buffer'] = blocks['geometry'].buffer(-(1.- epsilon) * blocks['max_radius'])\n",
    "    blocks['width_buffer'] = blocks['geometry'].buffer(-0.2 * blocks['max_radius'])\n",
    "\n",
    "    buildings_blocks = dgpd.sjoin(buildings, blocks, predicate='intersects').persist() #,how='right'\n",
    "    buildings_blocks = buildings_blocks[['block_id', 'geometry', 'epsilon_buffer','width_buffer','azimuth']]\n",
    "    buildings_blocks = buildings_blocks.set_index('block_id').repartition(npartitions=4)\n",
    "\n",
    "    block_grid_overlap = compute_block_grid_weights(blocks, grid)\n",
    "    block_grid_overlap = block_grid_overlap.compute()\n",
    "\n",
    "    # Metric 6\n",
    "    kl_df = compute_block_kl_metrics(buildings_blocks)\n",
    "    m6_grid = aggregate_m6(kl_df.compute(), block_grid_overlap)\n",
    " \n",
    "    # Metric 7\n",
    "    block_grid_overlap['weighted_max_radius'] = (\n",
    "        block_grid_overlap['max_radius'] * block_grid_overlap['area_weight']\n",
    "    )\n",
    "\n",
    "    grid_m7 = block_grid_overlap.groupby('grid_id').agg(\n",
    "        total_weighted_max_radius=('weighted_max_radius', 'sum'),\n",
    "        total_weight=('area_weight', 'sum')\n",
    "    )\n",
    "    grid_m7['m7'] = grid_m7['total_weighted_max_radius'] / grid_m7['total_weight']\n",
    "\n",
    "    # Metric 8\n",
    "    width_buffer_ratios = buildings_blocks.map_partitions(clip_buildings_by_buffer, buffer_type='width_buffer')\n",
    "    epsilon_buffer_ratios = buildings_blocks.map_partitions(clip_buildings_by_buffer, buffer_type='epsilon_buffer')\n",
    "    clipped_buildings_area_to_buffer_ratio = epsilon_buffer_ratios / width_buffer_ratios\n",
    "    clipped_buildings_area_to_buffer_ratio = clipped_buildings_area_to_buffer_ratio.replace([np.inf, -np.inf], np.nan)#.fillna(999)\n",
    "    ratio_df = clipped_buildings_area_to_buffer_ratio.to_frame(name='m8')\n",
    "    blocks_with_m8 = blocks.merge(ratio_df, left_on='block_id', right_index=True, how='left').compute()\n",
    "    block_grid_overlap = block_grid_overlap.merge(blocks_with_m8, how='left',left_on='block_id',right_index=True)\n",
    "    block_grid_overlap['weighted_m8'] = (\n",
    "        block_grid_overlap['m8'] * block_grid_overlap['area_weight']\n",
    "    )\n",
    "    grid_m8 = block_grid_overlap.groupby('grid_id').agg(\n",
    "        total_weighted_m8=('weighted_m8', 'sum'),\n",
    "        total_weight=('area_weight', 'sum')\n",
    "    )\n",
    "    grid_m8['m8'] = grid_m8['total_weighted_m8'] / grid_m8['total_weight']\n",
    "\n",
    "    # Merge all metrics\n",
    "    grid = grid.merge(m6_grid, left_index=True, right_index=True, how='left')\n",
    "    grid = grid.merge(grid_m7[['m7']], left_index=True, right_index=True, how='left')\n",
    "    grid = grid.merge(grid_m8[['m8']], left_index=True, right_index=True, how='left')\n",
    "\n",
    "    '''\n",
    "    # Fill NaNs\n",
    "    '''\n",
    "\n",
    "    grid['m6_raw'] = grid['m6']#.fillna(grid['m6'].mean())\n",
    "    grid['m6_std'] = grid['m6_raw'].map_partitions(standardize_metric_6, meta=('m6', 'float64'))\n",
    "\n",
    "\n",
    "    grid['m7_raw'] = grid['m7']#.fillna(grid['m7'].max())\n",
    "    grid['m7_std'] = grid['m7_raw'].map_partitions(standardize_metric_7, meta=('m7', 'float64'))\n",
    "    grid['m8_raw'] = grid['m8']#.fillna(grid['m8'].max())\n",
    "    grid['m8_std'] = grid['m8_raw'].map_partitions(standardize_metric_8, meta=('m8', 'float64'))\n",
    "    \n",
    "    grid = grid.drop(columns=['m6','m7','m8'])\n",
    "    # Save Output\n",
    "    grid = grid.compute()  \n",
    "    path = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{str(grid_size)}m_grid_{YOUR_NAME}_metrics_6_7_8.geoparquet'\n",
    "    grid.to_parquet(path)\n",
    "    \n",
    "    #path = f'{BLOCKS_PATH}/{city_name}/{city_name}_blocks_with_m8_{YOUR_NAME}.geoparquet'#f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{str(grid_size)}m_grid_{YOUR_NAME}_metrics_6_7_8.geoparquet'\n",
    "    #blocks_with_m8.to_parquet(path)\n",
    "    return  path\n",
    "\n",
    "def partition_tortuosity_clipped(df):\n",
    "    # df.geometry is the clipped segment\n",
    "    seg = df.geometry\n",
    "    L = seg.length.values            # true path length\n",
    "    b = seg.bounds                   # DataFrame with minx,miny,maxx,maxy\n",
    "    Dx = (b[\"maxx\"] - b[\"minx\"]).values\n",
    "    Dy = (b[\"maxy\"] - b[\"miny\"]).values\n",
    "    D  = np.hypot(Dx, Dy)            # straight‐line distance\n",
    "    S  = np.clip(D / L, 0, 1)        # straightness in [0,1]\n",
    "    W  = L * S                       # length‐weighted straightness\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"index_right\": df[\"index_right\"].values,\n",
    "        \"wt\":           W,\n",
    "        \"length\":      L\n",
    "    }, index=df.index)\n",
    "\n",
    "def overlay_partition(roads_part, grid_small):\n",
    "    # roads_part: a pandas GeoDataFrame partition of full roads GDF\n",
    "    # grid_small: a pandas GeoDataFrame of all grid cells with an index_right column\n",
    "    return gpd.overlay(roads_part, grid_small, how=\"intersection\")\n",
    "\n",
    "\n",
    "@delayed\n",
    "def metrics_roads_intersections(city_name):\n",
    "\n",
    "    paths = {\n",
    "    'grid': f'{GRIDS_PATH}/{city_name}/{city_name}_{str(grid_size)}m_grid.geoparquet',\n",
    "    'blocks': f'{BLOCKS_PATH}/{city_name}/{city_name}_blocks_{YOUR_NAME}.geoparquet',\n",
    "    'buildings_with_distances': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}_with_distances.geoparquet',\n",
    "    'roads': f'{ROADS_PATH}/{city_name}/{city_name}_OSM_roads.geoparquet',\n",
    "    'intersections': f'{INTERSECTIONS_PATH}/{city_name}/{city_name}_OSM_intersections.geoparquet'\n",
    "    }\n",
    "\n",
    "    # LOAD\n",
    "    epsg = get_epsg(city_name).compute()\n",
    "    grid = load_dataset(paths['grid'], epsg=epsg).persist()\n",
    "    roads = load_dataset(paths['roads'], epsg=epsg).persist()\n",
    "    intersections = load_dataset(paths['intersections'], epsg=epsg).compute()\n",
    "\n",
    "\n",
    "    included_road_types = ['trunk','motorway','primary','secondary','tertiary','primary_link','secondary_link','tertiary_link','trunk_link','motorway_link','residential','unclassified','road','living_street']\n",
    "    roads['highway'][roads['highway'].isin(included_road_types)]\n",
    "    \n",
    "\n",
    "    if 'geom' in grid.columns:\n",
    "        grid = grid.drop(columns=['geom'])\n",
    "\n",
    "\n",
    "    \n",
    "    # Prep for metric 10\n",
    "    intersections['osmid'] = intersections['osmid'].astype(int)\n",
    "    intersection_angles = compute_intersection_angles(roads, intersections)\n",
    "    street_count_mapping = intersections.set_index('osmid')['street_count'].to_dict()\n",
    "    intersection_angle_mapping = compute_intersection_mapping(intersection_angles, street_count_mapping)\n",
    "    intersection_angle_mapping = intersection_angle_mapping.compute()  \n",
    "\n",
    "    intersections_with_angles_metric = intersections.merge(\n",
    "        intersection_angle_mapping.rename(\"average_angle\"), left_on=\"osmid\", right_index=True, how=\"left\"\n",
    "    )\n",
    "\n",
    "    joined_intersection_angles_grid = dgpd.sjoin(intersections_with_angles_metric, grid, predicate=\"within\")\n",
    "    average_angle_between_roads = joined_intersection_angles_grid.groupby('index_right')['average_angle'].mean()\n",
    "    \n",
    "\n",
    "    # Prep for metric 9\n",
    "    \n",
    "    roads_simple = roads[['geometry']]\n",
    "    \n",
    "    grid_small = (\n",
    "        grid\n",
    "        .reset_index()[[\"index\",\"geometry\"]]\n",
    "        .rename(columns={\"index\":\"index_right\"})\n",
    "    )\n",
    "    \n",
    "    # inside metrics_roads_intersections, before overlay:\n",
    "    overlay_meta = gpd.GeoDataFrame(\n",
    "        {\n",
    "            \"index_right\": pd.Series(dtype=\"int64\"),\n",
    "            \"geometry\":    gpd.GeoSeries(dtype=\"geometry\")\n",
    "        },\n",
    "        geometry=\"geometry\"\n",
    "    )\n",
    "\n",
    "    roads_cells = roads_simple.map_partitions(\n",
    "        overlay_partition,\n",
    "        grid_small,\n",
    "        meta=overlay_meta\n",
    "    ).persist()\n",
    "\n",
    "    # roads_cells now has:\n",
    "    #  - geometry   = clipped road piece\n",
    "    #  - index_right = the cell it belongs to\n",
    "\n",
    "    # 3) compute wt & length in one pass per partition\n",
    "    out = roads_cells.map_partitions(\n",
    "        partition_tortuosity_clipped,   \n",
    "        meta=pd.DataFrame({\n",
    "            \"index_right\": pd.Series(dtype=\"int64\"),\n",
    "            \"wt\":           pd.Series(dtype=\"float64\"),\n",
    "            \"length\":      pd.Series(dtype=\"float64\")\n",
    "        })\n",
    "    ).persist()\n",
    "\n",
    "    # 4) aggregate back into grid\n",
    "    agg = out.groupby(\"index_right\").agg(\n",
    "        total_len=(\"length\",\"sum\"),\n",
    "        sum_wt   =(\"wt\",    \"sum\")\n",
    "    )\n",
    "    grid[\"m9_raw\"] = grid.index.map(agg[\"sum_wt\"]/agg[\"total_len\"]).astype(float)\n",
    "    grid[\"m9_std\"] = grid[\"m9_raw\"].map_partitions(\n",
    "        standardize_metric_9, meta=(\"m9\",\"float64\")\n",
    "    )\n",
    "    \n",
    "\n",
    "    '''\n",
    "    roads_with_tortuosity = calculate_tortuosity(roads.compute(), intersections)\n",
    "    joined_tortuosity_grid = dgpd.sjoin(roads_with_tortuosity.set_geometry('road_geometry'), grid, predicate=\"intersects\")\n",
    "    average_tortuosity = joined_tortuosity_grid.groupby('index_right')['tortuosity'].mean()\n",
    "\n",
    "\n",
    "    grid['m9_raw'] = grid.index.map(average_tortuosity).astype(float)#.fillna(0.).astype(float)\n",
    "    grid['m9_std'] = grid['m9_raw'].map_partitions(standardize_metric_9, meta=('m9', 'float64'))\n",
    "    '''\n",
    "    \n",
    "    grid['m10_raw'] = grid.index.map(average_angle_between_roads).astype(float)#.fillna(np.mean(average_angle_between_roads)).astype(float)\n",
    "    grid['m10_std'] = grid['m10_raw'].map_partitions(standardize_metric_10, meta=('m10', 'float64'))\n",
    "    \n",
    "    path = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{str(grid_size)}m_grid_metrics_9_10_{YOUR_NAME}.geoparquet'\n",
    "\n",
    "    if 'geom' in grid.columns:\n",
    "        grid = grid.drop(columns='geom')\n",
    "\n",
    "    grid.to_parquet(path)\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks completed in 291.58 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from dask import compute\n",
    "\n",
    "start_time = time.time()  # Start the timer\n",
    "\n",
    "cities = [\"Medellin\"]#['Nairobi']#\"Belo_Horizonte\", \n",
    "cities = [city.replace(' ', '_') for city in cities]\n",
    "\n",
    "tasks = []\n",
    "for city in cities:\n",
    "    tasks.append(building_and_intersection_metrics(city))\n",
    "    #tasks.append(building_distance_metrics(city))\n",
    "    #tasks.append(compute_m6_m7_m8(city))\n",
    "    #tasks.append(metrics_roads_intersections(city))\n",
    "\n",
    "results = compute(*tasks)\n",
    "\n",
    "end_time = time.time()  # End the timer\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Tasks completed in {elapsed_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py:3243: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'m1_zero-centered': 'float64', 'm2_zero-centered': 'float64', 'm3_zero-centered': 'float64', 'm4_zero-centered': 'float64', 'm5_zero-centered': 'float64', 'm6_zero-centered': 'float64', 'm7_zero-centered': 'float64', 'm8_zero-centered': 'float64', 'm9_zero-centered': 'float64', 'm10_zero-centered': 'float64', 'm11_zero-centered': 'float64', 'm12_zero-centered': 'float64', 'm13_zero-centered': 'float64'})\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py:3243: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'m1_final': 'float64', 'm2_final': 'float64', 'm3_final': 'float64', 'm4_final': 'float64', 'm5_final': 'float64', 'm6_final': 'float64', 'm7_final': 'float64', 'm8_final': 'float64', 'm9_final': 'float64', 'm10_final': 'float64', 'm11_final': 'float64', 'm12_final': 'float64', 'm13_final': 'float64'})\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    }
   ],
   "source": [
    "import dask_geopandas as dgpd\n",
    "import pandas as pd\n",
    "\n",
    "def consolidate_irregularity_index(city_name):\n",
    "    # Define file paths\n",
    "    path_metrics_most = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{grid_size}m_metrics_3_4_5_11_12_13_grid_{YOUR_NAME}.geoparquet'\n",
    "    path_metrics_distances = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{grid_size}m_grid_{YOUR_NAME}_metrics_1_2.geoparquet'\n",
    "    path_metrics_blocks = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{grid_size}m_grid_{YOUR_NAME}_metrics_6_7_8.geoparquet'\n",
    "    path_metrics_intersections = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{grid_size}m_grid_metrics_9_10_{YOUR_NAME}.geoparquet'\n",
    "    \n",
    "    # Read each file as a Dask GeoDataFrame\n",
    "    grid_most = dgpd.read_parquet(path_metrics_most)\n",
    "    grid_distances = dgpd.read_parquet(path_metrics_distances)[['m1_std','m1_raw','m2_std','m2_raw','has_buildings']]\n",
    "    grid_blocks = dgpd.read_parquet(path_metrics_blocks)[['m6_std','m6_raw','m7_std','m7_raw','m8_std','m8_raw']]\n",
    "    grid_intersections = dgpd.read_parquet(path_metrics_intersections)[['m9_std','m9_raw','m10_std','m10_raw']]\n",
    "    \n",
    "    # Drop duplicate geometry columns from non-base grids.\n",
    "    # Explicitly reassign so the changes persist.\n",
    "    grid_distances = grid_distances.drop(columns=['geometry'], errors='ignore')\n",
    "    grid_blocks    = grid_blocks.drop(columns=['geometry'], errors='ignore')\n",
    "    grid_intersections = grid_intersections.drop(columns=['geometry'], errors='ignore')\n",
    "    \n",
    "    # Merge the files on the index (left join on grid_most)\n",
    "    grid_combined = grid_most.merge(grid_distances, left_index=True, right_index=True, how='left')\n",
    "    grid_combined = grid_combined.merge(grid_blocks, left_index=True, right_index=True, how='left')\n",
    "    grid_combined = grid_combined.merge(grid_intersections, left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    # Process metrics: note that here we assume that standardized metric columns already exist\n",
    "    all_metrics_columns = ['m'+str(x) for x in range(1,14)]\n",
    "    all_metrics_columns_final = ['m'+str(x)+'_final' for x in range(1,14)]\n",
    "    metrics_standardized_names = [col+'_std' for col in all_metrics_columns]\n",
    "    \n",
    "    # Create mask for raw and standardized names, for this to propagate downstream.\n",
    "    raw_cols = [f'm{i}_raw' for i in range(1,14)]\n",
    "    grid_combined['has_features'] = grid_combined['has_buildings'] | grid_combined['has_roads'] | grid_combined['has_intersections']\n",
    "    grid_combined[metrics_standardized_names] = grid_combined[metrics_standardized_names].where(grid_combined['has_features'], np.nan)\n",
    "    grid_combined[raw_cols] = grid_combined[raw_cols].where(grid_combined['has_features'], np.nan)\n",
    "\n",
    "    # Create zero-centered copies of the standardized metric columns.\n",
    "    zero_centered_names_list = [col+'_zero-centered' for col in all_metrics_columns]\n",
    "    grid_combined[zero_centered_names_list] = grid_combined[metrics_standardized_names].copy() #grid_combined[metrics_standardized_names].copy()\n",
    "    \n",
    "    # Center at zero (subtract mean and divide by std)\n",
    "    grid_combined[zero_centered_names_list] = grid_combined[zero_centered_names_list].apply(\n",
    "        lambda x: (x - x.mean()) / x.std(), axis=1\n",
    "    )\n",
    "    \n",
    "    # Copy the zero-centered values to the base metric columns\n",
    "    grid_combined[all_metrics_columns_final] = grid_combined[zero_centered_names_list].copy()\n",
    "    \n",
    "    # Rescale metrics to [0,1] using min–max normalization.\n",
    "    grid_combined[all_metrics_columns_final] = grid_combined[all_metrics_columns_final].apply(\n",
    "        lambda x: (x - x.min()) / (x.max() - x.min()), axis=1\n",
    "    )\n",
    "    \n",
    "    # Calculate equal-weights irregularity index as the mean across all metrics.\n",
    "    # (Using axis=1 because we average across columns.)\n",
    "    grid_combined['regularity_index'] = grid_combined[all_metrics_columns_final].mean(axis=1)\n",
    "\n",
    "    # Add a column that counts the number of NA values per row among the 13 final metric columns.\n",
    "    grid_combined['na_count'] = grid_combined[all_metrics_columns_final].isna().sum(axis=1)\n",
    "    \n",
    "    \n",
    "    # Define output path and save the merged grid\n",
    "    out_path = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{grid_size}m_all_metrics_combined_{YOUR_NAME}.geoparquet'\n",
    "    #grid_combined.to_parquet(out_path)\n",
    "    \n",
    "    return grid_combined\n",
    "\n",
    "\n",
    "grid_combined = consolidate_irregularity_index('Medellin')\n",
    "grid_combined = grid_combined.compute()\n",
    "#print(\"Combined grid saved to:\", consolidated_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_combined.to_parquet('../Medellin_consolidated_index_results.parquet')#regularity_index.describe() #[['m1_raw','m2_raw','regularity_index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mt/3n9j2kc92kv4psztx687vtd80000gn/T/ipykernel_66546/401492889.py:1: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  grid_combined.to_file('../Medellin_consolidated_index_results.shp')\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'n_buildings' to 'n_building'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'cell_area_km2' to 'cell_area_'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'road_length' to 'road_lengt'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'n_intersections' to 'n_intersec'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'has_intersections' to 'has_inters'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'intersections_3plus' to 'intersecti'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'intersections_4way' to 'intersec_1'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'has_buildings' to 'has_buildi'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'has_features' to 'has_featur'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'm1_zero-centered' to 'm1_zero-ce'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'm2_zero-centered' to 'm2_zero-ce'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'm3_zero-centered' to 'm3_zero-ce'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'm4_zero-centered' to 'm4_zero-ce'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'm5_zero-centered' to 'm5_zero-ce'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'm6_zero-centered' to 'm6_zero-ce'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'm7_zero-centered' to 'm7_zero-ce'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'm8_zero-centered' to 'm8_zero-ce'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'm9_zero-centered' to 'm9_zero-ce'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'm10_zero-centered' to 'm10_zero-c'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'm11_zero-centered' to 'm11_zero-c'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'm12_zero-centered' to 'm12_zero-c'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'm13_zero-centered' to 'm13_zero-c'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'regularity_index' to 'regularity'\n",
      "  ogr_write(\n"
     ]
    }
   ],
   "source": [
    "grid_combined.to_file('../Medellin_consolidated_index_results.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_name = 'Nairobi'\n",
    "# Define file paths\n",
    "path_metrics_most = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{grid_size}m_metrics_3_4_5_11_12_13_grid_{YOUR_NAME}.geoparquet'\n",
    "path_metrics_distances = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{grid_size}m_grid_{YOUR_NAME}_metrics_1_2.geoparquet'\n",
    "path_metrics_blocks = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{grid_size}m_grid_{YOUR_NAME}_metrics_6_7_8.geoparquet'\n",
    "path_metrics_intersections = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{grid_size}m_grid_metrics_9_10_{YOUR_NAME}.geoparquet'\n",
    "\n",
    "# Read each file as a Dask GeoDataFrame\n",
    "grid_most = dgpd.read_parquet(path_metrics_most)\n",
    "grid_distances = dgpd.read_parquet(path_metrics_distances)[['m1','m1_raw','m2','m2_raw']]\n",
    "grid_blocks = dgpd.read_parquet(path_metrics_blocks)[['m6','m6_raw','m7','m7_raw','m8','m8_raw']]\n",
    "grid_intersections = dgpd.read_parquet(path_metrics_intersections)[['m9','m9_raw','m10','m10_raw']]\n",
    "\n",
    "# Drop duplicate geometry columns from non-base grids.\n",
    "# Explicitly reassign so the changes persist.\n",
    "grid_distances = grid_distances.drop(columns=['geometry'], errors='ignore')\n",
    "grid_blocks    = grid_blocks.drop(columns=['geometry'], errors='ignore')\n",
    "grid_intersections = grid_intersections.drop(columns=['geometry'], errors='ignore')\n",
    "\n",
    "# Merge the files on the index (left join on grid_most)\n",
    "grid_combined = grid_most.merge(grid_distances, left_index=True, right_index=True, how='left')\n",
    "grid_combined = grid_combined.merge(grid_blocks, left_index=True, right_index=True, how='left')\n",
    "grid_combined = grid_combined.merge(grid_intersections, left_index=True, right_index=True, how='left')\n",
    "\n",
    "# Process metrics: note that here we assume that standardized metric columns already exist\n",
    "all_metrics_columns = ['m'+str(x)+'_raw' for x in range(1,14)]\n",
    "all_metrics_columns_final = ['m'+str(x)+'_final' for x in range(1,14)]\n",
    "metrics_standardized_names = all_metrics_columns#= {col: col+'_standardized' for col in all_metrics_columns}\n",
    "\n",
    "# Create zero-centered copies of the standardized metric columns.\n",
    "zero_centered_names_list = [col+'_zero-centered' for col in all_metrics_columns]\n",
    "grid_combined[zero_centered_names_list] = grid_combined[metrics_standardized_names].copy()#grid_combined[list(metrics_standardized_names.values())].copy()\n",
    "\n",
    "# Center at zero (subtract mean and divide by std)\n",
    "grid_combined[zero_centered_names_list] = grid_combined[zero_centered_names_list].apply(\n",
    "    lambda x: (x - x.mean()) / x.std(), axis=1\n",
    ")\n",
    "\n",
    "# Copy the zero-centered values to the base metric columns\n",
    "grid_combined[all_metrics_columns_final] = grid_combined[zero_centered_names_list].copy()\n",
    "\n",
    "# Rescale metrics to [0,1] using min–max normalization.\n",
    "grid_combined[all_metrics_columns_final] = grid_combined[all_metrics_columns].apply(\n",
    "    lambda x: (x - x.min()) / (x.max() - x.min()), axis=1\n",
    ")\n",
    "\n",
    "# Calculate equal-weights irregularity index as the mean across all metrics.\n",
    "# (Using axis=1 because we average across columns.)\n",
    "grid_combined['regularity_index'] = grid_combined[all_metrics_columns_final].mean(axis=1)\n",
    "\n",
    "# Add a column that counts the number of NA values per row among the 13 final metric columns.\n",
    "grid_combined['na_count'] = grid_combined[all_metrics_columns_final].isna().sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py:4392: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta=('highway', 'bool'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    }
   ],
   "source": [
    "city_name = 'Medellin'\n",
    "\n",
    "paths = {\n",
    "'grid': f'{GRIDS_PATH}/{city_name}/{city_name}_{str(grid_size)}m_grid.geoparquet',\n",
    "'blocks': f'{BLOCKS_PATH}/{city_name}/{city_name}_blocks_{YOUR_NAME}.geoparquet',\n",
    "'buildings_with_distances': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}_with_distances.geoparquet',\n",
    "'roads': f'{ROADS_PATH}/{city_name}/{city_name}_OSM_roads.geoparquet',\n",
    "'intersections': f'{INTERSECTIONS_PATH}/{city_name}/{city_name}_OSM_intersections.geoparquet'\n",
    "}\n",
    "\n",
    "epsg = get_epsg(city_name).compute()\n",
    "roads = load_dataset(paths['roads'], epsg=epsg)\n",
    "intersections = load_dataset(paths['intersections'], epsg=epsg).compute()\n",
    "\n",
    "\n",
    "included_road_types = ['trunk','motorway','primary','secondary','tertiary','primary_link','secondary_link','tertiary_link','trunk_link','motorway_link','residential','unclassified','road','living_street']\n",
    "\n",
    "def highway_filter(highway_value):\n",
    "    # If highway_value is missing, return False\n",
    "    if pd.isna(highway_value):\n",
    "        return False\n",
    "    # Split the string by commas, and strip any whitespace from each part\n",
    "    types = [part.strip() for part in highway_value.split(',')]\n",
    "    # Return True if any of the types is in our included list\n",
    "    return any(t in included_road_types for t in types)\n",
    "\n",
    "# Now filter the roads GeoDataFrame:\n",
    "roads = roads[roads['highway'].apply(highway_filter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>v</th>\n",
       "      <th>key</th>\n",
       "      <th>osmid</th>\n",
       "      <th>highway</th>\n",
       "      <th>lanes</th>\n",
       "      <th>name</th>\n",
       "      <th>oneway</th>\n",
       "      <th>ref</th>\n",
       "      <th>reversed</th>\n",
       "      <th>length</th>\n",
       "      <th>geometry</th>\n",
       "      <th>maxspeed</th>\n",
       "      <th>width</th>\n",
       "      <th>access</th>\n",
       "      <th>bridge</th>\n",
       "      <th>tunnel</th>\n",
       "      <th>junction</th>\n",
       "      <th>service</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>287291520</td>\n",
       "      <td>4330951724</td>\n",
       "      <td>0</td>\n",
       "      <td>737517930</td>\n",
       "      <td>trunk</td>\n",
       "      <td>3</td>\n",
       "      <td>Doble Calzada Niquia - Hatillo</td>\n",
       "      <td>True</td>\n",
       "      <td>25</td>\n",
       "      <td>False</td>\n",
       "      <td>47.473184</td>\n",
       "      <td>LINESTRING (447935.599 705576.417, 447888.744 ...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>287291520</td>\n",
       "      <td>9883236579</td>\n",
       "      <td>0</td>\n",
       "      <td>1026331900</td>\n",
       "      <td>residential</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>False</td>\n",
       "      <td>nan</td>\n",
       "      <td>False</td>\n",
       "      <td>8.457614</td>\n",
       "      <td>LINESTRING (447935.599 705576.417, 447933.118 ...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>287291575</td>\n",
       "      <td>8880939061</td>\n",
       "      <td>0</td>\n",
       "      <td>443200030</td>\n",
       "      <td>residential</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>True</td>\n",
       "      <td>nan</td>\n",
       "      <td>False</td>\n",
       "      <td>13.300952</td>\n",
       "      <td>LINESTRING (442274.299 701537.538, 442261.036 ...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>287291575</td>\n",
       "      <td>4408219206</td>\n",
       "      <td>0</td>\n",
       "      <td>573773888</td>\n",
       "      <td>trunk</td>\n",
       "      <td>3</td>\n",
       "      <td>Doble Calzada Niquia - Hatillo</td>\n",
       "      <td>True</td>\n",
       "      <td>25</td>\n",
       "      <td>False</td>\n",
       "      <td>174.396808</td>\n",
       "      <td>LINESTRING (442274.299 701537.538, 442241.905 ...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>287291580</td>\n",
       "      <td>8880939092</td>\n",
       "      <td>0</td>\n",
       "      <td>559197931</td>\n",
       "      <td>unclassified</td>\n",
       "      <td>2</td>\n",
       "      <td>nan</td>\n",
       "      <td>True</td>\n",
       "      <td>nan</td>\n",
       "      <td>False</td>\n",
       "      <td>9.100614</td>\n",
       "      <td>LINESTRING (441363.446 701333.13, 441357.027 7...</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           u           v  key       osmid       highway lanes  \\\n",
       "0  287291520  4330951724    0   737517930         trunk     3   \n",
       "1  287291520  9883236579    0  1026331900   residential   nan   \n",
       "2  287291575  8880939061    0   443200030   residential   nan   \n",
       "3  287291575  4408219206    0   573773888         trunk     3   \n",
       "4  287291580  8880939092    0   559197931  unclassified     2   \n",
       "\n",
       "                             name  oneway  ref reversed      length  \\\n",
       "0  Doble Calzada Niquia - Hatillo    True   25    False   47.473184   \n",
       "1                             nan   False  nan    False    8.457614   \n",
       "2                             nan    True  nan    False   13.300952   \n",
       "3  Doble Calzada Niquia - Hatillo    True   25    False  174.396808   \n",
       "4                             nan    True  nan    False    9.100614   \n",
       "\n",
       "                                            geometry maxspeed width access  \\\n",
       "0  LINESTRING (447935.599 705576.417, 447888.744 ...      nan   nan    nan   \n",
       "1  LINESTRING (447935.599 705576.417, 447933.118 ...      nan   nan    nan   \n",
       "2  LINESTRING (442274.299 701537.538, 442261.036 ...      nan   nan    nan   \n",
       "3  LINESTRING (442274.299 701537.538, 442241.905 ...      nan   nan    nan   \n",
       "4  LINESTRING (441363.446 701333.13, 441357.027 7...       20     7    nan   \n",
       "\n",
       "  bridge tunnel junction service area  \n",
       "0    nan    nan      nan     nan  nan  \n",
       "1    nan    nan      nan     nan  nan  \n",
       "2    nan    nan      nan     nan  nan  \n",
       "3    nan    nan      nan     nan  nan  \n",
       "4    nan    nan      nan     nan  nan  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roads.head()\n",
    "\n",
    "# remove rows if u and v are not part of the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>osmid</th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "      <th>street_count</th>\n",
       "      <th>highway</th>\n",
       "      <th>railway</th>\n",
       "      <th>ref</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>287291520</td>\n",
       "      <td>6.383091</td>\n",
       "      <td>-75.470784</td>\n",
       "      <td>3</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>POINT (447935.599 705576.417)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>287291575</td>\n",
       "      <td>6.346507</td>\n",
       "      <td>-75.521937</td>\n",
       "      <td>3</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>POINT (442274.299 701537.538)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>287291580</td>\n",
       "      <td>6.344650</td>\n",
       "      <td>-75.530170</td>\n",
       "      <td>3</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>POINT (441363.446 701333.13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>287291588</td>\n",
       "      <td>6.338477</td>\n",
       "      <td>-75.542407</td>\n",
       "      <td>3</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>POINT (440009.309 700652.169)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>287291691</td>\n",
       "      <td>6.341378</td>\n",
       "      <td>-75.536104</td>\n",
       "      <td>3</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>POINT (440706.811 700972.124)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66120</th>\n",
       "      <td>12746001862</td>\n",
       "      <td>6.245379</td>\n",
       "      <td>-75.549317</td>\n",
       "      <td>3</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>POINT (439234.187 690361.064)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66121</th>\n",
       "      <td>12746001866</td>\n",
       "      <td>6.245122</td>\n",
       "      <td>-75.549496</td>\n",
       "      <td>3</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>POINT (439214.422 690332.685)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66122</th>\n",
       "      <td>12746001871</td>\n",
       "      <td>6.245466</td>\n",
       "      <td>-75.549690</td>\n",
       "      <td>3</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>POINT (439192.945 690370.791)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66123</th>\n",
       "      <td>12746001872</td>\n",
       "      <td>6.245563</td>\n",
       "      <td>-75.549142</td>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>POINT (439253.59 690381.429)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66124</th>\n",
       "      <td>12746001876</td>\n",
       "      <td>6.245109</td>\n",
       "      <td>-75.550246</td>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>POINT (439131.453 690331.422)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66125 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             osmid         y          x  street_count highway railway  ref  \\\n",
       "0        287291520  6.383091 -75.470784             3     nan     nan  nan   \n",
       "1        287291575  6.346507 -75.521937             3     nan     nan  nan   \n",
       "2        287291580  6.344650 -75.530170             3     nan     nan  nan   \n",
       "3        287291588  6.338477 -75.542407             3     nan     nan  nan   \n",
       "4        287291691  6.341378 -75.536104             3     nan     nan  nan   \n",
       "...            ...       ...        ...           ...     ...     ...  ...   \n",
       "66120  12746001862  6.245379 -75.549317             3     nan     nan  nan   \n",
       "66121  12746001866  6.245122 -75.549496             3     nan     nan  nan   \n",
       "66122  12746001871  6.245466 -75.549690             3     nan     nan  nan   \n",
       "66123  12746001872  6.245563 -75.549142             1     nan     nan  nan   \n",
       "66124  12746001876  6.245109 -75.550246             1     nan     nan  nan   \n",
       "\n",
       "                            geometry  \n",
       "0      POINT (447935.599 705576.417)  \n",
       "1      POINT (442274.299 701537.538)  \n",
       "2       POINT (441363.446 701333.13)  \n",
       "3      POINT (440009.309 700652.169)  \n",
       "4      POINT (440706.811 700972.124)  \n",
       "...                              ...  \n",
       "66120  POINT (439234.187 690361.064)  \n",
       "66121  POINT (439214.422 690332.685)  \n",
       "66122  POINT (439192.945 690370.791)  \n",
       "66123   POINT (439253.59 690381.429)  \n",
       "66124  POINT (439131.453 690331.422)  \n",
       "\n",
       "[66125 rows x 8 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "import dask.dataframe as dd\n",
    "from auxiliary_functions import load_dataset\n",
    "import dask_geopandas as dgpd\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from dask import delayed, compute\n",
    "from citywide_calculation import get_utm_crs\n",
    "from dask.diagnostics import ProgressBar\n",
    "from shapely.geometry import MultiLineString, LineString, Point\n",
    "from shapely.ops import polygonize, nearest_points\n",
    "from scipy.optimize import fminbound, minimize\n",
    "from metrics_groupby import metrics\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "YOUR_NAME = 'sara'\n",
    "grid_size = 200\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. Load common datasets (for metrics m1, m2, m3, m4, m5, m9, m10, m11, m12, m13)\n",
    "# ------------------------------------------------------------------------------\n",
    "@delayed\n",
    "def load_common_datasets(city_name):\n",
    "    epsg = get_epsg(city_name).compute()\n",
    "    \n",
    "    grid_path = f'{GRIDS_PATH}/{city_name}/{city_name}_{grid_size}m_grid.geoparquet'\n",
    "    grid = load_dataset(grid_path, epsg=epsg)\n",
    "    if 'geom' in grid.columns:\n",
    "        grid = grid.drop(columns=['geom'])\n",
    "    grid['cell_area'] = grid.geometry.area\n",
    "    \n",
    "    buildings_path = f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}.geoparquet'\n",
    "    buildings = load_dataset(buildings_path, epsg=epsg)\n",
    "    \n",
    "    roads_path = f'{ROADS_PATH}/{city_name}/{city_name}_OSM_roads.geoparquet'\n",
    "    roads = load_dataset(roads_path, epsg=epsg)\n",
    "    \n",
    "    intersections_path = f'{INTERSECTIONS_PATH}/{city_name}/{city_name}_OSM_intersections.geoparquet'\n",
    "    intersections = load_dataset(intersections_path, epsg=epsg)\n",
    "    \n",
    "    bld_with_dist_path = f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}_with_distances.geoparquet'\n",
    "    buildings_with_dist = load_dataset(bld_with_dist_path, epsg=epsg)\n",
    "    \n",
    "    return {\n",
    "         'grid': grid,\n",
    "         'buildings': buildings,\n",
    "         'buildings_with_dist': buildings_with_dist,\n",
    "         'roads': roads,\n",
    "         'intersections': intersections,\n",
    "         'epsg': epsg,\n",
    "    }\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. Metric functions using the common datasets\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Metric 1: Ratio of buildings within 20 m to total buildings\n",
    "@delayed\n",
    "def compute_metric_m1(city_name, datasets):\n",
    "    grid = datasets['grid']\n",
    "    buildings = datasets['buildings_with_dist']\n",
    "    # Ensure distances are floats\n",
    "    buildings['distance_to_nearest_road'] = buildings['distance_to_nearest_road'].astype(float)\n",
    "    \n",
    "    joined = dgpd.sjoin(buildings, grid, predicate='intersects')\n",
    "    total_buildings = joined.groupby('index_right').size()\n",
    "    grid = grid.assign(n_buildings=grid.index.map(total_buildings).fillna(0).astype(int))\n",
    "    \n",
    "    buildings_close = buildings[buildings['distance_to_nearest_road'] <= 20]\n",
    "    joined_close = dgpd.sjoin(buildings_close, grid, predicate='intersects')\n",
    "    count_close = joined_close.groupby('index_right').size()\n",
    "    \n",
    "    m1 = grid.index.map(count_close).fillna(0).astype(float) / grid['n_buildings']\n",
    "    return m1\n",
    "\n",
    "# Metric 2: Average distance to the nearest road\n",
    "@delayed\n",
    "def compute_metric_m2(city_name, datasets):\n",
    "    grid = datasets['grid']\n",
    "    buildings = datasets['buildings_with_dist']\n",
    "    joined = dgpd.sjoin(buildings, grid, predicate='intersects')\n",
    "    avg_distance = joined.groupby('index_right')['distance_to_nearest_road'].mean()\n",
    "    m2 = grid.index.map(avg_distance).fillna(0).astype(float)\n",
    "    return m2\n",
    "\n",
    "# Metric 3: Road length density (road length per cell area in km²)\n",
    "@delayed\n",
    "def compute_metric_m3(city_name, datasets):\n",
    "    grid = datasets['grid']\n",
    "    roads = datasets['roads']\n",
    "    grid = grid.assign(cell_area_km2=grid['cell_area'] / 1e6)\n",
    "    roads_joined = dgpd.sjoin(roads, grid, predicate='within')\n",
    "    road_length_km = roads_joined.groupby('index_right')['length'].sum() / 1000.\n",
    "    m3 = grid.index.map(road_length_km).fillna(0).astype(float) / grid['cell_area_km2']\n",
    "    return m3\n",
    "\n",
    "# Metric 4: Ratio of 4-way intersections to intersections with 3+ roads\n",
    "@delayed\n",
    "def compute_metric_m4(city_name, datasets):\n",
    "    grid = datasets['grid']\n",
    "    intersections = datasets['intersections'].compute()  # force computation if needed\n",
    "    intersections_3plus = intersections[intersections.street_count >= 3]\n",
    "    intersections_4way = intersections[intersections.street_count == 4]\n",
    "    \n",
    "    joined_3plus = dgpd.sjoin(intersections_3plus, grid, predicate='within')\n",
    "    count_3plus = joined_3plus.groupby('index_right').size()\n",
    "    \n",
    "    joined_4way = dgpd.sjoin(intersections_4way, grid, predicate='within')\n",
    "    count_4way = joined_4way.groupby('index_right').size()\n",
    "    \n",
    "    m4 = grid.index.map(count_4way).fillna(0).astype(float) / grid.index.map(count_3plus).fillna(0)\n",
    "    return m4\n",
    "\n",
    "# Metric 5: Intersection density per cell area (using 4-way intersections)\n",
    "@delayed\n",
    "def compute_metric_m5(city_name, datasets):\n",
    "    grid = datasets['grid']\n",
    "    intersections = datasets['intersections'].compute()\n",
    "    count_4way = dgpd.sjoin(intersections[intersections.street_count == 4], grid, predicate='within')\\\n",
    "                    .groupby('index_right').size()\n",
    "    m5 = (1000.**2) * grid.index.map(count_4way).fillna(0).astype(float) / grid['cell_area']\n",
    "    return m5\n",
    "\n",
    "# Metric 11: Building density (number of buildings per cell area)\n",
    "@delayed\n",
    "def compute_metric_m11(city_name, datasets):\n",
    "    grid = datasets['grid']\n",
    "    buildings = datasets['buildings']\n",
    "    buildings = buildings.assign(area=buildings.geometry.area)\n",
    "    joined = dgpd.sjoin(buildings, grid, predicate='within')\n",
    "    count_buildings = joined.groupby('index_right').size()\n",
    "    m11 = grid.index.map(count_buildings).fillna(0).astype(float) / grid['cell_area']\n",
    "    return m11\n",
    "\n",
    "# Metric 12: Built area share (sum of building areas divided by cell area)\n",
    "@delayed\n",
    "def compute_metric_m12(city_name, datasets):\n",
    "    grid = datasets['grid']\n",
    "    buildings = datasets['buildings']\n",
    "    buildings = buildings.assign(area=buildings.geometry.area)\n",
    "    joined = dgpd.sjoin(buildings, grid, predicate='within')\n",
    "    sum_area = joined.groupby('index_right')['area'].sum()\n",
    "    m12 = grid.index.map(sum_area).fillna(0).astype(float) / grid['cell_area']\n",
    "    return m12\n",
    "\n",
    "# Metric 13: Average building area\n",
    "@delayed\n",
    "def compute_metric_m13(city_name, datasets):\n",
    "    grid = datasets['grid']\n",
    "    buildings = datasets['buildings']\n",
    "    buildings = buildings.assign(area=buildings.geometry.area)\n",
    "    joined = dgpd.sjoin(buildings, grid, predicate='within')\n",
    "    avg_area = joined.groupby('index_right')['area'].mean()\n",
    "    m13 = grid.index.map(avg_area).fillna(0).astype(float)\n",
    "    return m13\n",
    "\n",
    "# Metric 9: Average road tortuosity\n",
    "@delayed\n",
    "def compute_metric_m9(city_name, datasets):\n",
    "    grid = datasets['grid']\n",
    "    roads = datasets['roads'].compute()\n",
    "    intersections = datasets['intersections'].compute()\n",
    "    # calculate_tortuosity is assumed to be defined elsewhere.\n",
    "    roads_with_tortuosity = calculate_tortuosity(roads, intersections)\n",
    "    joined_tortuosity = dgpd.sjoin(roads_with_tortuosity.set_geometry('road_geometry'), grid, predicate=\"intersects\")\n",
    "    average_tortuosity = joined_tortuosity.groupby('index_right')['tortuosity'].mean()\n",
    "    m9 = grid.index.map(average_tortuosity).fillna(-999.).astype(float)\n",
    "    return m9\n",
    "\n",
    "# Metric 10: Average intersection angle\n",
    "@delayed\n",
    "def compute_metric_m10(city_name, datasets):\n",
    "    grid = datasets['grid']\n",
    "    intersections = datasets['intersections'].compute()\n",
    "    intersection_angles = compute_intersection_angles(datasets['roads'], intersections)\n",
    "    street_count_mapping = intersections.set_index('osmid')['street_count'].to_dict()\n",
    "    intersection_angle_mapping = compute_intersection_mapping(intersection_angles, street_count_mapping).compute()\n",
    "    intersections_with_angles = intersections.merge(\n",
    "         intersection_angle_mapping.rename(\"average_angle\"),\n",
    "         left_on=\"osmid\", right_index=True, how=\"left\"\n",
    "    )\n",
    "    joined_angles = dgpd.sjoin(intersections_with_angles, grid, predicate=\"within\")\n",
    "    average_angle = joined_angles.groupby('index_right')['average_angle'].mean()\n",
    "    m10 = grid.index.map(average_angle).fillna(-999.).astype(float)\n",
    "    return m10\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3. Metric functions for block-based calculations (m6, m7, m8)\n",
    "# These load additional datasets (blocks, buildings with azimuths) and follow your original logic.\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Metric 6: KL divergence for building orientation\n",
    "@delayed\n",
    "def compute_metric_m6(city_name):\n",
    "    epsilon = 0.001\n",
    "    epsg = get_epsg(city_name).compute()\n",
    "    \n",
    "    grid_path = f'{GRIDS_PATH}/{city_name}/{city_name}_{grid_size}m_grid.geoparquet'\n",
    "    grid = load_dataset(grid_path, epsg=epsg)\n",
    "    if 'geom' in grid.columns:\n",
    "         grid = grid.drop(columns=['geom'])\n",
    "    \n",
    "    blocks_path = f'{BLOCKS_PATH}/{city_name}/{city_name}_blocks_{YOUR_NAME}.geoparquet'\n",
    "    blocks = load_dataset(blocks_path, epsg=epsg).persist()\n",
    "    \n",
    "    buildings_az_path = f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}_with_distances_and_azimuths.geoparquet'\n",
    "    buildings = load_dataset(buildings_az_path, epsg=epsg).persist()\n",
    "    buildings['azimuth'] = buildings['azimuth'].map_partitions(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    blocks['block_id'] = blocks.index\n",
    "    blocks['epsilon_buffer'] = blocks['geometry'].buffer(-(1.- epsilon) * blocks['max_radius'])\n",
    "    blocks['width_buffer'] = blocks['geometry'].buffer(-0.2 * blocks['max_radius'])\n",
    "    \n",
    "    buildings_blocks = dgpd.sjoin(buildings, blocks, predicate='intersects').persist()\n",
    "    buildings_blocks = buildings_blocks[['block_id', 'geometry', 'epsilon_buffer', 'width_buffer', 'azimuth']]\n",
    "    buildings_blocks = buildings_blocks.set_index('block_id').repartition(npartitions=4)\n",
    "    \n",
    "    block_grid_overlap = compute_block_grid_weights(blocks, grid)\n",
    "    block_grid_overlap = block_grid_overlap.compute()\n",
    "    \n",
    "    kl_df = compute_block_kl_metrics(buildings_blocks)\n",
    "    m6_grid = aggregate_m6(kl_df.compute(), block_grid_overlap)\n",
    "    \n",
    "    grid = grid.merge(m6_grid, left_index=True, right_index=True, how='left')\n",
    "    m6 = grid['m6'].fillna(0)\n",
    "    return m6\n",
    "\n",
    "# Metric 7: Average block width\n",
    "@delayed\n",
    "def compute_metric_m7(city_name):\n",
    "    epsilon = 0.001\n",
    "    epsg = get_epsg(city_name).compute()\n",
    "    \n",
    "    grid_path = f'{GRIDS_PATH}/{city_name}/{city_name}_{grid_size}m_grid.geoparquet'\n",
    "    grid = load_dataset(grid_path, epsg=epsg)\n",
    "    if 'geom' in grid.columns:\n",
    "         grid = grid.drop(columns=['geom'])\n",
    "    \n",
    "    blocks_path = f'{BLOCKS_PATH}/{city_name}/{city_name}_blocks_{YOUR_NAME}.geoparquet'\n",
    "    blocks = load_dataset(blocks_path, epsg=epsg).persist()\n",
    "    \n",
    "    block_grid_overlap = compute_block_grid_weights(blocks, grid)\n",
    "    block_grid_overlap = block_grid_overlap.compute()\n",
    "    block_grid_overlap['weighted_max_radius'] = block_grid_overlap['max_radius'] * block_grid_overlap['area_weight']\n",
    "    \n",
    "    grid_m7 = block_grid_overlap.groupby('grid_id').agg(\n",
    "         total_weighted_max_radius=('weighted_max_radius', 'sum'),\n",
    "         total_weight=('area_weight', 'sum')\n",
    "    )\n",
    "    grid_m7['m7'] = grid_m7['total_weighted_max_radius'] / grid_m7['total_weight']\n",
    "    \n",
    "    grid = grid.merge(grid_m7[['m7']], left_index=True, right_index=True, how='left')\n",
    "    m7 = grid['m7'].fillna(0)\n",
    "    return m7\n",
    "\n",
    "# Metric 8: Building density ratio (inner vs. outer buffer)\n",
    "@delayed\n",
    "def compute_metric_m8(city_name):\n",
    "    epsilon = 0.001\n",
    "    epsg = get_epsg(city_name).compute()\n",
    "    \n",
    "    grid_path = f'{GRIDS_PATH}/{city_name}/{city_name}_{grid_size}m_grid.geoparquet'\n",
    "    grid = load_dataset(grid_path, epsg=epsg)\n",
    "    if 'geom' in grid.columns:\n",
    "         grid = grid.drop(columns=['geom'])\n",
    "    \n",
    "    blocks_path = f'{BLOCKS_PATH}/{city_name}/{city_name}_blocks_{YOUR_NAME}.geoparquet'\n",
    "    blocks = load_dataset(blocks_path, epsg=epsg).persist()\n",
    "    \n",
    "    buildings_az_path = f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}_with_distances_and_azimuths.geoparquet'\n",
    "    buildings = load_dataset(buildings_az_path, epsg=epsg).persist()\n",
    "    buildings['azimuth'] = buildings['azimuth'].map_partitions(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    blocks['block_id'] = blocks.index\n",
    "    blocks['epsilon_buffer'] = blocks['geometry'].buffer(-(1.- epsilon) * blocks['max_radius'])\n",
    "    blocks['width_buffer'] = blocks['geometry'].buffer(-0.2 * blocks['max_radius'])\n",
    "    \n",
    "    buildings_blocks = dgpd.sjoin(buildings, blocks, predicate='intersects').persist()\n",
    "    buildings_blocks = buildings_blocks[['block_id', 'geometry', 'epsilon_buffer', 'width_buffer', 'azimuth']]\n",
    "    buildings_blocks = buildings_blocks.set_index('block_id').repartition(npartitions=4)\n",
    "    \n",
    "    block_grid_overlap = compute_block_grid_weights(blocks, grid)\n",
    "    block_grid_overlap = block_grid_overlap.compute()\n",
    "    \n",
    "    width_buffer_ratios = buildings_blocks.map_partitions(clip_buildings_by_buffer, buffer_type='width_buffer')\n",
    "    epsilon_buffer_ratios = buildings_blocks.map_partitions(clip_buildings_by_buffer, buffer_type='epsilon_buffer')\n",
    "    clipped_buildings_area_to_buffer_ratio = epsilon_buffer_ratios / width_buffer_ratios\n",
    "    clipped_buildings_area_to_buffer_ratio = clipped_buildings_area_to_buffer_ratio.replace([np.inf, -np.inf], np.nan).fillna(999)\n",
    "    ratio_df = clipped_buildings_area_to_buffer_ratio.to_frame(name='m8')\n",
    "    \n",
    "    blocks_with_m8 = blocks.merge(ratio_df, left_on='block_id', right_index=True, how='left').compute()\n",
    "    block_grid_overlap = block_grid_overlap.merge(blocks_with_m8, how='left', left_on='block_id', right_index=True)\n",
    "    block_grid_overlap['weighted_m8'] = block_grid_overlap['m8'] * block_grid_overlap['area_weight']\n",
    "    \n",
    "    grid_m8 = block_grid_overlap.groupby('grid_id').agg(\n",
    "         total_weighted_m8=('weighted_m8', 'sum'),\n",
    "         total_weight=('area_weight', 'sum')\n",
    "    )\n",
    "    grid_m8['m8'] = grid_m8['total_weighted_m8'] / grid_m8['total_weight']\n",
    "    \n",
    "    grid = grid.merge(grid_m8[['m8']], left_index=True, right_index=True, how='left')\n",
    "    m8 = grid['m8'].fillna(-999.)\n",
    "    return m8\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4. Merge all metrics into the grid and save\n",
    "# ------------------------------------------------------------------------------\n",
    "def calculate_metrics(city_name):\n",
    "    # Load common datasets (for m1, m2, m3, m4, m5, m9, m10, m11, m12, m13)\n",
    "    datasets = load_common_datasets(city_name)\n",
    "    \n",
    "    # Compute metrics that use the common datasets\n",
    "    m1   = compute_metric_m1(city_name, datasets)\n",
    "    m2   = compute_metric_m2(city_name, datasets)\n",
    "    m3   = compute_metric_m3(city_name, datasets)\n",
    "    m4   = compute_metric_m4(city_name, datasets)\n",
    "    m5   = compute_metric_m5(city_name, datasets)\n",
    "    m11  = compute_metric_m11(city_name, datasets)\n",
    "    m12  = compute_metric_m12(city_name, datasets)\n",
    "    m13  = compute_metric_m13(city_name, datasets)\n",
    "    m9   = compute_metric_m9(city_name, datasets)\n",
    "    m10  = compute_metric_m10(city_name, datasets)\n",
    "    \n",
    "    # Compute block-based metrics (m6, m7, m8) which load their own extra datasets\n",
    "    m6   = compute_metric_m6(city_name)\n",
    "    m7   = compute_metric_m7(city_name)\n",
    "    m8   = compute_metric_m8(city_name)\n",
    "    \n",
    "    @delayed\n",
    "    def merge_metrics(grid, m1, m2, m3, m4, m5, m6, m7, m8, m9, m10, m11, m12, m13):\n",
    "         grid['m1'] = m1\n",
    "         grid['m2'] = m2\n",
    "         grid['m3'] = m3\n",
    "         grid['m4'] = m4\n",
    "         grid['m5'] = m5\n",
    "         grid['m6'] = m6\n",
    "         grid['m7'] = m7\n",
    "         grid['m8'] = m8\n",
    "         grid['m9'] = m9\n",
    "         grid['m10'] = m10\n",
    "         grid['m11'] = m11\n",
    "         grid['m12'] = m12\n",
    "         grid['m13'] = m13\n",
    "         return grid\n",
    "    \n",
    "    final_grid = merge_metrics(datasets['grid'], m1, m2, m3, m4, m5, m6, m7, m8, m9, m10, m11, m12, m13)\n",
    "    \n",
    "    @delayed\n",
    "    def save_grid(grid, path):\n",
    "         if 'geom' in grid.columns:\n",
    "              grid = grid.drop(columns=['geom'])\n",
    "         grid.to_parquet(path)\n",
    "         return path\n",
    "    \n",
    "    out_path = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{grid_size}m_grid_all_metrics_{YOUR_NAME}.geoparquet'\n",
    "    saved = save_grid(final_grid, out_path)\n",
    "    result = compute(saved)\n",
    "    return result[0]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 5. Example usage\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    city = 'Nairobi'\n",
    "    city = city.replace(' ', '_')\n",
    "    final_path = calculate_metrics(city)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"All metrics computed and saved to {final_path}\")\n",
    "    print(f\"Tasks completed in {elapsed_time:.2f} seconds.\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import time\n",
    "start_time = time.time()\n",
    "city = 'Nairobi'\n",
    "city = city.replace(' ', '_')\n",
    "final_path = calculate_metrics(city)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"All metrics computed and saved to {final_path}\")\n",
    "print(f\"Tasks completed in {elapsed_time:.2f} seconds.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subdivisions2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
