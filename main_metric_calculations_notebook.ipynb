{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_NAME = 'sara'\n",
    "\n",
    "AWS_PROFILE = 'cities'\n",
    "\n",
    "'''\n",
    "# List of cities to process\n",
    "cities = [\"Belo Horizonte\", \"Campinas\"]#, \"Bogota\", \"Nairobi\", \"Bamako\", \n",
    "        #\"Lagos\", \"Accra\", \"Abidjan\", \"Mogadishu\", \"Cape Town\", \n",
    "        #\"Maputo\", \"Luanda\"]\n",
    "\n",
    "test_cities = [\"Belo Horizonte\"]\n",
    "#cities = test_cities\n",
    "\n",
    "cities = [city.replace(' ', '_') for city in cities]\n",
    "\n",
    "search_buffer_files = fs.ls(SEARCH_BUFFER_PATH)\n",
    "\n",
    "cities \n",
    "\n",
    "number_of_cities = len(cities)\n",
    "\n",
    "print(f'City count: {number_of_cities}')\n",
    "'''\n",
    "grid_size = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = \"s3://wri-cities-sandbox/identifyingLandSubdivisions/data\"\n",
    "INPUT_PATH = f'{MAIN_PATH}/input'\n",
    "CITY_INFO_PATH = f'{INPUT_PATH}/city_info'\n",
    "EXTENTS_PATH = f'{CITY_INFO_PATH}/extents'\n",
    "BUILDINGS_PATH = f'{INPUT_PATH}/buildings'\n",
    "BLOCKS_PATH = f'{INPUT_PATH}/blocks'\n",
    "ROADS_PATH = f'{INPUT_PATH}/roads'\n",
    "INTERSECTIONS_PATH = f'{INPUT_PATH}/intersections'\n",
    "GRIDS_PATH = f'{INPUT_PATH}/city_info/grids'\n",
    "SEARCH_BUFFER_PATH = f'{INPUT_PATH}/city_info/search_buffers'\n",
    "OUTPUT_PATH = f'{MAIN_PATH}/output'\n",
    "OUTPUT_PATH_CSV = f'{OUTPUT_PATH}/csv'\n",
    "OUTPUT_PATH_RASTER = f'{OUTPUT_PATH}/raster'\n",
    "OUTPUT_PATH_PNG = f'{OUTPUT_PATH}/png'\n",
    "OUTPUT_PATH_RAW = f'{OUTPUT_PATH}/raw_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '2159T8PH6PPXDCGW',\n",
       "  'HostId': '1KFS2frR0q91MuhgUdThCswCHYmB91zgJoBXlTQDFH6xdKc7pOJn8VWKnGsniaE71fJA7/QVZPA=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': '1KFS2frR0q91MuhgUdThCswCHYmB91zgJoBXlTQDFH6xdKc7pOJn8VWKnGsniaE71fJA7/QVZPA=',\n",
       "   'x-amz-request-id': '2159T8PH6PPXDCGW',\n",
       "   'date': 'Mon, 07 Apr 2025 19:58:03 GMT',\n",
       "   'content-type': 'application/xml',\n",
       "   'transfer-encoding': 'chunked',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'Buckets': [{'Name': 'aft-sandbox-540362055257',\n",
       "   'CreationDate': datetime.datetime(2022, 9, 13, 15, 12, 20, tzinfo=tzutc())},\n",
       "  {'Name': 'amplify-citiesindicatorsapi-dev-10508-deployment',\n",
       "   'CreationDate': datetime.datetime(2023, 8, 30, 5, 5, 13, tzinfo=tzutc())},\n",
       "  {'Name': 'cities-dev-sandbox',\n",
       "   'CreationDate': datetime.datetime(2025, 2, 7, 23, 18, 12, tzinfo=tzutc())},\n",
       "  {'Name': 'cities-heat',\n",
       "   'CreationDate': datetime.datetime(2023, 6, 1, 13, 22, 1, tzinfo=tzutc())},\n",
       "  {'Name': 'cities-tcm-dev-sandbox',\n",
       "   'CreationDate': datetime.datetime(2025, 4, 5, 6, 49, 50, tzinfo=tzutc())},\n",
       "  {'Name': 'era5-brazil',\n",
       "   'CreationDate': datetime.datetime(2025, 2, 15, 19, 51, 14, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-athena-us-west-2',\n",
       "   'CreationDate': datetime.datetime(2024, 1, 12, 18, 45, 11, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-climate-hazards',\n",
       "   'CreationDate': datetime.datetime(2024, 1, 3, 16, 57, 31, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-data-api',\n",
       "   'CreationDate': datetime.datetime(2024, 7, 16, 8, 53, 31, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-heat',\n",
       "   'CreationDate': datetime.datetime(2024, 3, 25, 15, 46, 55, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-indicators',\n",
       "   'CreationDate': datetime.datetime(2024, 5, 13, 15, 50, 58, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-sandbox',\n",
       "   'CreationDate': datetime.datetime(2024, 7, 27, 0, 51, 38, tzinfo=tzutc())}],\n",
       " 'Owner': {'DisplayName': 'aws-cities',\n",
       "  'ID': 'df12253943982d72f60594f06c2cacf9a1ee3a9e738c1649c9fb96e5127f1a5c'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check s3 connection using AWS_PROFILE=CitiesUserPermissionSet profile \n",
    "import boto3\n",
    "\n",
    "session = boto3.Session(profile_name=AWS_PROFILE)\n",
    "s3 = session.client('s3')\n",
    "\n",
    "# export CitiesUserPermissionSet profile to use in the next cells\n",
    "import os\n",
    "os.environ['AWS_PROFILE'] = AWS_PROFILE\n",
    "\n",
    "\n",
    "s3.list_buckets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-07 14:58:06,792][INFO    ][coiled] Fetching latest package priorities...\n",
      "[2025-04-07 14:58:06,793][INFO    ][coiled.package_sync] Resolving your local subdivisions2 Python environment...\n",
      "[2025-04-07 14:58:08,105][INFO    ][coiled.package_sync] Scanning 444 conda packages...\n",
      "[2025-04-07 14:58:08,115][INFO    ][coiled.package_sync] Scanning 259 python packages...\n",
      "[2025-04-07 14:58:09,660][INFO    ][coiled] Running pip check...\n",
      "[2025-04-07 14:58:11,787][INFO    ][coiled] Validating environment...\n",
      "[2025-04-07 14:58:14,481][INFO    ][coiled] Creating wheel for ~/Documents/Identifying Land Subdivisions/identifyingLandSubdivisions...\n",
      "[2025-04-07 14:58:14,842][WARNING ][coiled.package_sync] Package - libopenvino-intel-cpu-plugin, libopenvino-intel-cpu-plugin~=2025.0.0 has no install candidate for Python 3.12 linux-aarch64 on conda-forge\n",
      "[2025-04-07 14:58:14,844][INFO    ][coiled] Uploading coiled_local_identifyingLandSubdivisions...\n",
      "[2025-04-07 14:58:15,891][INFO    ][coiled] Requesting package sync build...\n",
      "[2025-04-07 14:58:17,052][INFO    ][coiled] Creating Cluster (name: ils-sara, https://cloud.coiled.io/clusters/826088?account=wri-cities-data ). This usually takes 1-2 minutes...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started a new Dask client on Coiled. Dashboard is available at https://cluster-xoogn.dask.host/65rIFtlFiOWat1y5/status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in callback None()\n",
      "handle: <Handle cancelled>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 1367, in _do_ssl_handshake\n",
      "    self.socket.do_handshake()\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/ssl.py\", line 1319, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "ssl.SSLEOFError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1010)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 202, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 691, in _handle_events\n",
      "    self._handle_read()\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 1427, in _handle_read\n",
      "    self._do_ssl_handshake()\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 1376, in _do_ssl_handshake\n",
      "    return self.close(exc_info=err)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 606, in close\n",
      "    self._signal_closed()\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 636, in _signal_closed\n",
      "    self._ssl_connect_future.exception()\n",
      "asyncio.exceptions.CancelledError\n",
      "Exception in callback None()\n",
      "handle: <Handle cancelled>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 1367, in _do_ssl_handshake\n",
      "    self.socket.do_handshake()\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/ssl.py\", line 1319, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "ssl.SSLEOFError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1010)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 202, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 691, in _handle_events\n",
      "    self._handle_read()\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 1427, in _handle_read\n",
      "    self._do_ssl_handshake()\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 1376, in _do_ssl_handshake\n",
      "    return self.close(exc_info=err)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 606, in close\n",
      "    self._signal_closed()\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 636, in _signal_closed\n",
      "    self._ssl_connect_future.exception()\n",
      "asyncio.exceptions.CancelledError\n",
      "Exception in callback None()\n",
      "handle: <Handle cancelled>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 1367, in _do_ssl_handshake\n",
      "    self.socket.do_handshake()\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/ssl.py\", line 1319, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "ssl.SSLEOFError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1010)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 202, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 691, in _handle_events\n",
      "    self._handle_read()\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 1427, in _handle_read\n",
      "    self._do_ssl_handshake()\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 1376, in _do_ssl_handshake\n",
      "    return self.close(exc_info=err)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 606, in close\n",
      "    self._signal_closed()\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 636, in _signal_closed\n",
      "    self._ssl_connect_future.exception()\n",
      "asyncio.exceptions.CancelledError\n",
      "2025-04-07 15:25:09,473 - distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client\n"
     ]
    }
   ],
   "source": [
    "import coiled\n",
    "\n",
    "cluster = coiled.Cluster(\n",
    "    workspace=\"wri-cities-data\",\n",
    "    name=f'ils-{YOUR_NAME}',\n",
    "    region=\"us-west-2\",\n",
    "    arm=True,\n",
    "    worker_vm_types=\"r8g.xlarge\",\n",
    "    spot_policy=\"spot\",\n",
    "    n_workers=8,\n",
    "    package_sync_ignore=[\"pyspark\", \"pypandoc\"]\n",
    ")\n",
    "client = cluster.get_client()\n",
    "\n",
    "print(f\"Started a new Dask client on Coiled. Dashboard is available at {client.dashboard_link}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_geopandas as dgpd\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from dask import delayed, compute, visualize\n",
    "import geopandas as gpd\n",
    "from dask.diagnostics import ProgressBar\n",
    "from citywide_calculation import get_utm_crs\n",
    "from metrics_calculation import calculate_minimum_distance_to_roads_option_B\n",
    "from shapely.geometry import MultiLineString, LineString, Point\n",
    "from shapely.ops import polygonize, nearest_points\n",
    "#from shapely.geometry import Polygon, LineString, Point, MultiPolygon, MultiLineString, GeometryCollection\n",
    "from scipy.optimize import fminbound, minimize\n",
    "from metrics_groupby import metrics\n",
    "\n",
    "from pre_processing import *\n",
    "from auxiliary_functions import *\n",
    "from standardize_metrics import *\n",
    "\n",
    "YOUR_NAME = 'sara'\n",
    "grid_size = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@delayed\n",
    "def building_and_intersection_metrics(city_name):\n",
    "    grid_cell_count = 0\n",
    "    paths = {\n",
    "        'grid': f'{GRIDS_PATH}/{city_name}/{city_name}_{str(grid_size)}m_grid.geoparquet',\n",
    "        'buildings': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}.geoparquet',\n",
    "        'roads': f'{ROADS_PATH}/{city_name}/{city_name}_OSM_roads.geoparquet',\n",
    "        'intersections': f'{INTERSECTIONS_PATH}/{city_name}/{city_name}_OSM_intersections.geoparquet'\n",
    "    }\n",
    "    # Get EPSG\n",
    "    epsg = get_epsg(city_name).compute()\n",
    "    # Load grid\n",
    "    grid = load_dataset(paths['grid'], epsg=epsg)#.compute()\n",
    "    if 'geom' in grid.columns:\n",
    "        grid = grid.drop(columns=['geom'])\n",
    "    grid['cell_area'] = grid.geometry.area\n",
    "\n",
    "    cells = grid.index.size\n",
    "    grid_cell_count += cells\n",
    "\n",
    "    # Load buildings and perform relevant calculations on it\n",
    "    buildings = load_dataset(paths['buildings'], epsg=epsg)#.compute()\n",
    "    buildings['area'] = buildings.geometry.area\n",
    "    joined_buildings = dgpd.sjoin(buildings, grid, predicate='within')  \n",
    "    counts_buildings = joined_buildings.groupby('index_right').size()\n",
    "    grid['n_buildings'] = grid.index.map(counts_buildings).fillna(0.).astype(int)\n",
    "    built_area_buildings = joined_buildings.groupby('index_right')['area'].sum()\n",
    "    grid['built_area'] = grid.index.map(built_area_buildings).fillna(0.).astype(float)\n",
    "\n",
    "    #total_buildings = row_count(buildings).compute()\n",
    "    #print(total_buildings)\n",
    "    # Load roads\n",
    "    roads = load_dataset(paths['roads'], epsg=epsg)#.compute()\n",
    "    included_road_types = ['trunk','motorway','primary','secondary','tertiary','primary_link','secondary_link','tertiary_link','trunk_link','motorway_link','residential','unclassified','road','living_street']\n",
    "    included_road_types = ['trunk','motorway','primary','secondary','tertiary','primary_link','secondary_link','tertiary_link','trunk_link','motorway_link','residential','unclassified','road','living_street']\n",
    "\n",
    "    def highway_filter(highway_value):\n",
    "        # If highway_value is missing, return False\n",
    "        if pd.isna(highway_value):\n",
    "            return False\n",
    "        # Split the string by commas, and strip any whitespace from each part\n",
    "        types = [part.strip() for part in highway_value.split(',')]\n",
    "        # Return True if any of the types is in our included list\n",
    "        return any(t in included_road_types for t in types)\n",
    "\n",
    "    # Now filter the roads GeoDataFrame:\n",
    "    roads = roads[roads['highway'].apply(highway_filter)]\n",
    "    #roads['highway'][roads['highway'].isin(included_road_types)]\n",
    "    \n",
    "    #road_union = roads.unary_union.compute()\n",
    "    #roads = roads.compute()\n",
    "\n",
    "    # Load intersections\n",
    "    intersections = load_dataset(paths['intersections'], epsg=epsg)#.compute()\n",
    "\n",
    "    intersections_3plus = intersections[intersections.street_count >= 3]\n",
    "    intersections_4way = intersections[intersections.street_count == 4]\n",
    "\n",
    "    grid['cell_area_km2'] = grid['cell_area']/1000000.\n",
    "    \n",
    "    roads_grid_joined = dgpd.sjoin(roads, grid, predicate='intersects')\n",
    "    road_length_km = roads_grid_joined.groupby('index_right')['length'].sum()/1000.\n",
    "    grid['road_length'] = grid.index.map(road_length_km).fillna(0.).astype(float)\n",
    "\n",
    "\n",
    "    joined_intersections_3plus = dgpd.sjoin(intersections_3plus, grid, predicate='intersects')\n",
    "    counts_intersections_3plus = joined_intersections_3plus.groupby('index_right').size()\n",
    "    grid['intersections_3plus'] = grid.index.map(counts_intersections_3plus).fillna(0.).astype(float)\n",
    "\n",
    "    joined_intersections_4way = dgpd.sjoin(intersections_4way, grid, predicate='intersects')\n",
    "    counts_intersections_4way = joined_intersections_4way.groupby('index_right').size()\n",
    "    grid['intersections_4way'] = grid.index.map(counts_intersections_4way).fillna(0.).astype(float) # OJO: NEED TO CHANGE NA\n",
    "\n",
    "\n",
    "    grid['m3_raw'] = grid['road_length']/grid['cell_area_km2']\n",
    "    grid['m3_raw'] = grid['m3_raw'].fillna(0.)\n",
    "    grid['m3_std'] = grid['m3_raw'].map_partitions(standardize_metric_3, meta=('m3', 'float64'))\n",
    "\n",
    "    grid['m4_raw'] = grid['intersections_4way'] / grid['intersections_3plus']\n",
    "    grid['m4_raw'] = grid['m4_raw'].mask((grid['m4_raw'].isna()) & (grid['road_length'] > 0), 0.)\n",
    "    avg_m4 = grid['m4_raw'].mean().compute()\n",
    "    grid['m4_raw'] = grid['m4_raw'].fillna(avg_m4)\n",
    "    grid['m4_std'] = grid['m4_raw'].map_partitions(standardize_metric_4, meta=('m4', 'float64'))\n",
    "\n",
    "\n",
    "    grid['m5_raw'] =  (1000.**2)*(grid['intersections_4way']/grid['cell_area']) #make sure this is equivalent to the meter calculation\n",
    "    grid['m5_raw'] = grid['m5_raw'].fillna(0.)\n",
    "    grid['m5_std'] = grid['m5_raw'].map_partitions(standardize_metric_5, meta=('m5', 'float64'))\n",
    "\n",
    "    \n",
    "\n",
    "    grid['m11_raw'] = 1.0*grid['n_buildings'] / grid['cell_area'] # Building density\n",
    "    grid['m11_raw'] = grid['m11_raw'].fillna(0.)\n",
    "    grid['m11_std'] = grid['m11_raw'].map_partitions(standardize_metric_11, meta=('m11', 'float64'))\n",
    "\n",
    "    grid['m12_raw'] = grid['built_area'] / grid['cell_area'] # Built area share\n",
    "    grid['m12_raw'] = grid['m12_raw'].fillna(0.)\n",
    "    grid['m12_std'] = grid['m12_raw'].map_partitions(standardize_metric_12, meta=('m12', 'float64'))\n",
    "\n",
    "    grid['m13_raw'] = grid['built_area'] / grid['n_buildings'] # Average building area\n",
    "    grid['m13_raw'] = grid['m13_raw'].fillna(0.)\n",
    "    grid['m13_std'] = grid['m13_raw'].map_partitions(standardize_metric_13, meta=('m13', 'float64'))\n",
    "\n",
    "    path = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{str(grid_size)}m_metrics_3_4_5_11_12_13_grid_{YOUR_NAME}.geoparquet'\n",
    "\n",
    "    if 'geom' in grid.columns:\n",
    "        grid = grid.drop(columns='geom')\n",
    "\n",
    "    grid.to_parquet(path)\n",
    "    return grid_cell_count, path\n",
    "\n",
    "\n",
    "@delayed\n",
    "def building_distance_metrics(city_name):\n",
    "\n",
    "     grid_cell_count = 0\n",
    "     paths = {\n",
    "         'grid': f'{GRIDS_PATH}/{city_name}/{city_name}_{str(grid_size)}m_grid.geoparquet',\n",
    "         'buildings': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}.geoparquet',\n",
    "         'buildings_with_distances': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}_with_distances.geoparquet',\n",
    "         'roads': f'{ROADS_PATH}/{city_name}/{city_name}_OSM_roads.geoparquet',\n",
    "         'intersections': f'{INTERSECTIONS_PATH}/{city_name}/{city_name}_OSM_intersections.geoparquet'\n",
    "     }\n",
    "     # Get EPSG\n",
    "     epsg = get_epsg(city_name).compute()\n",
    "     # Load grid\n",
    "     grid = load_dataset(paths['grid'], epsg=epsg)#.compute()\n",
    "     if 'geom' in grid.columns:\n",
    "         grid = grid.drop(columns=['geom'])\n",
    "     \n",
    "     buildings = load_dataset(paths['buildings_with_distances'], epsg=epsg)#.compute()\n",
    "     buildings['distance_to_nearest_road'] = buildings['distance_to_nearest_road'].astype(float)\n",
    "     buildings['area'] = buildings.geometry.area\n",
    "     joined_buildings = dgpd.sjoin(buildings, grid, predicate='intersects')  \n",
    "     counts_buildings = joined_buildings.groupby('index_right').size()\n",
    "     grid['n_buildings'] = grid.index.map(counts_buildings).fillna(0).astype(int)\n",
    "     average_distance = joined_buildings.groupby('index_right')['distance_to_nearest_road'].mean()\n",
    "     grid['average_distance_nearest_building'] = grid.index.map(average_distance).astype(float)#fillna(np.mean(average_distance)).astype(float)\n",
    "    \n",
    "    \n",
    "     buildings_closer_than_20m = buildings[buildings['distance_to_nearest_road'] <= 20]\n",
    "     joined_buildings_closer_than_20m = dgpd.sjoin(buildings_closer_than_20m, grid, predicate='intersects') \n",
    "     n_buildings_closer_than_20m = joined_buildings_closer_than_20m.groupby('index_right').size()\n",
    "     grid['n_buildings_closer_than_20m'] = grid.index.map(n_buildings_closer_than_20m).astype(float)#.fillna(np.mean(n_buildings_closer_than_20m))\n",
    "     grid = grid.assign(\n",
    "    n_buildings_closer_than_20m = grid['n_buildings_closer_than_20m'].mask(\n",
    "        (grid['n_buildings'] > 0) & (grid['n_buildings_closer_than_20m'].isna()),\n",
    "        0))\n",
    "     grid = grid.assign(\n",
    "         m1_raw = grid['n_buildings_closer_than_20m'] / grid['n_buildings']\n",
    "         )\n",
    "     grid['m1_raw'] = grid['m1_raw'].fillna(grid['m1_raw'].mean())#grid['n_buildings_closer_than_20m'] / grid['n_buildings']\n",
    "     grid['m1_std'] = grid['m1_raw'].map_partitions(standardize_metric_1, meta=('m1', 'float64'))\n",
    "     grid['m2_raw'] = grid['average_distance_nearest_building']\n",
    "     grid['m2_raw'] = grid['m2_raw'].fillna(grid['m2_raw'].mean())\n",
    "     grid['m2_std'] = grid['m2_raw'].map_partitions(standardize_metric_2, meta=('m2', 'float64'))\n",
    "     \n",
    "     path = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{str(grid_size)}m_grid_{YOUR_NAME}_metrics_1_2.geoparquet'\n",
    "    \n",
    "     if 'geom' in grid.columns:\n",
    "         grid = grid.drop(columns='geom')\n",
    "    \n",
    "     grid.to_parquet(path)\n",
    "\n",
    "\n",
    "\n",
    "@delayed\n",
    "def compute_m6_m7_m8(city_name):\n",
    "    \"\"\"\n",
    "    Computes:\n",
    "    - M6: KL divergence (building orientation)\n",
    "    - M7: Average block width\n",
    "    - M8: Building density ratio (inner vs. outer buffer)\n",
    "    \"\"\"\n",
    "\n",
    "    epsilon = 0.001\n",
    "    paths = {\n",
    "        'grid': f'{GRIDS_PATH}/{city_name}/{city_name}_{str(grid_size)}m_grid.geoparquet',\n",
    "        'blocks': f'{BLOCKS_PATH}/{city_name}/{city_name}_blocks_{YOUR_NAME}.geoparquet',\n",
    "        'buildings_with_distances': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}_with_distances.geoparquet',\n",
    "        'buildings_with_distances_azimuths': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}_with_distances_and_azimuths.geoparquet',\n",
    "        'buildings_to_blocks':f'{BLOCKS_PATH}/{city_name}/{city_name}_buildings_to_blocks_{YOUR_NAME}.geoparquet'\n",
    "    }\n",
    "\n",
    "    epsg = get_epsg(city_name).compute()\n",
    "    grid = load_dataset(paths['grid'], epsg=epsg)\n",
    "    blocks = load_dataset(paths['blocks'], epsg=epsg).persist()\n",
    "    buildings = load_dataset(paths['buildings_with_distances_azimuths'], epsg=epsg).persist()\n",
    "    buildings['azimuth'] = buildings['azimuth'].map_partitions(pd.to_numeric, errors='coerce')\n",
    "\n",
    "\n",
    "    if 'geom' in grid.columns:\n",
    "        grid = grid.drop(columns=['geom'])\n",
    "    \n",
    "    blocks['block_id'] = blocks.index\n",
    "    blocks['epsilon_buffer'] = blocks['geometry'].buffer(-(1.- epsilon) * blocks['max_radius'])\n",
    "    blocks['width_buffer'] = blocks['geometry'].buffer(-0.2 * blocks['max_radius'])\n",
    "\n",
    "    buildings_blocks = dgpd.sjoin(buildings, blocks, predicate='intersects').persist() #,how='right'\n",
    "    buildings_blocks = buildings_blocks[['block_id', 'geometry', 'epsilon_buffer','width_buffer','azimuth']]\n",
    "    buildings_blocks = buildings_blocks.set_index('block_id').repartition(npartitions=4)\n",
    "\n",
    "    block_grid_overlap = compute_block_grid_weights(blocks, grid)\n",
    "    block_grid_overlap = block_grid_overlap.compute()\n",
    "\n",
    "    # Metric 6\n",
    "    kl_df = compute_block_kl_metrics(buildings_blocks)\n",
    "    m6_grid = aggregate_m6(kl_df.compute(), block_grid_overlap)\n",
    " \n",
    "    # Metric 7\n",
    "    block_grid_overlap['weighted_max_radius'] = (\n",
    "        block_grid_overlap['max_radius'] * block_grid_overlap['area_weight']\n",
    "    )\n",
    "\n",
    "    grid_m7 = block_grid_overlap.groupby('grid_id').agg(\n",
    "        total_weighted_max_radius=('weighted_max_radius', 'sum'),\n",
    "        total_weight=('area_weight', 'sum')\n",
    "    )\n",
    "    grid_m7['m7'] = grid_m7['total_weighted_max_radius'] / grid_m7['total_weight']\n",
    "\n",
    "    # Metric 8\n",
    "    width_buffer_ratios = buildings_blocks.map_partitions(clip_buildings_by_buffer, buffer_type='width_buffer')\n",
    "    epsilon_buffer_ratios = buildings_blocks.map_partitions(clip_buildings_by_buffer, buffer_type='epsilon_buffer')\n",
    "    clipped_buildings_area_to_buffer_ratio = epsilon_buffer_ratios / width_buffer_ratios\n",
    "    clipped_buildings_area_to_buffer_ratio = clipped_buildings_area_to_buffer_ratio.replace([np.inf, -np.inf], np.nan)#.fillna(999)\n",
    "    ratio_df = clipped_buildings_area_to_buffer_ratio.to_frame(name='m8')\n",
    "    blocks_with_m8 = blocks.merge(ratio_df, left_on='block_id', right_index=True, how='left').compute()\n",
    "    block_grid_overlap = block_grid_overlap.merge(blocks_with_m8, how='left',left_on='block_id',right_index=True)\n",
    "    block_grid_overlap['weighted_m8'] = (\n",
    "        block_grid_overlap['m8'] * block_grid_overlap['area_weight']\n",
    "    )\n",
    "    grid_m8 = block_grid_overlap.groupby('grid_id').agg(\n",
    "        total_weighted_m8=('weighted_m8', 'sum'),\n",
    "        total_weight=('area_weight', 'sum')\n",
    "    )\n",
    "    grid_m8['m8'] = grid_m8['total_weighted_m8'] / grid_m8['total_weight']\n",
    "\n",
    "    # Merge all metrics\n",
    "    grid = grid.merge(m6_grid, left_index=True, right_index=True, how='left')\n",
    "    grid = grid.merge(grid_m7[['m7']], left_index=True, right_index=True, how='left')\n",
    "    grid = grid.merge(grid_m8[['m8']], left_index=True, right_index=True, how='left')\n",
    "\n",
    "    '''\n",
    "    # Fill NaNs\n",
    "    '''\n",
    "\n",
    "    grid['m6_raw'] = grid['m6'].fillna(grid['m6'].mean())\n",
    "    grid['m6_std'] = grid['m6_raw'].map_partitions(standardize_metric_6, meta=('m6', 'float64'))\n",
    "\n",
    "\n",
    "    grid['m7_raw'] = grid['m7'].fillna(grid['m7'].max())\n",
    "    grid['m7_std'] = grid['m7_raw'].map_partitions(standardize_metric_7, meta=('m7', 'float64'))\n",
    "    grid['m8_raw'] = grid['m8'].fillna(grid['m8'].max())\n",
    "    grid['m8_std'] = grid['m8_raw'].map_partitions(standardize_metric_8, meta=('m8', 'float64'))\n",
    "    \n",
    "    grid = grid.drop(columns=['m6','m7','m8'])\n",
    "    # Save Output\n",
    "    grid = grid.compute()  \n",
    "    path = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{str(grid_size)}m_grid_{YOUR_NAME}_metrics_6_7_8.geoparquet'\n",
    "    grid.to_parquet(path)\n",
    "    \n",
    "    #path = f'{BLOCKS_PATH}/{city_name}/{city_name}_blocks_with_m8_{YOUR_NAME}.geoparquet'#f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{str(grid_size)}m_grid_{YOUR_NAME}_metrics_6_7_8.geoparquet'\n",
    "    #blocks_with_m8.to_parquet(path)\n",
    "    return  path\n",
    "\n",
    "@delayed\n",
    "def metrics_roads_intersections(city_name):\n",
    "\n",
    "    paths = {\n",
    "    'grid': f'{GRIDS_PATH}/{city_name}/{city_name}_{str(grid_size)}m_grid.geoparquet',\n",
    "    'blocks': f'{BLOCKS_PATH}/{city_name}/{city_name}_blocks_{YOUR_NAME}.geoparquet',\n",
    "    'buildings_with_distances': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}_with_distances.geoparquet',\n",
    "    'roads': f'{ROADS_PATH}/{city_name}/{city_name}_OSM_roads.geoparquet',\n",
    "    'intersections': f'{INTERSECTIONS_PATH}/{city_name}/{city_name}_OSM_intersections.geoparquet'\n",
    "    }\n",
    "\n",
    "    epsg = get_epsg(city_name).compute()\n",
    "    grid = load_dataset(paths['grid'], epsg=epsg)\n",
    "    roads = load_dataset(paths['roads'], epsg=epsg)\n",
    "    intersections = load_dataset(paths['intersections'], epsg=epsg).compute()\n",
    "\n",
    "\n",
    "    included_road_types = ['trunk','motorway','primary','secondary','tertiary','primary_link','secondary_link','tertiary_link','trunk_link','motorway_link','residential','unclassified','road','living_street']\n",
    "    roads['highway'][roads['highway'].isin(included_road_types)]\n",
    "    \n",
    "\n",
    "    if 'geom' in grid.columns:\n",
    "        grid = grid.drop(columns=['geom'])\n",
    "\n",
    "    intersections['osmid'] = intersections['osmid'].astype(int)\n",
    "    intersection_angles = compute_intersection_angles(roads, intersections)\n",
    "    street_count_mapping = intersections.set_index('osmid')['street_count'].to_dict()\n",
    "    intersection_angle_mapping = compute_intersection_mapping(intersection_angles, street_count_mapping)\n",
    "    intersection_angle_mapping = intersection_angle_mapping.compute()  \n",
    "\n",
    "\n",
    "    intersections_with_angles_metric = intersections.merge(\n",
    "        intersection_angle_mapping.rename(\"average_angle\"), left_on=\"osmid\", right_index=True, how=\"left\"\n",
    "    )\n",
    "\n",
    "    joined_intersection_angles_grid = dgpd.sjoin(intersections_with_angles_metric, grid, predicate=\"within\")\n",
    "    average_angle_between_roads = joined_intersection_angles_grid.groupby('index_right')['average_angle'].mean()\n",
    "\n",
    "\n",
    "    roads_with_tortuosity = calculate_tortuosity(roads.compute(), intersections)\n",
    "    joined_tortuosity_grid = dgpd.sjoin(roads_with_tortuosity.set_geometry('road_geometry'), grid, predicate=\"intersects\")\n",
    "    average_tortuosity = joined_tortuosity_grid.groupby('index_right')['tortuosity'].mean()\n",
    "\n",
    "\n",
    "    grid['m9_raw'] = grid.index.map(average_tortuosity).astype(float).fillna(0.).astype(float)\n",
    "    grid['m9_std'] = grid['m9_raw'].map_partitions(standardize_metric_9, meta=('m9', 'float64'))\n",
    "    grid['m10_raw'] = grid.index.map(average_angle_between_roads).fillna(np.mean(average_angle_between_roads)).astype(float)\n",
    "    grid['m10_std'] = grid['m10_raw'].map_partitions(standardize_metric_10, meta=('m10', 'float64'))\n",
    "\n",
    "    path = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{str(grid_size)}m_grid_metrics_9_10_{YOUR_NAME}.geoparquet'\n",
    "\n",
    "    if 'geom' in grid.columns:\n",
    "        grid = grid.drop(columns='geom')\n",
    "\n",
    "    grid.to_parquet(path)\n",
    "\n",
    "    return path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks completed in 94.34 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from dask import compute\n",
    "\n",
    "start_time = time.time()  # Start the timer\n",
    "\n",
    "cities = ['Nairobi']\n",
    "cities = [city.replace(' ', '_') for city in cities]\n",
    "\n",
    "tasks = [building_and_intersection_metrics(city_name) for city_name in cities]\n",
    "tasks.append([building_distance_metrics(city_name) for city_name in cities])\n",
    "tasks.append([compute_m6_m7_m8(city_name) for city_name in cities])\n",
    "tasks.append([metrics_roads_intersections(city_name) for city_name in cities])\n",
    "results = compute(*tasks)  \n",
    "\n",
    "end_time = time.time()  # End the timer\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Tasks completed in {elapsed_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py:3243: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'m1_zero-centered': 'float64', 'm2_zero-centered': 'float64', 'm3_zero-centered': 'float64', 'm4_zero-centered': 'float64', 'm5_zero-centered': 'float64', 'm6_zero-centered': 'float64', 'm7_zero-centered': 'float64', 'm8_zero-centered': 'float64', 'm9_zero-centered': 'float64', 'm10_zero-centered': 'float64', 'm11_zero-centered': 'float64', 'm12_zero-centered': 'float64', 'm13_zero-centered': 'float64'})\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py:3243: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'m1_final': 'float64', 'm2_final': 'float64', 'm3_final': 'float64', 'm4_final': 'float64', 'm5_final': 'float64', 'm6_final': 'float64', 'm7_final': 'float64', 'm8_final': 'float64', 'm9_final': 'float64', 'm10_final': 'float64', 'm11_final': 'float64', 'm12_final': 'float64', 'm13_final': 'float64'})\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    }
   ],
   "source": [
    "import dask_geopandas as dgpd\n",
    "import pandas as pd\n",
    "\n",
    "def consolidate_irregularity_index(city_name):\n",
    "    # Define file paths\n",
    "    path_metrics_most = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{grid_size}m_metrics_3_4_5_11_12_13_grid_{YOUR_NAME}.geoparquet'\n",
    "    path_metrics_distances = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{grid_size}m_grid_{YOUR_NAME}_metrics_1_2.geoparquet'\n",
    "    path_metrics_blocks = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{grid_size}m_grid_{YOUR_NAME}_metrics_6_7_8.geoparquet'\n",
    "    path_metrics_intersections = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{grid_size}m_grid_metrics_9_10_{YOUR_NAME}.geoparquet'\n",
    "    \n",
    "    # Read each file as a Dask GeoDataFrame\n",
    "    grid_most = dgpd.read_parquet(path_metrics_most)\n",
    "    grid_distances = dgpd.read_parquet(path_metrics_distances)[['m1_std','m1_raw','m2_std','m2_raw']]\n",
    "    grid_blocks = dgpd.read_parquet(path_metrics_blocks)[['m6_std','m6_raw','m7_std','m7_raw','m8_std','m8_raw']]\n",
    "    grid_intersections = dgpd.read_parquet(path_metrics_intersections)[['m9_std','m9_raw','m10_std','m10_raw']]\n",
    "    \n",
    "    # Drop duplicate geometry columns from non-base grids.\n",
    "    # Explicitly reassign so the changes persist.\n",
    "    grid_distances = grid_distances.drop(columns=['geometry'], errors='ignore')\n",
    "    grid_blocks    = grid_blocks.drop(columns=['geometry'], errors='ignore')\n",
    "    grid_intersections = grid_intersections.drop(columns=['geometry'], errors='ignore')\n",
    "    \n",
    "    # Merge the files on the index (left join on grid_most)\n",
    "    grid_combined = grid_most.merge(grid_distances, left_index=True, right_index=True, how='left')\n",
    "    grid_combined = grid_combined.merge(grid_blocks, left_index=True, right_index=True, how='left')\n",
    "    grid_combined = grid_combined.merge(grid_intersections, left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    # Process metrics: note that here we assume that standardized metric columns already exist\n",
    "    all_metrics_columns = ['m'+str(x) for x in range(1,14)]\n",
    "    all_metrics_columns_final = ['m'+str(x)+'_final' for x in range(1,14)]\n",
    "    metrics_standardized_names = {col: col+'_std' for col in all_metrics_columns}\n",
    "    \n",
    "    # Create zero-centered copies of the standardized metric columns.\n",
    "    zero_centered_names_list = [col+'_zero-centered' for col in all_metrics_columns]\n",
    "    grid_combined[zero_centered_names_list] = grid_combined[list(metrics_standardized_names.values())].copy() #grid_combined[metrics_standardized_names].copy()\n",
    "    \n",
    "    # Center at zero (subtract mean and divide by std)\n",
    "    grid_combined[zero_centered_names_list] = grid_combined[zero_centered_names_list].apply(\n",
    "        lambda x: (x - x.mean()) / x.std(), axis=1\n",
    "    )\n",
    "    \n",
    "    # Copy the zero-centered values to the base metric columns\n",
    "    grid_combined[all_metrics_columns_final] = grid_combined[zero_centered_names_list].copy()\n",
    "    \n",
    "    # Rescale metrics to [0,1] using minâ€“max normalization.\n",
    "    grid_combined[all_metrics_columns_final] = grid_combined[all_metrics_columns_final].apply(\n",
    "        lambda x: (x - x.min()) / (x.max() - x.min()), axis=1\n",
    "    )\n",
    "    \n",
    "    # Calculate equal-weights irregularity index as the mean across all metrics.\n",
    "    # (Using axis=1 because we average across columns.)\n",
    "    grid_combined['regularity_index'] = grid_combined[all_metrics_columns_final].mean(axis=1)\n",
    "\n",
    "    # Add a column that counts the number of NA values per row among the 13 final metric columns.\n",
    "    grid_combined['na_count'] = grid_combined[all_metrics_columns_final].isna().sum(axis=1)\n",
    "    \n",
    "    \n",
    "    # Define output path and save the merged grid\n",
    "    out_path = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{grid_size}m_all_metrics_combined_{YOUR_NAME}.geoparquet'\n",
    "    #grid_combined.to_parquet(out_path)\n",
    "    \n",
    "    return grid_combined\n",
    "\n",
    "\n",
    "grid_combined = consolidate_irregularity_index('Nairobi')\n",
    "grid_combined = grid_combined.compute()\n",
    "#print(\"Combined grid saved to:\", consolidated_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    47144.000000\n",
       "mean         0.297990\n",
       "std          0.110996\n",
       "min          0.092473\n",
       "25%          0.204986\n",
       "50%          0.286641\n",
       "75%          0.372654\n",
       "max          0.833333\n",
       "Name: regularity_index, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_combined.regularity_index.describe() #[['m1_raw','m2_raw','regularity_index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExpired",
     "evalue": "[Errno 16] The remote file corresponding to filename wri-cities-sandbox/identifyingLandSubdivisions/data/output/raster/Nairobi/Nairobi_200m_grid_sara_metrics_1_2.geoparquet/part.0.parquet and Etag \"b8bff194a4c9d0eb9ac26c27a854b612\" no longer exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/s3fs/core.py:114\u001b[0m, in \u001b[0;36m_error_wrapper\u001b[0;34m(func, args, kwargs, retries)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m S3_RETRYABLE_ERRORS \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/aiobotocore/client.py:412\u001b[0m, in \u001b[0;36mAioBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    411\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 412\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (PreconditionFailed) when calling the GetObject operation: At least one of the pre-conditions you specified did not hold",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/s3fs/core.py:2359\u001b[0m, in \u001b[0;36mS3File._fetch_range\u001b[0;34m(self, start, end)\u001b[0m\n\u001b[1;32m   2358\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_fetch_range\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2360\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2361\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2362\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2363\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2364\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2365\u001b[0m \u001b[43m        \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreq_kw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreq_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2367\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2369\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/s3fs/core.py:2531\u001b[0m, in \u001b[0;36m_fetch_range\u001b[0;34m(fs, bucket, key, version_id, start, end, req_kw)\u001b[0m\n\u001b[1;32m   2530\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetch: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, bucket, key, start, end)\n\u001b[0;32m-> 2531\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_inner_fetch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/fsspec/asyn.py:103\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_result, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m return_result\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/fsspec/asyn.py:56\u001b[0m, in \u001b[0;36m_runner\u001b[0;34m(event, coro, result, timeout)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m coro\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/s3fs/core.py:2549\u001b[0m, in \u001b[0;36m_inner_fetch\u001b[0;34m(fs, bucket, key, version_id, start, end, req_kw)\u001b[0m\n\u001b[1;32m   2547\u001b[0m         resp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBody\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 2549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _error_wrapper(_call_and_read, retries\u001b[38;5;241m=\u001b[39mfs\u001b[38;5;241m.\u001b[39mretries)\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/s3fs/core.py:146\u001b[0m, in \u001b[0;36m_error_wrapper\u001b[0;34m(func, args, kwargs, retries)\u001b[0m\n\u001b[1;32m    145\u001b[0m err \u001b[38;5;241m=\u001b[39m translate_boto_error(err)\n\u001b[0;32m--> 146\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/s3fs/core.py:114\u001b[0m, in \u001b[0;36m_error_wrapper\u001b[0;34m(func, args, kwargs, retries)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m S3_RETRYABLE_ERRORS \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/s3fs/core.py:2536\u001b[0m, in \u001b[0;36m_inner_fetch.<locals>._call_and_read\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2535\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_call_and_read\u001b[39m():\n\u001b[0;32m-> 2536\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m fs\u001b[38;5;241m.\u001b[39m_call_s3(\n\u001b[1;32m   2537\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_object\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2538\u001b[0m         Bucket\u001b[38;5;241m=\u001b[39mbucket,\n\u001b[1;32m   2539\u001b[0m         Key\u001b[38;5;241m=\u001b[39mkey,\n\u001b[1;32m   2540\u001b[0m         Range\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbytes=\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (start, end \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m   2541\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mversion_id_kw(version_id),\n\u001b[1;32m   2542\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mreq_kw,\n\u001b[1;32m   2543\u001b[0m     )\n\u001b[1;32m   2544\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/s3fs/core.py:371\u001b[0m, in \u001b[0;36mS3FileSystem._call_s3\u001b[0;34m(self, method, *akwarglist, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m additional_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_s3_method_kwargs(method, \u001b[38;5;241m*\u001b[39makwarglist, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _error_wrapper(\n\u001b[1;32m    372\u001b[0m     method, kwargs\u001b[38;5;241m=\u001b[39madditional_kwargs, retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretries\n\u001b[1;32m    373\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/s3fs/core.py:146\u001b[0m, in \u001b[0;36m_error_wrapper\u001b[0;34m(func, args, kwargs, retries)\u001b[0m\n\u001b[1;32m    145\u001b[0m err \u001b[38;5;241m=\u001b[39m translate_boto_error(err)\n\u001b[0;32m--> 146\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 22] At least one of the pre-conditions you specified did not hold",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFileExpired\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Read each file as a Dask GeoDataFrame\u001b[39;00m\n\u001b[1;32m      9\u001b[0m grid_most \u001b[38;5;241m=\u001b[39m dgpd\u001b[38;5;241m.\u001b[39mread_parquet(path_metrics_most)\n\u001b[0;32m---> 10\u001b[0m grid_distances \u001b[38;5;241m=\u001b[39m \u001b[43mdgpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_metrics_distances\u001b[49m\u001b[43m)\u001b[49m[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm1\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm1_raw\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm2\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm2_raw\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m     11\u001b[0m grid_blocks \u001b[38;5;241m=\u001b[39m dgpd\u001b[38;5;241m.\u001b[39mread_parquet(path_metrics_blocks)[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm6\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm6_raw\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm7\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm7_raw\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm8\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm8_raw\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m     12\u001b[0m grid_intersections \u001b[38;5;241m=\u001b[39m dgpd\u001b[38;5;241m.\u001b[39mread_parquet(path_metrics_intersections)[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm9\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm9_raw\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm10\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm10_raw\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask_geopandas/io/parquet.py:93\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_parquet\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m read_parquet\n\u001b[0;32m---> 93\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGeoArrowEngine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# check if spatial partitioning information was stored\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     spatial_partitions \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m_meta\u001b[38;5;241m.\u001b[39mattrs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspatial_partitions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/backends.py:149\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m     exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(e)(\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname(func)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod registered to the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m backend.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m     )\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/backends.py:140\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, dispatch_name)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py:5376\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, columns, filters, categories, index, storage_options, dtype_backend, calculate_divisions, ignore_metadata_file, metadata_task_size, split_row_groups, blocksize, aggregate_files, parquet_file_extension, filesystem, engine, arrow_to_pandas, **kwargs)\u001b[0m\n\u001b[1;32m   5353\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   5354\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine is not supported when using the pyarrow filesystem.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5355\u001b[0m         )\n\u001b[1;32m   5357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_collection(\n\u001b[1;32m   5358\u001b[0m         ReadParquetPyarrowFS(\n\u001b[1;32m   5359\u001b[0m             path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5372\u001b[0m         )\n\u001b[1;32m   5373\u001b[0m     )\n\u001b[1;32m   5375\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_collection(\n\u001b[0;32m-> 5376\u001b[0m     \u001b[43mReadParquetFSSpec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5378\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_convert_to_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5379\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcalculate_divisions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcalculate_divisions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_metadata_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_metadata_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata_task_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_task_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5386\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_row_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_row_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5388\u001b[0m \u001b[43m        \u001b[49m\u001b[43maggregate_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregate_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparquet_file_extension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparquet_file_extension\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_set_parquet_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5393\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_series\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5394\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5395\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/_expr.py:59\u001b[0m, in \u001b[0;36mExpr.__new__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m inst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m     58\u001b[0m inst\u001b[38;5;241m.\u001b[39moperands \u001b[38;5;241m=\u001b[39m [_unpack_collections(o) \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m operands]\n\u001b[0;32m---> 59\u001b[0m _name \u001b[38;5;241m=\u001b[39m \u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _name \u001b[38;5;129;01min\u001b[39;00m Expr\u001b[38;5;241m.\u001b[39m_instances:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Expr\u001b[38;5;241m.\u001b[39m_instances[_name]\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/functools.py:998\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, instance, owner)\u001b[0m\n\u001b[1;32m    996\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[1;32m    997\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[0;32m--> 998\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1000\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/dataframe/dask_expr/io/parquet.py:786\u001b[0m, in \u001b[0;36mReadParquet._name\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;129m@cached_property\u001b[39m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_name\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_funcname\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;241m+\u001b[39m _tokenize_deterministic(\n\u001b[0;32m--> 786\u001b[0m             funcname(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)), \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchecksum\u001b[49m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperands[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    787\u001b[0m         )\n\u001b[1;32m    788\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/dataframe/dask_expr/io/parquet.py:792\u001b[0m, in \u001b[0;36mReadParquet.checksum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    791\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchecksum\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 792\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_info\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchecksum\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/dataframe/dask_expr/io/parquet.py:1371\u001b[0m, in \u001b[0;36mReadParquetFSSpec._dataset_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;66;03m# Collect general dataset info\u001b[39;00m\n\u001b[1;32m   1352\u001b[0m args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1353\u001b[0m     paths,\n\u001b[1;32m   1354\u001b[0m     fs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1369\u001b[0m     },\n\u001b[1;32m   1370\u001b[0m )\n\u001b[0;32m-> 1371\u001b[0m dataset_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collect_dataset_info\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m checksum \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1373\u001b[0m files_for_checksum \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/dataframe/io/parquet/arrow.py:992\u001b[0m, in \u001b[0;36mArrowDatasetEngine._collect_dataset_info\u001b[0;34m(cls, paths, fs, categories, index, gather_statistics, filters, split_row_groups, blocksize, aggregate_files, ignore_metadata_file, metadata_task_size, parquet_file_extension, kwargs)\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# Final \"catch-all\" pyarrow.dataset call\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 992\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[43mpa_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_wrapped_fs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_processed_dataset_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;66;03m# Get file_frag sample and extract physical_schema\u001b[39;00m\n\u001b[1;32m    999\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyarrow/dataset.py:785\u001b[0m, in \u001b[0;36mdataset\u001b[0;34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_is_path_like(elem) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m source):\n\u001b[0;32m--> 785\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_filesystem_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(elem, Dataset) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m source):\n\u001b[1;32m    787\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _union_dataset(source, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyarrow/dataset.py:475\u001b[0m, in \u001b[0;36m_filesystem_dataset\u001b[0;34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[0m\n\u001b[1;32m    467\u001b[0m options \u001b[38;5;241m=\u001b[39m FileSystemFactoryOptions(\n\u001b[1;32m    468\u001b[0m     partitioning\u001b[38;5;241m=\u001b[39mpartitioning,\n\u001b[1;32m    469\u001b[0m     partition_base_dir\u001b[38;5;241m=\u001b[39mpartition_base_dir,\n\u001b[1;32m    470\u001b[0m     exclude_invalid_files\u001b[38;5;241m=\u001b[39mexclude_invalid_files,\n\u001b[1;32m    471\u001b[0m     selector_ignore_prefixes\u001b[38;5;241m=\u001b[39mselector_ignore_prefixes\n\u001b[1;32m    472\u001b[0m )\n\u001b[1;32m    473\u001b[0m factory \u001b[38;5;241m=\u001b[39m FileSystemDatasetFactory(fs, paths_or_selector, \u001b[38;5;28mformat\u001b[39m, options)\n\u001b[0;32m--> 475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfactory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyarrow/_dataset.pyx:3025\u001b[0m, in \u001b[0;36mpyarrow._dataset.DatasetFactory.finish\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyarrow/error.pxi:154\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyarrow/error.pxi:88\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/fsspec/spec.py:2083\u001b[0m, in \u001b[0;36mAbstractBufferedFile.read\u001b[0;34m(self, length)\u001b[0m\n\u001b[1;32m   2080\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m length \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   2081\u001b[0m     \u001b[38;5;66;03m# don't even bother calling fetch\u001b[39;00m\n\u001b[1;32m   2082\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 2083\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2085\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m   2086\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m read: \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2087\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2090\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache\u001b[38;5;241m.\u001b[39m_log_stats(),\n\u001b[1;32m   2091\u001b[0m )\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/fsspec/caching.py:249\u001b[0m, in \u001b[0;36mReadAheadCache._fetch\u001b[0;34m(self, start, end)\u001b[0m\n\u001b[1;32m    247\u001b[0m end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, end \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize)\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_requested_bytes \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m end \u001b[38;5;241m-\u001b[39m start\n\u001b[0;32m--> 249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# new block replaces old\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m=\u001b[39m start\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache)\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/s3fs/core.py:2371\u001b[0m, in \u001b[0;36mS3File._fetch_range\u001b[0;34m(self, start, end)\u001b[0m\n\u001b[1;32m   2369\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m   2370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ex\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m errno\u001b[38;5;241m.\u001b[39mEINVAL \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpre-conditions\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ex\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m-> 2371\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m FileExpired(\n\u001b[1;32m   2372\u001b[0m             filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetails[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m], e_tag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetails\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mETag\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2373\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mex\u001b[39;00m\n\u001b[1;32m   2374\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2375\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mFileExpired\u001b[0m: [Errno 16] The remote file corresponding to filename wri-cities-sandbox/identifyingLandSubdivisions/data/output/raster/Nairobi/Nairobi_200m_grid_sara_metrics_1_2.geoparquet/part.0.parquet and Etag \"b8bff194a4c9d0eb9ac26c27a854b612\" no longer exists."
     ]
    }
   ],
   "source": [
    "city_name = 'Nairobi'\n",
    "# Define file paths\n",
    "path_metrics_most = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{grid_size}m_metrics_3_4_5_11_12_13_grid_{YOUR_NAME}.geoparquet'\n",
    "path_metrics_distances = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{grid_size}m_grid_{YOUR_NAME}_metrics_1_2.geoparquet'\n",
    "path_metrics_blocks = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{grid_size}m_grid_{YOUR_NAME}_metrics_6_7_8.geoparquet'\n",
    "path_metrics_intersections = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{grid_size}m_grid_metrics_9_10_{YOUR_NAME}.geoparquet'\n",
    "\n",
    "# Read each file as a Dask GeoDataFrame\n",
    "grid_most = dgpd.read_parquet(path_metrics_most)\n",
    "grid_distances = dgpd.read_parquet(path_metrics_distances)[['m1','m1_raw','m2','m2_raw']]\n",
    "grid_blocks = dgpd.read_parquet(path_metrics_blocks)[['m6','m6_raw','m7','m7_raw','m8','m8_raw']]\n",
    "grid_intersections = dgpd.read_parquet(path_metrics_intersections)[['m9','m9_raw','m10','m10_raw']]\n",
    "\n",
    "# Drop duplicate geometry columns from non-base grids.\n",
    "# Explicitly reassign so the changes persist.\n",
    "grid_distances = grid_distances.drop(columns=['geometry'], errors='ignore')\n",
    "grid_blocks    = grid_blocks.drop(columns=['geometry'], errors='ignore')\n",
    "grid_intersections = grid_intersections.drop(columns=['geometry'], errors='ignore')\n",
    "\n",
    "# Merge the files on the index (left join on grid_most)\n",
    "grid_combined = grid_most.merge(grid_distances, left_index=True, right_index=True, how='left')\n",
    "grid_combined = grid_combined.merge(grid_blocks, left_index=True, right_index=True, how='left')\n",
    "grid_combined = grid_combined.merge(grid_intersections, left_index=True, right_index=True, how='left')\n",
    "\n",
    "# Process metrics: note that here we assume that standardized metric columns already exist\n",
    "all_metrics_columns = ['m'+str(x) for x in range(1,14)]\n",
    "all_metrics_columns_final = ['m'+str(x)+'_final' for x in range(1,14)]\n",
    "metrics_standardized_names = all_metrics_columns#= {col: col+'_standardized' for col in all_metrics_columns}\n",
    "\n",
    "# Create zero-centered copies of the standardized metric columns.\n",
    "zero_centered_names_list = [col+'_zero-centered' for col in all_metrics_columns]\n",
    "grid_combined[zero_centered_names_list] = grid_combined[metrics_standardized_names].copy()#grid_combined[list(metrics_standardized_names.values())].copy()\n",
    "\n",
    "# Center at zero (subtract mean and divide by std)\n",
    "grid_combined[zero_centered_names_list] = grid_combined[zero_centered_names_list].apply(\n",
    "    lambda x: (x - x.mean()) / x.std(), axis=1\n",
    ")\n",
    "\n",
    "# Copy the zero-centered values to the base metric columns\n",
    "grid_combined[all_metrics_columns_final] = grid_combined[zero_centered_names_list].copy()\n",
    "\n",
    "# Rescale metrics to [0,1] using minâ€“max normalization.\n",
    "grid_combined[all_metrics_columns_final] = grid_combined[all_metrics_columns].apply(\n",
    "    lambda x: (x - x.min()) / (x.max() - x.min()), axis=1\n",
    ")\n",
    "\n",
    "# Calculate equal-weights irregularity index as the mean across all metrics.\n",
    "# (Using axis=1 because we average across columns.)\n",
    "grid_combined['regularity_index'] = grid_combined[all_metrics_columns_final].mean(axis=1)\n",
    "\n",
    "# Add a column that counts the number of NA values per row among the 13 final metric columns.\n",
    "grid_combined['na_count'] = grid_combined[all_metrics_columns_final].isna().sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif __name__ == \"__main__\":\\n    import time\\n    start_time = time.time()\\n    city = \\'Nairobi\\'\\n    city = city.replace(\\' \\', \\'_\\')\\n    final_path = calculate_metrics(city)\\n    elapsed_time = time.time() - start_time\\n    print(f\"All metrics computed and saved to {final_path}\")\\n    print(f\"Tasks completed in {elapsed_time:.2f} seconds.\")\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "import dask.dataframe as dd\n",
    "from auxiliary_functions import load_dataset\n",
    "import dask_geopandas as dgpd\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from dask import delayed, compute\n",
    "from citywide_calculation import get_utm_crs\n",
    "from metrics_calculation import calculate_minimum_distance_to_roads_option_B\n",
    "from dask.diagnostics import ProgressBar\n",
    "from shapely.geometry import MultiLineString, LineString, Point\n",
    "from shapely.ops import polygonize, nearest_points\n",
    "from scipy.optimize import fminbound, minimize\n",
    "from metrics_groupby import metrics\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "YOUR_NAME = 'sara'\n",
    "grid_size = 200\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. Load common datasets (for metrics m1, m2, m3, m4, m5, m9, m10, m11, m12, m13)\n",
    "# ------------------------------------------------------------------------------\n",
    "@delayed\n",
    "def load_common_datasets(city_name):\n",
    "    epsg = get_epsg(city_name).compute()\n",
    "    \n",
    "    grid_path = f'{GRIDS_PATH}/{city_name}/{city_name}_{grid_size}m_grid.geoparquet'\n",
    "    grid = load_dataset(grid_path, epsg=epsg)\n",
    "    if 'geom' in grid.columns:\n",
    "        grid = grid.drop(columns=['geom'])\n",
    "    grid['cell_area'] = grid.geometry.area\n",
    "    \n",
    "    buildings_path = f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}.geoparquet'\n",
    "    buildings = load_dataset(buildings_path, epsg=epsg)\n",
    "    \n",
    "    roads_path = f'{ROADS_PATH}/{city_name}/{city_name}_OSM_roads.geoparquet'\n",
    "    roads = load_dataset(roads_path, epsg=epsg)\n",
    "    \n",
    "    intersections_path = f'{INTERSECTIONS_PATH}/{city_name}/{city_name}_OSM_intersections.geoparquet'\n",
    "    intersections = load_dataset(intersections_path, epsg=epsg)\n",
    "    \n",
    "    bld_with_dist_path = f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}_with_distances.geoparquet'\n",
    "    buildings_with_dist = load_dataset(bld_with_dist_path, epsg=epsg)\n",
    "    \n",
    "    return {\n",
    "         'grid': grid,\n",
    "         'buildings': buildings,\n",
    "         'buildings_with_dist': buildings_with_dist,\n",
    "         'roads': roads,\n",
    "         'intersections': intersections,\n",
    "         'epsg': epsg,\n",
    "    }\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. Metric functions using the common datasets\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Metric 1: Ratio of buildings within 20 m to total buildings\n",
    "@delayed\n",
    "def compute_metric_m1(city_name, datasets):\n",
    "    grid = datasets['grid']\n",
    "    buildings = datasets['buildings_with_dist']\n",
    "    # Ensure distances are floats\n",
    "    buildings['distance_to_nearest_road'] = buildings['distance_to_nearest_road'].astype(float)\n",
    "    \n",
    "    joined = dgpd.sjoin(buildings, grid, predicate='intersects')\n",
    "    total_buildings = joined.groupby('index_right').size()\n",
    "    grid = grid.assign(n_buildings=grid.index.map(total_buildings).fillna(0).astype(int))\n",
    "    \n",
    "    buildings_close = buildings[buildings['distance_to_nearest_road'] <= 20]\n",
    "    joined_close = dgpd.sjoin(buildings_close, grid, predicate='intersects')\n",
    "    count_close = joined_close.groupby('index_right').size()\n",
    "    \n",
    "    m1 = grid.index.map(count_close).fillna(0).astype(float) / grid['n_buildings']\n",
    "    return m1\n",
    "\n",
    "# Metric 2: Average distance to the nearest road\n",
    "@delayed\n",
    "def compute_metric_m2(city_name, datasets):\n",
    "    grid = datasets['grid']\n",
    "    buildings = datasets['buildings_with_dist']\n",
    "    joined = dgpd.sjoin(buildings, grid, predicate='intersects')\n",
    "    avg_distance = joined.groupby('index_right')['distance_to_nearest_road'].mean()\n",
    "    m2 = grid.index.map(avg_distance).fillna(0).astype(float)\n",
    "    return m2\n",
    "\n",
    "# Metric 3: Road length density (road length per cell area in kmÂ²)\n",
    "@delayed\n",
    "def compute_metric_m3(city_name, datasets):\n",
    "    grid = datasets['grid']\n",
    "    roads = datasets['roads']\n",
    "    grid = grid.assign(cell_area_km2=grid['cell_area'] / 1e6)\n",
    "    roads_joined = dgpd.sjoin(roads, grid, predicate='within')\n",
    "    road_length_km = roads_joined.groupby('index_right')['length'].sum() / 1000.\n",
    "    m3 = grid.index.map(road_length_km).fillna(0).astype(float) / grid['cell_area_km2']\n",
    "    return m3\n",
    "\n",
    "# Metric 4: Ratio of 4-way intersections to intersections with 3+ roads\n",
    "@delayed\n",
    "def compute_metric_m4(city_name, datasets):\n",
    "    grid = datasets['grid']\n",
    "    intersections = datasets['intersections'].compute()  # force computation if needed\n",
    "    intersections_3plus = intersections[intersections.street_count >= 3]\n",
    "    intersections_4way = intersections[intersections.street_count == 4]\n",
    "    \n",
    "    joined_3plus = dgpd.sjoin(intersections_3plus, grid, predicate='within')\n",
    "    count_3plus = joined_3plus.groupby('index_right').size()\n",
    "    \n",
    "    joined_4way = dgpd.sjoin(intersections_4way, grid, predicate='within')\n",
    "    count_4way = joined_4way.groupby('index_right').size()\n",
    "    \n",
    "    m4 = grid.index.map(count_4way).fillna(0).astype(float) / grid.index.map(count_3plus).fillna(0)\n",
    "    return m4\n",
    "\n",
    "# Metric 5: Intersection density per cell area (using 4-way intersections)\n",
    "@delayed\n",
    "def compute_metric_m5(city_name, datasets):\n",
    "    grid = datasets['grid']\n",
    "    intersections = datasets['intersections'].compute()\n",
    "    count_4way = dgpd.sjoin(intersections[intersections.street_count == 4], grid, predicate='within')\\\n",
    "                    .groupby('index_right').size()\n",
    "    m5 = (1000.**2) * grid.index.map(count_4way).fillna(0).astype(float) / grid['cell_area']\n",
    "    return m5\n",
    "\n",
    "# Metric 11: Building density (number of buildings per cell area)\n",
    "@delayed\n",
    "def compute_metric_m11(city_name, datasets):\n",
    "    grid = datasets['grid']\n",
    "    buildings = datasets['buildings']\n",
    "    buildings = buildings.assign(area=buildings.geometry.area)\n",
    "    joined = dgpd.sjoin(buildings, grid, predicate='within')\n",
    "    count_buildings = joined.groupby('index_right').size()\n",
    "    m11 = grid.index.map(count_buildings).fillna(0).astype(float) / grid['cell_area']\n",
    "    return m11\n",
    "\n",
    "# Metric 12: Built area share (sum of building areas divided by cell area)\n",
    "@delayed\n",
    "def compute_metric_m12(city_name, datasets):\n",
    "    grid = datasets['grid']\n",
    "    buildings = datasets['buildings']\n",
    "    buildings = buildings.assign(area=buildings.geometry.area)\n",
    "    joined = dgpd.sjoin(buildings, grid, predicate='within')\n",
    "    sum_area = joined.groupby('index_right')['area'].sum()\n",
    "    m12 = grid.index.map(sum_area).fillna(0).astype(float) / grid['cell_area']\n",
    "    return m12\n",
    "\n",
    "# Metric 13: Average building area\n",
    "@delayed\n",
    "def compute_metric_m13(city_name, datasets):\n",
    "    grid = datasets['grid']\n",
    "    buildings = datasets['buildings']\n",
    "    buildings = buildings.assign(area=buildings.geometry.area)\n",
    "    joined = dgpd.sjoin(buildings, grid, predicate='within')\n",
    "    avg_area = joined.groupby('index_right')['area'].mean()\n",
    "    m13 = grid.index.map(avg_area).fillna(0).astype(float)\n",
    "    return m13\n",
    "\n",
    "# Metric 9: Average road tortuosity\n",
    "@delayed\n",
    "def compute_metric_m9(city_name, datasets):\n",
    "    grid = datasets['grid']\n",
    "    roads = datasets['roads'].compute()\n",
    "    intersections = datasets['intersections'].compute()\n",
    "    # calculate_tortuosity is assumed to be defined elsewhere.\n",
    "    roads_with_tortuosity = calculate_tortuosity(roads, intersections)\n",
    "    joined_tortuosity = dgpd.sjoin(roads_with_tortuosity.set_geometry('road_geometry'), grid, predicate=\"intersects\")\n",
    "    average_tortuosity = joined_tortuosity.groupby('index_right')['tortuosity'].mean()\n",
    "    m9 = grid.index.map(average_tortuosity).fillna(-999.).astype(float)\n",
    "    return m9\n",
    "\n",
    "# Metric 10: Average intersection angle\n",
    "@delayed\n",
    "def compute_metric_m10(city_name, datasets):\n",
    "    grid = datasets['grid']\n",
    "    intersections = datasets['intersections'].compute()\n",
    "    intersection_angles = compute_intersection_angles(datasets['roads'], intersections)\n",
    "    street_count_mapping = intersections.set_index('osmid')['street_count'].to_dict()\n",
    "    intersection_angle_mapping = compute_intersection_mapping(intersection_angles, street_count_mapping).compute()\n",
    "    intersections_with_angles = intersections.merge(\n",
    "         intersection_angle_mapping.rename(\"average_angle\"),\n",
    "         left_on=\"osmid\", right_index=True, how=\"left\"\n",
    "    )\n",
    "    joined_angles = dgpd.sjoin(intersections_with_angles, grid, predicate=\"within\")\n",
    "    average_angle = joined_angles.groupby('index_right')['average_angle'].mean()\n",
    "    m10 = grid.index.map(average_angle).fillna(-999.).astype(float)\n",
    "    return m10\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3. Metric functions for block-based calculations (m6, m7, m8)\n",
    "# These load additional datasets (blocks, buildings with azimuths) and follow your original logic.\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Metric 6: KL divergence for building orientation\n",
    "@delayed\n",
    "def compute_metric_m6(city_name):\n",
    "    epsilon = 0.001\n",
    "    epsg = get_epsg(city_name).compute()\n",
    "    \n",
    "    grid_path = f'{GRIDS_PATH}/{city_name}/{city_name}_{grid_size}m_grid.geoparquet'\n",
    "    grid = load_dataset(grid_path, epsg=epsg)\n",
    "    if 'geom' in grid.columns:\n",
    "         grid = grid.drop(columns=['geom'])\n",
    "    \n",
    "    blocks_path = f'{BLOCKS_PATH}/{city_name}/{city_name}_blocks_{YOUR_NAME}.geoparquet'\n",
    "    blocks = load_dataset(blocks_path, epsg=epsg).persist()\n",
    "    \n",
    "    buildings_az_path = f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}_with_distances_and_azimuths.geoparquet'\n",
    "    buildings = load_dataset(buildings_az_path, epsg=epsg).persist()\n",
    "    buildings['azimuth'] = buildings['azimuth'].map_partitions(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    blocks['block_id'] = blocks.index\n",
    "    blocks['epsilon_buffer'] = blocks['geometry'].buffer(-(1.- epsilon) * blocks['max_radius'])\n",
    "    blocks['width_buffer'] = blocks['geometry'].buffer(-0.2 * blocks['max_radius'])\n",
    "    \n",
    "    buildings_blocks = dgpd.sjoin(buildings, blocks, predicate='intersects').persist()\n",
    "    buildings_blocks = buildings_blocks[['block_id', 'geometry', 'epsilon_buffer', 'width_buffer', 'azimuth']]\n",
    "    buildings_blocks = buildings_blocks.set_index('block_id').repartition(npartitions=4)\n",
    "    \n",
    "    block_grid_overlap = compute_block_grid_weights(blocks, grid)\n",
    "    block_grid_overlap = block_grid_overlap.compute()\n",
    "    \n",
    "    kl_df = compute_block_kl_metrics(buildings_blocks)\n",
    "    m6_grid = aggregate_m6(kl_df.compute(), block_grid_overlap)\n",
    "    \n",
    "    grid = grid.merge(m6_grid, left_index=True, right_index=True, how='left')\n",
    "    m6 = grid['m6'].fillna(0)\n",
    "    return m6\n",
    "\n",
    "# Metric 7: Average block width\n",
    "@delayed\n",
    "def compute_metric_m7(city_name):\n",
    "    epsilon = 0.001\n",
    "    epsg = get_epsg(city_name).compute()\n",
    "    \n",
    "    grid_path = f'{GRIDS_PATH}/{city_name}/{city_name}_{grid_size}m_grid.geoparquet'\n",
    "    grid = load_dataset(grid_path, epsg=epsg)\n",
    "    if 'geom' in grid.columns:\n",
    "         grid = grid.drop(columns=['geom'])\n",
    "    \n",
    "    blocks_path = f'{BLOCKS_PATH}/{city_name}/{city_name}_blocks_{YOUR_NAME}.geoparquet'\n",
    "    blocks = load_dataset(blocks_path, epsg=epsg).persist()\n",
    "    \n",
    "    block_grid_overlap = compute_block_grid_weights(blocks, grid)\n",
    "    block_grid_overlap = block_grid_overlap.compute()\n",
    "    block_grid_overlap['weighted_max_radius'] = block_grid_overlap['max_radius'] * block_grid_overlap['area_weight']\n",
    "    \n",
    "    grid_m7 = block_grid_overlap.groupby('grid_id').agg(\n",
    "         total_weighted_max_radius=('weighted_max_radius', 'sum'),\n",
    "         total_weight=('area_weight', 'sum')\n",
    "    )\n",
    "    grid_m7['m7'] = grid_m7['total_weighted_max_radius'] / grid_m7['total_weight']\n",
    "    \n",
    "    grid = grid.merge(grid_m7[['m7']], left_index=True, right_index=True, how='left')\n",
    "    m7 = grid['m7'].fillna(0)\n",
    "    return m7\n",
    "\n",
    "# Metric 8: Building density ratio (inner vs. outer buffer)\n",
    "@delayed\n",
    "def compute_metric_m8(city_name):\n",
    "    epsilon = 0.001\n",
    "    epsg = get_epsg(city_name).compute()\n",
    "    \n",
    "    grid_path = f'{GRIDS_PATH}/{city_name}/{city_name}_{grid_size}m_grid.geoparquet'\n",
    "    grid = load_dataset(grid_path, epsg=epsg)\n",
    "    if 'geom' in grid.columns:\n",
    "         grid = grid.drop(columns=['geom'])\n",
    "    \n",
    "    blocks_path = f'{BLOCKS_PATH}/{city_name}/{city_name}_blocks_{YOUR_NAME}.geoparquet'\n",
    "    blocks = load_dataset(blocks_path, epsg=epsg).persist()\n",
    "    \n",
    "    buildings_az_path = f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}_with_distances_and_azimuths.geoparquet'\n",
    "    buildings = load_dataset(buildings_az_path, epsg=epsg).persist()\n",
    "    buildings['azimuth'] = buildings['azimuth'].map_partitions(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    blocks['block_id'] = blocks.index\n",
    "    blocks['epsilon_buffer'] = blocks['geometry'].buffer(-(1.- epsilon) * blocks['max_radius'])\n",
    "    blocks['width_buffer'] = blocks['geometry'].buffer(-0.2 * blocks['max_radius'])\n",
    "    \n",
    "    buildings_blocks = dgpd.sjoin(buildings, blocks, predicate='intersects').persist()\n",
    "    buildings_blocks = buildings_blocks[['block_id', 'geometry', 'epsilon_buffer', 'width_buffer', 'azimuth']]\n",
    "    buildings_blocks = buildings_blocks.set_index('block_id').repartition(npartitions=4)\n",
    "    \n",
    "    block_grid_overlap = compute_block_grid_weights(blocks, grid)\n",
    "    block_grid_overlap = block_grid_overlap.compute()\n",
    "    \n",
    "    width_buffer_ratios = buildings_blocks.map_partitions(clip_buildings_by_buffer, buffer_type='width_buffer')\n",
    "    epsilon_buffer_ratios = buildings_blocks.map_partitions(clip_buildings_by_buffer, buffer_type='epsilon_buffer')\n",
    "    clipped_buildings_area_to_buffer_ratio = epsilon_buffer_ratios / width_buffer_ratios\n",
    "    clipped_buildings_area_to_buffer_ratio = clipped_buildings_area_to_buffer_ratio.replace([np.inf, -np.inf], np.nan).fillna(999)\n",
    "    ratio_df = clipped_buildings_area_to_buffer_ratio.to_frame(name='m8')\n",
    "    \n",
    "    blocks_with_m8 = blocks.merge(ratio_df, left_on='block_id', right_index=True, how='left').compute()\n",
    "    block_grid_overlap = block_grid_overlap.merge(blocks_with_m8, how='left', left_on='block_id', right_index=True)\n",
    "    block_grid_overlap['weighted_m8'] = block_grid_overlap['m8'] * block_grid_overlap['area_weight']\n",
    "    \n",
    "    grid_m8 = block_grid_overlap.groupby('grid_id').agg(\n",
    "         total_weighted_m8=('weighted_m8', 'sum'),\n",
    "         total_weight=('area_weight', 'sum')\n",
    "    )\n",
    "    grid_m8['m8'] = grid_m8['total_weighted_m8'] / grid_m8['total_weight']\n",
    "    \n",
    "    grid = grid.merge(grid_m8[['m8']], left_index=True, right_index=True, how='left')\n",
    "    m8 = grid['m8'].fillna(-999.)\n",
    "    return m8\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4. Merge all metrics into the grid and save\n",
    "# ------------------------------------------------------------------------------\n",
    "def calculate_metrics(city_name):\n",
    "    # Load common datasets (for m1, m2, m3, m4, m5, m9, m10, m11, m12, m13)\n",
    "    datasets = load_common_datasets(city_name)\n",
    "    \n",
    "    # Compute metrics that use the common datasets\n",
    "    m1   = compute_metric_m1(city_name, datasets)\n",
    "    m2   = compute_metric_m2(city_name, datasets)\n",
    "    m3   = compute_metric_m3(city_name, datasets)\n",
    "    m4   = compute_metric_m4(city_name, datasets)\n",
    "    m5   = compute_metric_m5(city_name, datasets)\n",
    "    m11  = compute_metric_m11(city_name, datasets)\n",
    "    m12  = compute_metric_m12(city_name, datasets)\n",
    "    m13  = compute_metric_m13(city_name, datasets)\n",
    "    m9   = compute_metric_m9(city_name, datasets)\n",
    "    m10  = compute_metric_m10(city_name, datasets)\n",
    "    \n",
    "    # Compute block-based metrics (m6, m7, m8) which load their own extra datasets\n",
    "    m6   = compute_metric_m6(city_name)\n",
    "    m7   = compute_metric_m7(city_name)\n",
    "    m8   = compute_metric_m8(city_name)\n",
    "    \n",
    "    @delayed\n",
    "    def merge_metrics(grid, m1, m2, m3, m4, m5, m6, m7, m8, m9, m10, m11, m12, m13):\n",
    "         grid['m1'] = m1\n",
    "         grid['m2'] = m2\n",
    "         grid['m3'] = m3\n",
    "         grid['m4'] = m4\n",
    "         grid['m5'] = m5\n",
    "         grid['m6'] = m6\n",
    "         grid['m7'] = m7\n",
    "         grid['m8'] = m8\n",
    "         grid['m9'] = m9\n",
    "         grid['m10'] = m10\n",
    "         grid['m11'] = m11\n",
    "         grid['m12'] = m12\n",
    "         grid['m13'] = m13\n",
    "         return grid\n",
    "    \n",
    "    final_grid = merge_metrics(datasets['grid'], m1, m2, m3, m4, m5, m6, m7, m8, m9, m10, m11, m12, m13)\n",
    "    \n",
    "    @delayed\n",
    "    def save_grid(grid, path):\n",
    "         if 'geom' in grid.columns:\n",
    "              grid = grid.drop(columns=['geom'])\n",
    "         grid.to_parquet(path)\n",
    "         return path\n",
    "    \n",
    "    out_path = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{grid_size}m_grid_all_metrics_{YOUR_NAME}.geoparquet'\n",
    "    saved = save_grid(final_grid, out_path)\n",
    "    result = compute(saved)\n",
    "    return result[0]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 5. Example usage\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    city = 'Nairobi'\n",
    "    city = city.replace(' ', '_')\n",
    "    final_path = calculate_metrics(city)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"All metrics computed and saved to {final_path}\")\n",
    "    print(f\"Tasks completed in {elapsed_time:.2f} seconds.\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['block_id'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m city \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNairobi\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m city \u001b[38;5;241m=\u001b[39m city\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m final_path \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll metrics computed and saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 359\u001b[0m, in \u001b[0;36mcalculate_metrics\u001b[0;34m(city_name)\u001b[0m\n\u001b[1;32m    357\u001b[0m out_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUTPUT_PATH_RASTER\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcity_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcity_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mm_grid_all_metrics_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mYOUR_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.geoparquet\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    358\u001b[0m saved \u001b[38;5;241m=\u001b[39m save_grid(final_grid, out_path)\n\u001b[0;32m--> 359\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msaved\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/base.py:662\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    659\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 662\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "Cell \u001b[0;32mIn[7], line 244\u001b[0m, in \u001b[0;36mcompute_metric_m7\u001b[0;34m()\u001b[0m\n\u001b[1;32m    241\u001b[0m blocks_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBLOCKS_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcity_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcity_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_blocks_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mYOUR_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.geoparquet\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    242\u001b[0m blocks \u001b[38;5;241m=\u001b[39m load_dataset(blocks_path, epsg\u001b[38;5;241m=\u001b[39mepsg)\u001b[38;5;241m.\u001b[39mpersist()\n\u001b[0;32m--> 244\u001b[0m block_grid_overlap \u001b[38;5;241m=\u001b[39m compute_block_grid_weights(blocks, grid)\n\u001b[1;32m    245\u001b[0m block_grid_overlap \u001b[38;5;241m=\u001b[39m block_grid_overlap\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[1;32m    246\u001b[0m block_grid_overlap[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted_max_radius\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m block_grid_overlap[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_radius\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m block_grid_overlap[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marea_weight\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/auxiliary_functions.py:286\u001b[0m, in \u001b[0;36mcompute_block_grid_weights\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/dask_geopandas/expr.py:660\u001b[0m, in \u001b[0;36m__getitem__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py:421\u001b[0m, in \u001b[0;36m__getitem__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/dask/_collections.py:8\u001b[0m, in \u001b[0;36mnew_collection\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/functools.py:998\u001b[0m, in \u001b[0;36m__get__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/dask/dataframe/dask_expr/_expr.py:2131\u001b[0m, in \u001b[0;36m_meta\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/functools.py:998\u001b[0m, in \u001b[0;36m__get__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/dask/dataframe/dask_expr/_expr.py:567\u001b[0m, in \u001b[0;36m_meta\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/geopandas/geodataframe.py:1750\u001b[0m, in \u001b[0;36m__getitem__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36m_raise_if_missing\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['block_id'] not in index\""
     ]
    }
   ],
   "source": [
    "'''\n",
    "import time\n",
    "start_time = time.time()\n",
    "city = 'Nairobi'\n",
    "city = city.replace(' ', '_')\n",
    "final_path = calculate_metrics(city)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"All metrics computed and saved to {final_path}\")\n",
    "print(f\"Tasks completed in {elapsed_time:.2f} seconds.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subdivisions2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
