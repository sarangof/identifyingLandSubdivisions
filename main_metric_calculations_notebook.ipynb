{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_NAME = 'sara'\n",
    "\n",
    "AWS_PROFILE = 'cities'\n",
    "\n",
    "'''\n",
    "# List of cities to process\n",
    "cities = [\"Belo Horizonte\", \"Campinas\"]#, \"Bogota\", \"Nairobi\", \"Bamako\", \n",
    "        #\"Lagos\", \"Accra\", \"Abidjan\", \"Mogadishu\", \"Cape Town\", \n",
    "        #\"Maputo\", \"Luanda\"]\n",
    "\n",
    "test_cities = [\"Belo Horizonte\"]\n",
    "#cities = test_cities\n",
    "\n",
    "cities = [city.replace(' ', '_') for city in cities]\n",
    "\n",
    "search_buffer_files = fs.ls(SEARCH_BUFFER_PATH)\n",
    "\n",
    "cities \n",
    "\n",
    "number_of_cities = len(cities)\n",
    "\n",
    "print(f'City count: {number_of_cities}')\n",
    "'''\n",
    "grid_size = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = \"s3://wri-cities-sandbox/identifyingLandSubdivisions/data\"\n",
    "INPUT_PATH = f'{MAIN_PATH}/input'\n",
    "CITY_INFO_PATH = f'{INPUT_PATH}/city_info'\n",
    "EXTENTS_PATH = f'{CITY_INFO_PATH}/extents'\n",
    "BUILDINGS_PATH = f'{INPUT_PATH}/buildings'\n",
    "BLOCKS_PATH = f'{INPUT_PATH}/blocks'\n",
    "ROADS_PATH = f'{INPUT_PATH}/roads'\n",
    "INTERSECTIONS_PATH = f'{INPUT_PATH}/intersections'\n",
    "GRIDS_PATH = f'{INPUT_PATH}/city_info/grids'\n",
    "SEARCH_BUFFER_PATH = f'{INPUT_PATH}/city_info/search_buffers'\n",
    "OUTPUT_PATH = f'{MAIN_PATH}/output'\n",
    "OUTPUT_PATH_CSV = f'{OUTPUT_PATH}/csv'\n",
    "OUTPUT_PATH_RASTER = f'{OUTPUT_PATH}/raster'\n",
    "OUTPUT_PATH_PNG = f'{OUTPUT_PATH}/png'\n",
    "OUTPUT_PATH_RAW = f'{OUTPUT_PATH}/raw_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'H0TKAGET3ZH984MA',\n",
       "  'HostId': 'jZrK3YSkjgx3GsyAb6X/p5EO4Avk407kTfL6F0LvL1et4WE0+flqOLYAWkJ1VT/n3iaZA+BFzS4=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'jZrK3YSkjgx3GsyAb6X/p5EO4Avk407kTfL6F0LvL1et4WE0+flqOLYAWkJ1VT/n3iaZA+BFzS4=',\n",
       "   'x-amz-request-id': 'H0TKAGET3ZH984MA',\n",
       "   'date': 'Tue, 06 May 2025 01:02:24 GMT',\n",
       "   'content-type': 'application/xml',\n",
       "   'transfer-encoding': 'chunked',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'Buckets': [{'Name': '540362055257-storage-class-analysis',\n",
       "   'CreationDate': datetime.datetime(2025, 4, 30, 16, 32, 6, tzinfo=tzutc())},\n",
       "  {'Name': 'aft-sandbox-540362055257',\n",
       "   'CreationDate': datetime.datetime(2022, 9, 13, 15, 12, 20, tzinfo=tzutc())},\n",
       "  {'Name': 'amplify-citiesindicatorsapi-dev-10508-deployment',\n",
       "   'CreationDate': datetime.datetime(2023, 8, 30, 5, 5, 13, tzinfo=tzutc())},\n",
       "  {'Name': 'cities-heat',\n",
       "   'CreationDate': datetime.datetime(2023, 6, 1, 13, 22, 1, tzinfo=tzutc())},\n",
       "  {'Name': 'cities-test-sandbox',\n",
       "   'CreationDate': datetime.datetime(2025, 4, 18, 19, 10, 49, tzinfo=tzutc())},\n",
       "  {'Name': 'era5-brazil',\n",
       "   'CreationDate': datetime.datetime(2025, 2, 15, 19, 51, 14, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-athena-us-west-2',\n",
       "   'CreationDate': datetime.datetime(2024, 1, 12, 18, 45, 11, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-climate-hazards',\n",
       "   'CreationDate': datetime.datetime(2024, 1, 3, 16, 57, 31, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-data-api',\n",
       "   'CreationDate': datetime.datetime(2024, 7, 16, 8, 53, 31, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-heat',\n",
       "   'CreationDate': datetime.datetime(2024, 3, 25, 15, 46, 55, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-indicators',\n",
       "   'CreationDate': datetime.datetime(2024, 5, 13, 15, 50, 58, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-sandbox',\n",
       "   'CreationDate': datetime.datetime(2024, 7, 27, 0, 51, 38, tzinfo=tzutc())}],\n",
       " 'Owner': {'DisplayName': 'aws-cities',\n",
       "  'ID': 'df12253943982d72f60594f06c2cacf9a1ee3a9e738c1649c9fb96e5127f1a5c'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check s3 connection using AWS_PROFILE=CitiesUserPermissionSet profile \n",
    "import boto3\n",
    "\n",
    "session = boto3.Session(profile_name=AWS_PROFILE)\n",
    "s3 = session.client('s3')\n",
    "\n",
    "# export CitiesUserPermissionSet profile to use in the next cells\n",
    "import os\n",
    "os.environ['AWS_PROFILE'] = AWS_PROFILE\n",
    "\n",
    "\n",
    "s3.list_buckets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-05-05 20:02:26,108][INFO    ][coiled] Fetching latest package priorities...\n",
      "[2025-05-05 20:02:26,109][INFO    ][coiled.package_sync] Resolving your local subdivisions2 Python environment...\n",
      "[2025-05-05 20:02:27,020][INFO    ][coiled.package_sync] Scanning 444 conda packages...\n",
      "[2025-05-05 20:02:27,029][INFO    ][coiled.package_sync] Scanning 260 python packages...\n",
      "[2025-05-05 20:02:27,921][INFO    ][coiled] Running pip check...\n",
      "[2025-05-05 20:02:29,424][INFO    ][coiled] Validating environment...\n",
      "[2025-05-05 20:02:52,095][INFO    ][coiled] Creating wheel for ~/Documents/Identifying Land Subdivisions/identifyingLandSubdivisions...\n",
      "[2025-05-05 20:02:52,313][INFO    ][coiled] Creating wheel for /opt/spark-2.2.0/python...\n",
      "[2025-05-05 20:02:52,701][WARNING ][coiled.package_sync] Package - libopenvino-intel-cpu-plugin, libopenvino-intel-cpu-plugin~=2025.0.0 has no install candidate for Python 3.12 linux-aarch64 on conda-forge\n",
      "[2025-05-05 20:02:52,703][INFO    ][coiled] Uploading coiled_local_identifyingLandSubdivisions...\n",
      "[2025-05-05 20:02:53,635][INFO    ][coiled] Uploading coiled_local_python...\n",
      "[2025-05-05 20:02:54,481][INFO    ][coiled] Requesting package sync build...\n",
      "[2025-05-05 20:02:55,195][INFO    ][coiled] Creating Cluster (name: ils-sara, https://cloud.coiled.io/clusters/866658?account=wri-cities-data ). This usually takes 1-2 minutes...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started a new Dask client on Coiled. Dashboard is available at https://cluster-qyvxx.dask.host/y22h7KkaTICJJExd/status\n"
     ]
    }
   ],
   "source": [
    "import coiled\n",
    "\n",
    "cluster = coiled.Cluster(\n",
    "    workspace=\"wri-cities-data\",\n",
    "    name=f'ils-{YOUR_NAME}',\n",
    "    region=\"us-west-2\",\n",
    "    arm=True,\n",
    "    worker_vm_types=\"r8g.xlarge\",\n",
    "    spot_policy=\"spot\",\n",
    "    n_workers=4,\n",
    "    package_sync_ignore=[\"pyspark\", \"pypandoc\"]\n",
    ")\n",
    "client = cluster.get_client()\n",
    "\n",
    "print(f\"Started a new Dask client on Coiled. Dashboard is available at {client.dashboard_link}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_geopandas as dgpd\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from dask import delayed, compute, visualize\n",
    "import geopandas as gpd\n",
    "from dask.diagnostics import ProgressBar\n",
    "from shapely.geometry import MultiLineString, LineString, Point\n",
    "from shapely.ops import polygonize, nearest_points\n",
    "#from shapely.geometry import Polygon, LineString, Point, MultiPolygon, MultiLineString, GeometryCollection\n",
    "from scipy.optimize import fminbound, minimize\n",
    "from metrics_groupby import metrics\n",
    "\n",
    "from pre_processing import *\n",
    "from auxiliary_functions import *\n",
    "from standardize_metrics import *\n",
    "\n",
    "YOUR_NAME = 'sara'\n",
    "grid_size = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     results \u001b[38;5;241m=\u001b[39m compute(\u001b[38;5;241m*\u001b[39mdelayed_jobs)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m---> 19\u001b[0m \u001b[43mpreprocess_all_cities\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAbidjan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAccra\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBamako\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBelo_Horizonte\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBogota\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCampinas\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCape_Town\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLagos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLuanda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNairobi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMedellin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#[\"Abidjan\", \"Accra\", \"Nairobi\", \"Bamako\", \"Belo_Horizonte\", \"Bogota\", \"Campinas\", \"Cape_Town\", \"Lagos\",\"Luanda\", \"Nairobi\", \"Medellin\"])\u001b[39;00m\n\u001b[1;32m     24\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# End the timer\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m, in \u001b[0;36mpreprocess_all_cities\u001b[0;34m(city_list)\u001b[0m\n\u001b[1;32m     14\u001b[0m delayed_jobs\u001b[38;5;241m.\u001b[39mappend([calculate_building_distances_to_roads(city) \u001b[38;5;28;01mfor\u001b[39;00m city \u001b[38;5;129;01min\u001b[39;00m city_list])\n\u001b[1;32m     15\u001b[0m delayed_jobs\u001b[38;5;241m.\u001b[39mappend([produce_blocks(city,YOUR_NAME,grid_size) \u001b[38;5;28;01mfor\u001b[39;00m city \u001b[38;5;129;01min\u001b[39;00m city_list])\n\u001b[0;32m---> 16\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdelayed_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/base.py:662\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    659\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 662\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/threading.py:655\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    653\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 655\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/threading.py:359\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 359\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pre_processing import *\n",
    "\n",
    "import time\n",
    "start_time = time.time()  # Start the timer\n",
    "\n",
    "%autoreload\n",
    "#from main import *\n",
    "\n",
    "\n",
    "def preprocess_all_cities(city_list):\n",
    "    #delayed_jobs = [delayed(calculate_building_distances_to_roads)(city) for city in city_list]\n",
    "    #delayed_jobs.append([delayed(produce_blocks)(city) for city in city_list])\n",
    "    delayed_jobs = []\n",
    "    delayed_jobs.append([calculate_building_distances_to_roads(city) for city in city_list])\n",
    "    delayed_jobs.append([produce_blocks(city,YOUR_NAME,grid_size) for city in city_list])\n",
    "    results = compute(*delayed_jobs)\n",
    "    return results\n",
    "\n",
    "preprocess_all_cities([\"Abidjan\", \"Accra\", \"Bamako\", \"Belo_Horizonte\", \n",
    "         \"Bogota\", \"Campinas\", \"Cape_Town\", \"Lagos\",\n",
    "        \"Luanda\", \"Nairobi\",\"Medellin\"]) \n",
    "#[\"Abidjan\", \"Accra\", \"Nairobi\", \"Bamako\", \"Belo_Horizonte\", \"Bogota\", \"Campinas\", \"Cape_Town\", \"Lagos\",\"Luanda\", \"Nairobi\", \"Medellin\"])\n",
    "\n",
    "end_time = time.time()  # End the timer\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Tasks completed in {elapsed_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks completed in 713.35 seconds.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pre_processing import *\n",
    "\n",
    "import time\n",
    "start_time = time.time()  # Start the timer\n",
    "\n",
    "\n",
    "city_list = [\"Abidjan\", \"Accra\", \"Bamako\", \"Belo_Horizonte\", \"Bogota\", \"Campinas\", \"Cape_Town\", \"Lagos\",\"Luanda\", \"Nairobi\",\"Medellin\"]\n",
    "#[\"Abidjan\", \"Accra\", \"Nairobi\", \"Bamako\", \"Belo_Horizonte\", \"Bogota\", \"Campinas\", \"Cape_Town\", \"Lagos\",\"Luanda\", \"Nairobi\", \"Medellin\"])\n",
    "\n",
    "delayed_jobs = []\n",
    "delayed_jobs.append([produce_azimuths(city, YOUR_NAME, grid_size) for city in city_list])\n",
    "results = compute(*delayed_jobs)\n",
    "\n",
    "end_time = time.time()  # End the timer\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Tasks completed in {elapsed_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "import dask_geopandas as dgpd\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely import wkb\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "def building_and_intersection_metrics(city_name):\n",
    "    grid_cell_count = 0\n",
    "    paths = {\n",
    "        'grid': f'{GRIDS_PATH}/{city_name}/{city_name}_{str(grid_size)}m_grid.geoparquet',\n",
    "        'buildings': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}.geoparquet',\n",
    "        'buildings_with_distances': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}_with_distances.geoparquet',\n",
    "        'roads': f'{ROADS_PATH}/{city_name}/{city_name}_OSM_roads.geoparquet',\n",
    "        'intersections': f'{INTERSECTIONS_PATH}/{city_name}/{city_name}_OSM_intersections.geoparquet'\n",
    "     }\n",
    "    # Get EPSG\n",
    "    epsg = get_epsg(city_name).compute()\n",
    "    # slim to just geometry & persist\n",
    "    roads = load_dataset(paths['roads'], epsg=epsg)\n",
    "    grid = load_dataset(paths['grid'], epsg=epsg)#.compute()\n",
    "\n",
    "    if 'geom' in grid.columns:\n",
    "        grid = grid.drop(columns=['geom'])\n",
    "    grid['cell_area'] = grid.geometry.area\n",
    "\n",
    "    cells = grid.index.size\n",
    "    grid_cell_count += cells\n",
    "\n",
    "    # Load buildings and perform relevant calculations on it\n",
    "    buildings = load_dataset(paths['buildings'], epsg=epsg)#.compute()\n",
    "    buildings['area'] = buildings.geometry.area\n",
    "    joined_buildings = dgpd.sjoin(buildings, grid, predicate='intersects')  \n",
    "    counts_buildings = joined_buildings.groupby('index_right').size()\n",
    "    \n",
    "    grid['n_buildings'] = counts_buildings.astype(int)\n",
    "    grid['n_buildings'] = grid['n_buildings'].fillna(0).astype(int)\n",
    "    built_area_buildings = joined_buildings.groupby('index_right')['area'].sum()\n",
    "    grid['built_area'] = built_area_buildings.astype(float)\n",
    "    grid['built_area'] = grid['built_area'].fillna(0.0)\n",
    "    \n",
    "    included_road_types = ['trunk','motorway','primary','secondary','tertiary','primary_link','secondary_link','tertiary_link','trunk_link','motorway_link','residential','unclassified','road','living_street']\n",
    "\n",
    "    roads_geo = roads[['geometry']]#.persist()\n",
    "\n",
    "    # tiny grid dataframe for overlay\n",
    "    grid_small = (\n",
    "        grid.reset_index()[['index','geometry']]\n",
    "            .rename(columns={'index':'index_right'})\n",
    "    )\n",
    "\n",
    "    # per-partition overlay + length\n",
    "    def road_length_partition(df, grid_sm):\n",
    "        clipped = gpd.overlay(df, grid_sm, how='intersection')\n",
    "        L = clipped.geometry.length.values\n",
    "        return pd.DataFrame({\n",
    "            'index_right': clipped['index_right'].values,\n",
    "            'length_in_cell': L\n",
    "        }, index=clipped.index)\n",
    "\n",
    "    meta_rl = pd.DataFrame({\n",
    "        'index_right': pd.Series(dtype='int64'),\n",
    "        'length_in_cell': pd.Series(dtype='float64')\n",
    "    })\n",
    "\n",
    "    road_parts = roads_geo.map_partitions(\n",
    "        road_length_partition, grid_small, meta=meta_rl\n",
    "    ).persist()\n",
    "\n",
    "    agg_rl = road_parts.groupby('index_right').agg(\n",
    "        total_len_m=('length_in_cell','sum')\n",
    "    )\n",
    "\n",
    "    grid['cell_area_km2'] = grid['cell_area']/1000000.\n",
    "    \n",
    "    grid['road_length'] = (agg_rl['total_len_m'] / 1000.)\n",
    "    grid['road_length'] = grid['road_length'].fillna(0.0)\n",
    "    grid['has_roads'] = grid['road_length'] > 0\n",
    "\n",
    "    # Intersection metrics\n",
    "    intersections = load_dataset(paths['intersections'], epsg=epsg)\n",
    "    ji = dgpd.sjoin(intersections, grid, predicate='intersects')\n",
    "    counts2 = ji[ji.street_count>=2].groupby('index_right').size()\n",
    "    counts3 = ji[ji.street_count>=3].groupby('index_right').size()\n",
    "    counts4 = ji[ji.street_count==4].groupby('index_right').size()\n",
    "    \n",
    "    grid['n_intersections'] = counts2\n",
    "    grid['n_intersections'] = grid['n_intersections'].fillna(0.).astype(int)\n",
    "\n",
    "    \n",
    "    # right after you build n_intersections:\n",
    "    grid['has_intersections'] = (grid['n_intersections'] > 0).astype('bool')\n",
    "\n",
    "\n",
    "    \n",
    "    grid['intersections_3plus'] = counts3\n",
    "    grid['intersections_3plus'] = grid['intersections_3plus'].fillna(0).astype(int)\n",
    "    \n",
    "    grid['intersections_4way']  = counts4\n",
    "    grid['intersections_4way']  = grid['intersections_4way'].fillna(0).astype(int)\n",
    "\n",
    "    # Downstream metrics\n",
    "    grid['m3_raw'] = grid['road_length'] / grid['cell_area_km2']\n",
    "    grid['m3_std'] = grid['m3_raw'].map_partitions(standardize_metric_3, meta=('m3','float64'))\n",
    "    grid['m4_raw'] = grid['intersections_4way'] / grid['intersections_3plus']\n",
    "    grid['m4_raw'] = grid['m4_raw'].mask(\n",
    "        grid['m4_raw'].isna() & grid['has_roads'], \n",
    "        0.0)\n",
    "    grid['m4_std'] = grid['m4_raw'].map_partitions(standardize_metric_4, meta=('m4','float64'))\n",
    "\n",
    "    grid['m5_raw'] = (1000**2) * (grid['n_intersections'] / grid['cell_area'])\n",
    "    grid['m5_raw'] = grid['m5_raw'].mask(\n",
    "        grid['has_roads'] & grid['m5_raw'].isna(), \n",
    "        0.0)\n",
    "    grid['m5_std'] = grid['m5_raw'].map_partitions(standardize_metric_5, meta=('m5','float64'))\n",
    "    grid['m10_raw'] = grid['n_buildings'] / grid['cell_area_km2']\n",
    "    grid['m10_std'] = grid['m10_raw'].map_partitions(standardize_metric_10, meta=('m10','float64'))\n",
    "    grid['m11_raw'] = grid['built_area'] / grid['cell_area']\n",
    "    grid['m11_std'] = grid['m11_raw'].map_partitions(standardize_metric_11, meta=('m11','float64'))\n",
    "    grid['m12_raw'] = grid['built_area'] / grid['n_buildings']\n",
    "    grid['m12_std'] = grid['m12_raw'].map_partitions(standardize_metric_12, meta=('m12','float64'))\n",
    "\n",
    "    # Write & return\n",
    "    out = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{grid_size}m_metrics_3_4_5_10_11_12_grid_{YOUR_NAME}.geoparquet'\n",
    "    if 'geom' in grid.columns:\n",
    "        grid = grid.drop(columns=['geom'])\n",
    "    grid.to_parquet(out)\n",
    "    return out\n",
    "\n",
    "@delayed\n",
    "def building_distance_metrics(city_name):\n",
    "     paths = {\n",
    "         'grid': f'{GRIDS_PATH}/{city_name}/{city_name}_{str(grid_size)}m_grid.geoparquet',\n",
    "         'buildings': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}.geoparquet',\n",
    "         'buildings_with_distances': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}_with_distances.geoparquet',\n",
    "         'roads': f'{ROADS_PATH}/{city_name}/{city_name}_OSM_roads.geoparquet',\n",
    "         'intersections': f'{INTERSECTIONS_PATH}/{city_name}/{city_name}_OSM_intersections.geoparquet'\n",
    "     }\n",
    "     # Get EPSG\n",
    "     epsg = get_epsg(city_name).compute()\n",
    "     # Load grid\n",
    "     grid = load_dataset(paths['grid'], epsg=epsg)#.compute()\n",
    "     if 'geom' in grid.columns:\n",
    "         grid = grid.drop(columns=['geom'])\n",
    "     \n",
    "     buildings = load_dataset(paths['buildings_with_distances'], epsg=epsg)#.compute()\n",
    "     buildings['distance_to_nearest_road'] = buildings['distance_to_nearest_road'].astype(float)\n",
    "     buildings['area'] = buildings.geometry.area\n",
    "     joined_buildings = dgpd.sjoin(buildings, grid, predicate='intersects')  \n",
    "     counts_buildings = joined_buildings.groupby('index_right').size()\n",
    "     grid['n_buildings'] = counts_buildings\n",
    "     grid['n_buildings'] = grid['n_buildings'].fillna(0).astype(int)\n",
    "\n",
    "     grid['has_buildings'] = grid['n_buildings']    > 0\n",
    "     average_distance = joined_buildings.groupby('index_right')['distance_to_nearest_road'].mean()\n",
    "     grid['average_distance_nearest_building'] = average_distance\n",
    "     grid['average_distance_nearest_building'] = grid['average_distance_nearest_building'].fillna(0.0)\n",
    "\n",
    "    \n",
    "    \n",
    "     buildings_closer_than_20m = buildings[buildings['distance_to_nearest_road'] <= 20]\n",
    "     joined_buildings_closer_than_20m = dgpd.sjoin(buildings_closer_than_20m, grid, predicate='intersects') \n",
    "     n_buildings_closer_than_20m = joined_buildings_closer_than_20m.groupby('index_right').size()\n",
    "     grid['n_buildings_closer_than_20m'] = n_buildings_closer_than_20m\n",
    "     grid['n_buildings_closer_than_20m'] = grid['n_buildings_closer_than_20m'].fillna(0.0)\n",
    "     grid = grid.assign(\n",
    "    n_buildings_closer_than_20m = grid['n_buildings_closer_than_20m'].mask(\n",
    "        (grid['n_buildings'] > 0) & (grid['n_buildings_closer_than_20m'].isna()),\n",
    "        0))\n",
    "     grid = grid.assign(\n",
    "         m1_raw = grid['n_buildings_closer_than_20m'] / grid['n_buildings']\n",
    "         )\n",
    "     grid['m1_raw'] = grid['m1_raw']#.fillna(grid['m1_raw'].mean())#grid['n_buildings_closer_than_20m'] / grid['n_buildings']\n",
    "     grid['m1_std'] = grid['m1_raw'].map_partitions(standardize_metric_1, meta=('m1', 'float64'))\n",
    "     grid['m2_raw'] = grid['average_distance_nearest_building']\n",
    "     grid['m2_raw'] = grid['m2_raw']#.fillna(grid['m2_raw'].mean())\n",
    "     grid['m2_std'] = grid['m2_raw'].map_partitions(standardize_metric_2, meta=('m2', 'float64'))\n",
    "     \n",
    "     path = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{str(grid_size)}m_grid_{YOUR_NAME}_metrics_1_2.geoparquet'\n",
    "    \n",
    "     if 'geom' in grid.columns:\n",
    "         grid = grid.drop(columns='geom')\n",
    "    \n",
    "     grid.to_parquet(path)\n",
    "\n",
    "@delayed\n",
    "def compute_m6_m7(city_name):\n",
    "    \"\"\"\n",
    "    Computes:\n",
    "    - M6: KL divergence (building orientation), weighted by:\n",
    "         (a) block’s proportional overlap with each grid cell\n",
    "         (b) number of buildings inside that block∩cell\n",
    "      fallback: unweighted KL for cells with ≥2 buildings but no blocks\n",
    "    - M7: Average block width\n",
    "    \"\"\"\n",
    "\n",
    "    # 0) paths & load\n",
    "    epsg = get_epsg(city_name).compute()\n",
    "    grid = load_dataset(f'{GRIDS_PATH}/{city_name}/{city_name}_{grid_size}m_grid.geoparquet',\n",
    "                        epsg=epsg)\n",
    "    blocks    = load_dataset(f'{BLOCKS_PATH}/{city_name}/{city_name}_blocks_{YOUR_NAME}.geoparquet', epsg=epsg).persist()\n",
    "    buildings = load_dataset(f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}_with_distances_and_azimuths.geoparquet', epsg=epsg).persist()\n",
    "    buildings['azimuth'] = buildings['azimuth']\\\n",
    "        .map_partitions(pd.to_numeric,\n",
    "                        meta=('azimuth','float64'),\n",
    "                        errors='coerce')\n",
    "\n",
    "\n",
    "    # drop stray geometry column if present\n",
    "    if 'geom' in grid.columns:\n",
    "        grid = grid.drop(columns=['geom'])\n",
    "\n",
    "    # 1) prepare blocks\n",
    "    epsilon = 0.001\n",
    "    blocks = blocks.assign(\n",
    "        block_id=blocks.index,\n",
    "        epsilon_buffer=blocks.geometry.buffer(-(1-epsilon)*blocks.max_radius),\n",
    "        width_buffer  =blocks.geometry.buffer(-0.2*blocks.max_radius)\n",
    "    )\n",
    "\n",
    "    # 2) compute block→grid overlaps (with area_weight)\n",
    "    #    compute_block_grid_weights returns a **pandas** GeoDataFrame after compute()\n",
    "    bgo = compute_block_grid_weights(blocks, grid).compute()\n",
    "\n",
    "    # 3) count **per (block, cell)** buildings inside each overlap\n",
    "    #    (so blocks only contribute where they actually contain buildings)\n",
    "    buildings_pdf = buildings.compute()[['geometry']]\n",
    "    # join buildings → block_cell overlaps\n",
    "    join = gpd.sjoin(buildings_pdf,\n",
    "                     bgo[['block_id','grid_id','geometry']],\n",
    "                     predicate='intersects')\n",
    "    # count\n",
    "    n_bc = (\n",
    "        join\n",
    "        .groupby(['block_id','grid_id'])\n",
    "        .size()\n",
    "        .rename('n_buildings_cell')\n",
    "        .reset_index()\n",
    "    )\n",
    "    # merge back, fill zero\n",
    "    bgo = (\n",
    "        bgo\n",
    "        .merge(n_bc, on=['block_id','grid_id'], how='left')\n",
    "        .fillna({'n_buildings_cell': 0})\n",
    "    )\n",
    "\n",
    "    # 4) block-level KL & per-cell weighted m6\n",
    "    kl_df   = compute_block_kl_metrics(\n",
    "                  # still uses your buildings_blocks → block-level KL\n",
    "                  dgpd.sjoin(buildings, blocks, predicate='intersects')\n",
    "                     [['block_id','geometry','epsilon_buffer','width_buffer','azimuth']]\n",
    "                     .set_index('block_id')\n",
    "                     .repartition(npartitions=4)\n",
    "              ).compute()\n",
    "    # adjust aggregate to use n_buildings_cell\n",
    "    df = (\n",
    "        bgo\n",
    "        .merge(kl_df, on='block_id', how='left')\n",
    "        .dropna(subset=['standardized_kl'])\n",
    "        .assign(weight = lambda d: d.area_weight * d.n_buildings_cell,\n",
    "                weighted_kl = lambda d: d.standardized_kl * d.weight)\n",
    "    )\n",
    "    grid_m6 = (\n",
    "        df\n",
    "        .groupby('grid_id')\n",
    "        .agg(total_weighted_kl=('weighted_kl','sum'),\n",
    "             total_weight=('weight','sum'))\n",
    "    )\n",
    "    grid_m6['m6'] = grid_m6.total_weighted_kl / grid_m6.total_weight\n",
    "\n",
    "    # 5) compute M7 as before\n",
    "    bgo['weighted_max_radius'] = bgo.max_radius * bgo.area_weight\n",
    "    grid_m7 = (\n",
    "        bgo\n",
    "        .groupby('grid_id')\n",
    "        .agg(total_weighted_max_radius=('weighted_max_radius','sum'),\n",
    "             total_weight=('area_weight', 'sum'))\n",
    "    )\n",
    "    grid_m7['m7'] = grid_m7.total_weighted_max_radius / grid_m7.total_weight\n",
    "\n",
    "    # 6) unweighted KL fallback for cells with ≥2 buildings (no blocks)\n",
    "    def kl_divergence(arr, bins=18):\n",
    "        hist,_ = np.histogram(arr, bins=bins, range=(0,180))\n",
    "        P = hist/hist.sum() if hist.sum()>0 else np.ones(bins)/bins\n",
    "        Q = np.ones(bins)/bins\n",
    "        return entropy(P, Q) / np.log(bins)\n",
    "\n",
    "    b2g = dgpd.sjoin(buildings[['geometry','azimuth']],\n",
    "                     grid[['geometry']],\n",
    "                     predicate='intersects'\n",
    "                    ).persist()\n",
    "    m6_unw = (\n",
    "        b2g.groupby('index_right')['azimuth']\n",
    "           .apply(lambda s: kl_divergence(s.to_numpy()),\n",
    "                  meta=('azimuth','float64'))\n",
    "           .rename('m6_unweighted')\n",
    "    )\n",
    "\n",
    "    # 7) stitch everything back onto the full grid\n",
    "    #    name all indexes “grid_id” so reset_index() yields a column you can join on\n",
    "    grid        = grid       .rename_axis(\"grid_id\")\n",
    "    grid_m6     = grid_m6    .rename_axis(\"grid_id\")\n",
    "    m6_unw      = m6_unw     .rename_axis(\"grid_id\")\n",
    "    grid_m7     = grid_m7    .rename_axis(\"grid_id\")\n",
    "\n",
    "    grid = (\n",
    "        grid\n",
    "        .reset_index()                                # now has “grid_id” column\n",
    "        .merge(grid_m6.reset_index(),   on=\"grid_id\", how=\"left\")\n",
    "        .merge(m6_unw.reset_index(),    on=\"grid_id\", how=\"left\")\n",
    "        .merge(grid_m7.reset_index()[['grid_id','m7']],\n",
    "               on=\"grid_id\", how=\"left\")\n",
    "        .set_index(\"grid_id\")\n",
    "    )\n",
    "\n",
    "    # 8) build & standardize raw columns\n",
    "    grid['m6_raw'] = grid['m6'].fillna(grid['m6_unweighted'])\n",
    "    grid['m6_std'] = grid['m6_raw'].map_partitions(\n",
    "                        standardize_metric_6, meta=('m6','float64')\n",
    "                     )\n",
    "\n",
    "    grid['m7_raw'] = grid['m7'].fillna(200)\n",
    "    grid['m7_std'] = grid['m7_raw'].map_partitions(\n",
    "                        standardize_metric_7, meta=('m7','float64')\n",
    "                     )\n",
    "\n",
    "    grid = grid.drop(columns=['m6','m7'])\n",
    "    pdf  = grid.compute()\n",
    "\n",
    "    # 9) write out\n",
    "    out = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{grid_size}m_grid_{YOUR_NAME}_metrics_6_7.geoparquet'\n",
    "    pdf.to_parquet(out, engine='pyarrow', index=False)\n",
    "    return out\n",
    "\n",
    "@delayed\n",
    "def metrics_roads_intersections(city_name):\n",
    "\n",
    "    paths = {\n",
    "    'grid': f'{GRIDS_PATH}/{city_name}/{city_name}_{str(grid_size)}m_grid.geoparquet',\n",
    "    'blocks': f'{BLOCKS_PATH}/{city_name}/{city_name}_blocks_{YOUR_NAME}.geoparquet',\n",
    "    'buildings_with_distances': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}_with_distances.geoparquet',\n",
    "    'roads': f'{ROADS_PATH}/{city_name}/{city_name}_OSM_roads.geoparquet',\n",
    "    'intersections': f'{INTERSECTIONS_PATH}/{city_name}/{city_name}_OSM_intersections.geoparquet'\n",
    "    }\n",
    "\n",
    "    # LOAD\n",
    "    epsg = get_epsg(city_name).compute()\n",
    "    grid = load_dataset(paths['grid'], epsg=epsg).persist()\n",
    "    roads = load_dataset(paths['roads'], epsg=epsg).persist()\n",
    "    intersections = load_dataset(paths['intersections'], epsg=epsg).compute()\n",
    "\n",
    "\n",
    "    included_road_types = ['trunk','motorway','primary','secondary','tertiary','primary_link','secondary_link','tertiary_link','trunk_link','motorway_link','residential','unclassified','road','living_street']\n",
    "    roads['highway'][roads['highway'].isin(included_road_types)]\n",
    "    \n",
    "\n",
    "    if 'geom' in grid.columns:\n",
    "        grid = grid.drop(columns=['geom'])\n",
    "\n",
    "\n",
    "    \n",
    "    # Prep for metric 10\n",
    "    intersections['osmid'] = intersections['osmid'].astype(int)\n",
    "    intersection_angles = compute_intersection_angles(roads, intersections)\n",
    "    street_count_mapping = intersections.set_index('osmid')['street_count'].to_dict()\n",
    "    intersection_angle_mapping = compute_intersection_mapping(intersection_angles, street_count_mapping)\n",
    "    intersection_angle_mapping = intersection_angle_mapping.compute()  \n",
    "\n",
    "    intersections_with_angles_metric = intersections.merge(\n",
    "        intersection_angle_mapping.rename(\"average_angle\"), left_on=\"osmid\", right_index=True, how=\"left\"\n",
    "    )\n",
    "\n",
    "    joined_intersection_angles_grid = dgpd.sjoin(intersections_with_angles_metric, grid, predicate=\"within\")\n",
    "    average_angle_between_roads = joined_intersection_angles_grid.groupby('index_right')['average_angle'].mean()\n",
    "    \n",
    "\n",
    "    # Prep for metric 9\n",
    "    \n",
    "    roads_simple = roads[['geometry']]\n",
    "    \n",
    "    grid_small = (\n",
    "        grid\n",
    "        .reset_index()[[\"index\",\"geometry\"]]\n",
    "        .rename(columns={\"index\":\"index_right\"})\n",
    "    )\n",
    "    \n",
    "    # inside metrics_roads_intersections, before overlay:\n",
    "    overlay_meta = gpd.GeoDataFrame(\n",
    "        {\n",
    "            \"index_right\": pd.Series(dtype=\"int64\"),\n",
    "            \"geometry\":    gpd.GeoSeries(dtype=\"geometry\")\n",
    "        },\n",
    "        geometry=\"geometry\"\n",
    "    )\n",
    "\n",
    "    roads_cells = roads_simple.map_partitions(\n",
    "        overlay_partition,\n",
    "        grid_small,\n",
    "        meta=overlay_meta\n",
    "    ).persist()\n",
    "\n",
    "    # roads_cells now has:\n",
    "    #  - geometry   = clipped road piece\n",
    "    #  - index_right = the cell it belongs to\n",
    "\n",
    "    # 3) compute wt & length in one pass per partition\n",
    "    out = roads_cells.map_partitions(\n",
    "        partition_tortuosity_clipped,   \n",
    "        meta=pd.DataFrame({\n",
    "            \"index_right\": pd.Series(dtype=\"int64\"),\n",
    "            \"wt\":           pd.Series(dtype=\"float64\"),\n",
    "            \"length\":      pd.Series(dtype=\"float64\")\n",
    "        })\n",
    "    ).persist()\n",
    "\n",
    "    # 4) aggregate back into grid\n",
    "    agg = out.groupby(\"index_right\").agg(\n",
    "        total_len=(\"length\",\"sum\"),\n",
    "        sum_wt   =(\"wt\",    \"sum\")\n",
    "    )\n",
    "    grid[\"m8_raw\"] = grid.index.map(agg[\"sum_wt\"]/agg[\"total_len\"]).astype(float)\n",
    "    grid[\"m8_std\"] = grid[\"m8_raw\"].map_partitions(\n",
    "        standardize_metric_8, meta=(\"m8\",\"float64\")\n",
    "    )\n",
    "    \n",
    "    grid['m9_raw'] = grid.index.map(average_angle_between_roads).astype(float)#.fillna(np.mean(average_angle_between_roads)).astype(float)\n",
    "    grid['m9_std'] = grid['m9_raw'].map_partitions(standardize_metric_9, meta=('m9', 'float64'))\n",
    "    \n",
    "    path = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{str(grid_size)}m_grid_metrics_8_9_{YOUR_NAME}.geoparquet'\n",
    "\n",
    "    if 'geom' in grid.columns:\n",
    "        grid = grid.drop(columns='geom')\n",
    "\n",
    "    grid.to_parquet(path)\n",
    "\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks completed in 33.83 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from dask import compute\n",
    "\n",
    "start_time = time.time()  # Start the timer\n",
    "\n",
    "#cities = [\"Abidjan\", \"Accra\", \"Bamako\", \"Belo_Horizonte\",\"Bogota\", 'Nairobi',\"Campinas\", \"Cape_Town\", \"Luanda\", \"Medellin\"]\n",
    "cities = ['Medellin']\n",
    "#\"Lagos\"\n",
    "cities = [city.replace(' ', '_') for city in cities]\n",
    "\n",
    "tasks = []\n",
    "for city in cities:\n",
    "    tasks.append(building_and_intersection_metrics(city))\n",
    "    tasks.append(building_distance_metrics(city))\n",
    "    tasks.append(compute_m6_m7(city))\n",
    "    tasks.append(metrics_roads_intersections(city))\n",
    "\n",
    "results = compute(*tasks)\n",
    "\n",
    "end_time = time.time()  # End the timer\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Tasks completed in {elapsed_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written outputs: ['s3://wri-cities-sandbox/identifyingLandSubdivisions/data/output/raster/Medellin/Medellin_200m_all_metrics_combined_sara.geoparquet']\n"
     ]
    }
   ],
   "source": [
    "import dask_geopandas as dgpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely import wkb\n",
    "\n",
    "def consolidate_irregularity_index(city_name):\n",
    "    # 1) Paths (same as before) …\n",
    "    base = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{grid_size}m'\n",
    "    p0 = f'{base}_metrics_3_4_5_10_11_12_grid_{YOUR_NAME}.geoparquet'\n",
    "    p1 = f'{base}_grid_{YOUR_NAME}_metrics_1_2.geoparquet'\n",
    "    p2 = f'{base}_grid_{YOUR_NAME}_metrics_6_7.geoparquet' \n",
    "    p3 = f'{base}_grid_metrics_8_9_{YOUR_NAME}.geoparquet'\n",
    "\n",
    "    # 2) Lazy-read  \n",
    "    g0 = dgpd.read_parquet(p0)\n",
    "    g1 = dgpd.read_parquet(p1)[['m1_std','m2_std','m1_raw','m2_raw','has_buildings']].drop(columns='geometry', errors='ignore')\n",
    "    g2 = dgpd.read_parquet(p2)[['m6_std','m7_std','m6_raw','m7_raw']].drop(columns='geometry', errors='ignore')\n",
    "    g3 = dgpd.read_parquet(p3)[['m8_std','m9_std','m8_raw','m9_raw']].drop(columns='geometry', errors='ignore')\n",
    "\n",
    "    # 3) Merge (keeps geometry)\n",
    "    df = (g0\n",
    "          .merge(g1, left_index=True, right_index=True, how='left')\n",
    "          .merge(g2, left_index=True, right_index=True, how='left')\n",
    "          .merge(g3, left_index=True, right_index=True, how='left'))\n",
    "\n",
    "    # 4) Mask\n",
    "    std_cols = [f'm{i}_std' for i in range(1,13)]\n",
    "    df['has_features'] = df['has_buildings'] | df['has_roads'] | df['has_intersections']\n",
    "    df[std_cols] = df[std_cols].where(df['has_features'], np.nan)\n",
    "\n",
    "    # 5) Persist & repartition\n",
    "    df = df.persist().repartition(npartitions=8)\n",
    "\n",
    "    all_metrics_columns_raw = ['m'+str(x)+'_raw' for x in range(1,13)]\n",
    "    all_metrics_columns_final = ['m'+str(x)+'_final' for x in range(1,13)]\n",
    "    all_metrics_columns_std = ['m'+str(x)+'_std' for x in range(1,13)]\n",
    "    all_metrics_columns_zc = ['m'+str(x)+'_zc' for x in range(1,13)]\n",
    "    \n",
    "    # 6) Compute global scalars on standardized metrics\n",
    "    means = df[all_metrics_columns_std].mean().compute()\n",
    "    stds  = df[all_metrics_columns_std].std().compute()\n",
    "    mins  = df[all_metrics_columns_std].min().compute()\n",
    "    maxs  = df[all_metrics_columns_std].max().compute()\n",
    "\n",
    "    # Convert those into z-mins / z-maxs and re-index to your zc_cols\n",
    "    zmin = (mins - means) / stds\n",
    "    zmax = (maxs - means) / stds\n",
    "    zmin.index = all_metrics_columns_zc\n",
    "    zmax.index = all_metrics_columns_zc\n",
    "\n",
    "    # 7) One-pass normalize + index\n",
    "    def normalize_partition(pdf, means, stds, zmin, zmax):\n",
    "        # zero-center -> z DataFrame\n",
    "        z = (pdf[all_metrics_columns_std] - means) / stds\n",
    "        z.columns = all_metrics_columns_zc\n",
    "        pdf[all_metrics_columns_zc] = z\n",
    "\n",
    "        # min–max on z -> f DataFrame\n",
    "        f = (z - zmin) / (zmax - zmin)\n",
    "        f.columns = all_metrics_columns_final\n",
    "        pdf[all_metrics_columns_final] = f\n",
    "\n",
    "        groupA = [f\"m{i}_final\" for i in range(1,10)]        # metrics 1–9\n",
    "        groupB = groupA + [\"m12_final\"]                     # metrics 1–9 plus 12\n",
    "\n",
    "        # … after pdf[all_metrics_columns_final] = f …\n",
    "        # compute two regularity indices and their NA‐counts\n",
    "        pdf[\"regularity_index_A\"] = f[groupA].mean(axis=1)\n",
    "        pdf[\"na_count_A\"]          = f[groupA].isna().sum(axis=1)\n",
    "\n",
    "        pdf[\"regularity_index_B\"] = f[groupB].mean(axis=1)\n",
    "        pdf[\"na_count_B\"]          = f[groupB].isna().sum(axis=1)\n",
    "        return pdf\n",
    "\n",
    "\n",
    "    # 8) Build meta and map_partitions\n",
    "    meta = df._meta.copy()\n",
    "    for c in (\n",
    "        all_metrics_columns_zc\n",
    "      + all_metrics_columns_final\n",
    "      + ['regularity_index_A','na_count_A','regularity_index_B','na_count_B']\n",
    "    ):\n",
    "        meta[c] = pd.Series(dtype='float64')\n",
    "\n",
    "\n",
    "    df = df.map_partitions(\n",
    "        normalize_partition,\n",
    "        means, stds, zmin, zmax,\n",
    "        meta=meta\n",
    "    )\n",
    "\n",
    "    import geopandas as gpd\n",
    "    \n",
    "    out = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{grid_size}m_all_metrics_combined_{YOUR_NAME}.geoparquet'\n",
    "    # … after your df = df.map_partitions(...), instead of WKB+pd.to_parquet:\n",
    "    pdf = df.compute()\n",
    "\n",
    "    # rehydrate into a GeoDataFrame, borrow the original CRS:\n",
    "    gdf = gpd.GeoDataFrame(pdf, geometry='geometry', crs=g0.crs)\n",
    "\n",
    "    # write **one** parquet file, with embedded CRS and GeoArrow extension types:\n",
    "    gdf.to_parquet(out, engine='pyarrow', index=False)\n",
    "\n",
    "    '''\n",
    "    # 9) Compute into pandas, then convert geometry to WKB\n",
    "    pdf = df.compute()\n",
    "    pdf['geometry'] = pdf.geometry.apply(lambda geom: wkb.dumps(geom, hex=False))\n",
    "\n",
    "    # 10) Write one parquet file via pandas (supports single_file)\n",
    "    \n",
    "    pdf.to_parquet(out, engine='pyarrow', index=False)#single_file=True, \n",
    "    #df.to_parquet(out, engine='pyarrow', index=False)#\n",
    "    '''\n",
    "    return out\n",
    "\n",
    "# Dispatch in parallel:\n",
    "#cities = [\"Abidjan\",\"Accra\",\"Bamako\",\"Belo_Horizonte\",\"Bogota\",\"Nairobi\",\"Campinas\",\"Cape_Town\",\"Luanda\",\"Medellin\"]\n",
    "cities = ['Medellin']\n",
    "futures = client.map(consolidate_irregularity_index, cities)\n",
    "results = client.gather(futures)\n",
    "print(\"Written outputs:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "cities =[\"Abidjan\",\"Accra\",\"Bamako\",\"Belo_Horizonte\",\"Bogota\", \"Nairobi\",\"Campinas\",\"Cape_Town\",\"Luanda\",\"Medellin\"]\n",
    "paths = [f'{OUTPUT_PATH_RASTER}/{city}/{city}_{grid_size}m_all_metrics_combined_{YOUR_NAME}.geoparquet' \n",
    "         for city in cities]\n",
    "\n",
    "# Read into GeoPandas and concat\n",
    "gdfs = [gpd.read_parquet(path) for path in paths]\n",
    "combined = gpd.GeoDataFrame(pd.concat(gdfs, ignore_index=True), crs=gdfs[0].crs)\n",
    "\n",
    "# Write a single GeoPackage file\n",
    "combined.to_file('all_cities_combined.gpkg', driver='GPKG')\n",
    "print(\"Combined GeoPackage saved as all_cities_combined.gpkg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_combined.to_parquet('../Medellin_consolidated_index_results.parquet')#regularity_index.describe() #[['m1_raw','m2_raw','regularity_index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mt/3n9j2kc92kv4psztx687vtd80000gn/T/ipykernel_71266/401492889.py:1: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  grid_combined.to_file('../Medellin_consolidated_index_results.shp')\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'cell_area_km2' to 'cell_area_'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'n_buildings' to 'n_building'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'road_length' to 'road_lengt'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'n_intersections' to 'n_intersec'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'intersections_3plus' to 'intersecti'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'intersections_4way' to 'intersec_1'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'has_intersections' to 'has_inters'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'has_buildings' to 'has_buildi'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'has_features' to 'has_featur'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'm1_zero-centered' to 'm1_zero-ce'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'm2_zero-centered' to 'm2_zero-ce'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'm3_zero-centered' to 'm3_zero-ce'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'm4_zero-centered' to 'm4_zero-ce'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'm5_zero-centered' to 'm5_zero-ce'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'm6_zero-centered' to 'm6_zero-ce'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'm7_zero-centered' to 'm7_zero-ce'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'm8_zero-centered' to 'm8_zero-ce'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'm9_zero-centered' to 'm9_zero-ce'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'm10_zero-centered' to 'm10_zero-c'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'm11_zero-centered' to 'm11_zero-c'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'm12_zero-centered' to 'm12_zero-c'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'm13_zero-centered' to 'm13_zero-c'\n",
      "  ogr_write(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'regularity_index' to 'regularity'\n",
      "  ogr_write(\n"
     ]
    }
   ],
   "source": [
    "grid_combined.to_file('../Medellin_consolidated_index_results.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mde_grid = gpd.read_parquet('../Medellin_consolidated_index_results.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>m1_raw</th>\n",
       "      <th>m1_std</th>\n",
       "      <th>m1_zero-centered</th>\n",
       "      <th>m1_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10207.000000</td>\n",
       "      <td>10207.000000</td>\n",
       "      <td>1.332400e+04</td>\n",
       "      <td>8563.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.498136</td>\n",
       "      <td>0.498136</td>\n",
       "      <td>-5.119491e-17</td>\n",
       "      <td>0.357416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.331203</td>\n",
       "      <td>0.331203</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.390134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-7.411228e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.824746e-01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>3.998596e-01</td>\n",
       "      <td>0.165408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>6.608733e-01</td>\n",
       "      <td>0.765262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.809505e-01</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             m1_raw        m1_std  m1_zero-centered     m1_final\n",
       "count  10207.000000  10207.000000      1.332400e+04  8563.000000\n",
       "mean       0.498136      0.498136     -5.119491e-17     0.357416\n",
       "std        0.331203      0.331203      1.000000e+00     0.390134\n",
       "min        0.000000      0.000000     -7.411228e+00     0.000000\n",
       "25%        0.200000      0.200000     -1.824746e-01     0.000000\n",
       "50%        0.540541      0.540541      3.998596e-01     0.165408\n",
       "75%        0.782609      0.782609      6.608733e-01     0.765262\n",
       "max        1.000000      1.000000      6.809505e-01     1.000000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_combined[['m1_raw','m1_std','m1_zero-centered','m1_final']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>m2_raw</th>\n",
       "      <th>m2_std</th>\n",
       "      <th>m2_zero-centered</th>\n",
       "      <th>m2_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10207.000000</td>\n",
       "      <td>10207.000000</td>\n",
       "      <td>1.332400e+04</td>\n",
       "      <td>9090.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>52.151067</td>\n",
       "      <td>0.649161</td>\n",
       "      <td>4.266243e-17</td>\n",
       "      <td>0.094862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>89.740864</td>\n",
       "      <td>0.299055</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.143445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.866936e-01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>13.234007</td>\n",
       "      <td>0.544541</td>\n",
       "      <td>-6.832878e-01</td>\n",
       "      <td>0.005162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>22.771024</td>\n",
       "      <td>0.772290</td>\n",
       "      <td>-4.674279e-01</td>\n",
       "      <td>0.036609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>45.545923</td>\n",
       "      <td>0.867660</td>\n",
       "      <td>4.301169e-01</td>\n",
       "      <td>0.119164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1223.135659</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.193408e+01</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             m2_raw        m2_std  m2_zero-centered     m2_final\n",
       "count  10207.000000  10207.000000      1.332400e+04  9090.000000\n",
       "mean      52.151067      0.649161      4.266243e-17     0.094862\n",
       "std       89.740864      0.299055      1.000000e+00     0.143445\n",
       "min        0.000000      0.000000     -6.866936e-01     0.000000\n",
       "25%       13.234007      0.544541     -6.832878e-01     0.005162\n",
       "50%       22.771024      0.772290     -4.674279e-01     0.036609\n",
       "75%       45.545923      0.867660      4.301169e-01     0.119164\n",
       "max     1223.135659      1.000000      1.193408e+01     1.000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_combined[['m2_raw','m2_std','m2_zero-centered','m2_final']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>m8_raw</th>\n",
       "      <th>m8_std</th>\n",
       "      <th>m8_zero-centered</th>\n",
       "      <th>m8_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9090.000000</td>\n",
       "      <td>9090.000000</td>\n",
       "      <td>1.332400e+04</td>\n",
       "      <td>10207.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.946877</td>\n",
       "      <td>0.053123</td>\n",
       "      <td>5.119491e-17</td>\n",
       "      <td>0.649161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.080329</td>\n",
       "      <td>0.080329</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.299055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-7.310172e-01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.933268</td>\n",
       "      <td>0.002891</td>\n",
       "      <td>-7.310172e-01</td>\n",
       "      <td>0.544541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.979499</td>\n",
       "      <td>0.020501</td>\n",
       "      <td>-4.778693e-01</td>\n",
       "      <td>0.772290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.997109</td>\n",
       "      <td>0.066732</td>\n",
       "      <td>5.349885e-01</td>\n",
       "      <td>0.867660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>2.055812e+00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            m8_raw       m8_std  m8_zero-centered      m8_final\n",
       "count  9090.000000  9090.000000      1.332400e+04  10207.000000\n",
       "mean      0.946877     0.053123      5.119491e-17      0.649161\n",
       "std       0.080329     0.080329      1.000000e+00      0.299055\n",
       "min       0.440000     0.000000     -7.310172e-01      0.000000\n",
       "25%       0.933268     0.002891     -7.310172e-01      0.544541\n",
       "50%       0.979499     0.020501     -4.778693e-01      0.772290\n",
       "75%       0.997109     0.066732      5.349885e-01      0.867660\n",
       "max       1.000000     0.560000      2.055812e+00      1.000000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_combined[['m8_raw','m8_std','m8_zero-centered','m8_final']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regularity_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10844.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.185482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.100525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.083352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.118102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.150246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.212495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.698691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       regularity_index\n",
       "count      10844.000000\n",
       "mean           0.185482\n",
       "std            0.100525\n",
       "min            0.083352\n",
       "25%            0.118102\n",
       "50%            0.150246\n",
       "75%            0.212495\n",
       "max            0.698691"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mde_grid[['regularity_index']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subdivisions2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
