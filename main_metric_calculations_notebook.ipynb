{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_NAME = 'sara'\n",
    "\n",
    "AWS_PROFILE = 'cities'\n",
    "\n",
    "grid_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = \"s3://wri-cities-sandbox/identifyingLandSubdivisions/data\"\n",
    "INPUT_PATH = f'{MAIN_PATH}/input'\n",
    "CITY_INFO_PATH = f'{INPUT_PATH}/city_info'\n",
    "EXTENTS_PATH = f'{CITY_INFO_PATH}/extents'\n",
    "BUILDINGS_PATH = f'{INPUT_PATH}/buildings'\n",
    "BLOCKS_PATH = f'{INPUT_PATH}/blocks'\n",
    "ROADS_PATH = f'{INPUT_PATH}/roads'\n",
    "INTERSECTIONS_PATH = f'{INPUT_PATH}/intersections'\n",
    "GRIDS_PATH = f'{INPUT_PATH}/city_info/grids'\n",
    "SEARCH_BUFFER_PATH = f'{INPUT_PATH}/city_info/search_buffers'\n",
    "OUTPUT_PATH = f'{MAIN_PATH}/output'\n",
    "OUTPUT_PATH_CSV = f'{OUTPUT_PATH}/csv'\n",
    "OUTPUT_PATH_RASTER = f'{OUTPUT_PATH}/raster'\n",
    "OUTPUT_PATH_PNG = f'{OUTPUT_PATH}/png'\n",
    "OUTPUT_PATH_RAW = f'{OUTPUT_PATH}/raw_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '046P9REZSNEXTGV9',\n",
       "  'HostId': 'K2SrnrZzJ+cg2K81+tWGZQ1iohMhceldYNpVtI8SXXgX+UepAZ1NeEEWf6Ij4EyjR60u6PjGg0x1RbwRlA1OBg==',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'K2SrnrZzJ+cg2K81+tWGZQ1iohMhceldYNpVtI8SXXgX+UepAZ1NeEEWf6Ij4EyjR60u6PjGg0x1RbwRlA1OBg==',\n",
       "   'x-amz-request-id': '046P9REZSNEXTGV9',\n",
       "   'date': 'Wed, 14 May 2025 20:16:08 GMT',\n",
       "   'content-type': 'application/xml',\n",
       "   'transfer-encoding': 'chunked',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'Buckets': [{'Name': 'aft-sandbox-540362055257',\n",
       "   'CreationDate': datetime.datetime(2022, 9, 13, 15, 12, 20, tzinfo=tzutc())},\n",
       "  {'Name': 'amplify-citiesindicatorsapi-dev-10508-deployment',\n",
       "   'CreationDate': datetime.datetime(2023, 8, 30, 5, 5, 13, tzinfo=tzutc())},\n",
       "  {'Name': 'cities-heat',\n",
       "   'CreationDate': datetime.datetime(2023, 6, 1, 13, 22, 1, tzinfo=tzutc())},\n",
       "  {'Name': 'cities-test-sandbox',\n",
       "   'CreationDate': datetime.datetime(2025, 4, 18, 19, 10, 49, tzinfo=tzutc())},\n",
       "  {'Name': 'era5-brazil',\n",
       "   'CreationDate': datetime.datetime(2025, 2, 15, 19, 51, 14, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-athena-us-west-2',\n",
       "   'CreationDate': datetime.datetime(2024, 1, 12, 18, 45, 11, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-climate-hazards',\n",
       "   'CreationDate': datetime.datetime(2024, 1, 3, 16, 57, 31, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-data-api',\n",
       "   'CreationDate': datetime.datetime(2024, 7, 16, 8, 53, 31, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-heat',\n",
       "   'CreationDate': datetime.datetime(2024, 3, 25, 15, 46, 55, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-indicators',\n",
       "   'CreationDate': datetime.datetime(2024, 5, 13, 15, 50, 58, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-sandbox',\n",
       "   'CreationDate': datetime.datetime(2024, 7, 27, 0, 51, 38, tzinfo=tzutc())}],\n",
       " 'Owner': {'DisplayName': 'aws-cities',\n",
       "  'ID': 'df12253943982d72f60594f06c2cacf9a1ee3a9e738c1649c9fb96e5127f1a5c'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check s3 connection using AWS_PROFILE=CitiesUserPermissionSet profile \n",
    "import boto3\n",
    "\n",
    "session = boto3.Session(profile_name=AWS_PROFILE)\n",
    "s3 = session.client('s3')\n",
    "\n",
    "# export CitiesUserPermissionSet profile to use in the next cells\n",
    "import os\n",
    "os.environ['AWS_PROFILE'] = AWS_PROFILE\n",
    "\n",
    "s3.list_buckets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-05-14 15:16:11,067][INFO    ][coiled] Fetching latest package priorities...\n",
      "[2025-05-14 15:16:11,069][INFO    ][coiled.package_sync] Resolving your local subdivisions2 Python environment...\n",
      "[2025-05-14 15:16:12,206][INFO    ][coiled.package_sync] Scanning 444 conda packages...\n",
      "[2025-05-14 15:16:12,214][INFO    ][coiled.package_sync] Scanning 259 python packages...\n",
      "[2025-05-14 15:16:13,647][INFO    ][coiled] Running pip check...\n",
      "[2025-05-14 15:16:15,555][INFO    ][coiled] Validating environment...\n",
      "[2025-05-14 15:16:17,732][INFO    ][coiled] Creating wheel for ~/Documents/Identifying Land Subdivisions/identifyingLandSubdivisions...\n",
      "[2025-05-14 15:16:17,963][WARNING ][coiled.package_sync] Package - libopenvino-intel-cpu-plugin, libopenvino-intel-cpu-plugin~=2025.0.0 has no install candidate for Python 3.12 linux-aarch64 on conda-forge\n",
      "[2025-05-14 15:16:17,965][INFO    ][coiled] Uploading coiled_local_identifyingLandSubdivisions...\n",
      "[2025-05-14 15:16:19,771][INFO    ][coiled] Requesting package sync build...\n",
      "[2025-05-14 15:16:20,653][INFO    ][coiled] Creating Cluster (name: ils-sara, https://cloud.coiled.io/clusters/879387?account=wri-cities-data ). This usually takes 1-2 minutes...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started a new Dask client on Coiled. Dashboard is available at https://cluster-qdjsp.dask.host/TEIgUmffcQ6JR55s/status\n"
     ]
    }
   ],
   "source": [
    "# START COILED CLIENT.\n",
    "\n",
    "import coiled\n",
    "\n",
    "cluster = coiled.Cluster(\n",
    "    workspace=\"wri-cities-data\",\n",
    "    name=f'ils-{YOUR_NAME}',\n",
    "    region=\"us-west-2\",\n",
    "    arm=True,\n",
    "    worker_vm_types=\"r8g.xlarge\",\n",
    "    spot_policy=\"spot\",\n",
    "    n_workers=4,\n",
    "    package_sync_ignore=[\"pyspark\", \"pypandoc\"]\n",
    ")\n",
    "client = cluster.get_client()\n",
    "\n",
    "print(f\"Started a new Dask client on Coiled. Dashboard is available at {client.dashboard_link}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_geopandas as dgpd\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from dask import delayed, compute, visualize\n",
    "import geopandas as gpd\n",
    "from dask.diagnostics import ProgressBar\n",
    "from shapely.geometry import MultiLineString, LineString, Point\n",
    "from shapely.ops import polygonize, nearest_points\n",
    "#from shapely.geometry import Polygon, LineString, Point, MultiPolygon, MultiLineString, GeometryCollection\n",
    "from scipy.optimize import fminbound, minimize\n",
    "#from unused_code.metrics_groupby import metrics\n",
    "from dask import delayed\n",
    "import dask_geopandas as dgpd\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely import wkb\n",
    "from scipy.stats import entropy\n",
    "import time\n",
    "from dask import compute\n",
    "\n",
    "from pre_processing import *\n",
    "from auxiliary_functions import *\n",
    "from standardize_metrics import *\n",
    "from metrics_calculation import *\n",
    "\n",
    "import time\n",
    "\n",
    "YOUR_NAME = 'sara'\n",
    "grid_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE BUILDING DISTANCES TO ROADS AND PRODUCE BLOCKS.\n",
    "\n",
    "start_time = time.time()  # Start the timer\n",
    "\n",
    "%autoreload\n",
    "#from main import *\n",
    "\n",
    "def preprocess_all_cities(city_list):\n",
    "    #delayed_jobs = [delayed(calculate_building_distances_to_roads)(city) for city in city_list]\n",
    "    #delayed_jobs.append([delayed(produce_blocks)(city) for city in city_list])\n",
    "    delayed_jobs = []\n",
    "    delayed_jobs.append([calculate_building_distances_to_roads(city) for city in city_list])\n",
    "    delayed_jobs.append([produce_blocks(city,YOUR_NAME,grid_size) for city in city_list])\n",
    "    results = compute(*delayed_jobs)\n",
    "    return results\n",
    "\n",
    "preprocess_all_cities([\"Abidjan\", \"Accra\", \"Bamako\", \"Belo_Horizonte\", \"Bogota\", \"Campinas\", \"Cape_Town\", \"Lagos\",\"Luanda\", \"Nairobi\",\"Medellin\"]) \n",
    "#[\"Abidjan\", \"Accra\", \"Nairobi\", \"Bamako\", \"Belo_Horizonte\", \"Bogota\", \"Campinas\", \"Cape_Town\", \"Lagos\",\"Luanda\", \"Nairobi\", \"Medellin\"])\n",
    "\n",
    "end_time = time.time()  # End the timer\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Tasks completed in {elapsed_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRODUCE AZIMUTHS\n",
    "\n",
    "start_time = time.time()  # Start the timer\n",
    "\n",
    "\n",
    "city_list = cities = [\"Abidjan\", \"Accra\", \"Bamako\", \"Belo_Horizonte\", \"Bogota\", \"Campinas\", \"Cape_Town\", \"Lagos\",\"Luanda\", \"Nairobi\", \"Medellin\"]\n",
    "#[\"Abidjan\", \"Accra\", \"Nairobi\", \"Bamako\", \"Belo_Horizonte\", \"Bogota\", \"Campinas\", \"Cape_Town\", \"Lagos\",\"Luanda\", \"Nairobi\", \"Medellin\"])\n",
    "\n",
    "delayed_jobs = []\n",
    "delayed_jobs.append([produce_azimuths(city, YOUR_NAME, grid_size) for city in city_list])\n",
    "results = compute(*delayed_jobs)\n",
    "\n",
    "end_time = time.time()  # End the timer\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Tasks completed in {elapsed_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks completed in 79.13 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()  # Start the timer\n",
    "\n",
    "cities = [\"Nairobi\", \"Medellin\"]#\"Accra\", \"Bamako\", \"Belo_Horizonte\", \"Bogota\",\"Campinas\", \"Cape_Town\",\"Abidjan\",\"Luanda\",  ]\n",
    "#cities = ['Medellin']\n",
    "#\"Lagos\"\n",
    "cities = [city.replace(' ', '_') for city in cities]\n",
    "\n",
    "tasks = []\n",
    "for city in cities:\n",
    "    tasks.append(building_and_intersection_metrics(city,grid_size,YOUR_NAME))\n",
    "    tasks.append(building_distance_metrics(city, grid_size, YOUR_NAME))\n",
    "    tasks.append(compute_m6_m7(city, grid_size, YOUR_NAME))\n",
    "    tasks.append(metrics_roads_intersections(city, grid_size, YOUR_NAME))\n",
    "\n",
    "results = compute(*tasks)\n",
    "\n",
    "end_time = time.time()  # End the timer\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Tasks completed in {elapsed_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_geopandas as dgpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely import wkb\n",
    "\n",
    "def consolidate_irregularity_index(city_name):\n",
    "    # 1) Paths (same as before) …\n",
    "    base = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{grid_size}m'\n",
    "    p0 = f'{base}_metrics_3_4_5_10_11_12_grid_{YOUR_NAME}.geoparquet'\n",
    "    p1 = f'{base}_grid_{YOUR_NAME}_metrics_1_2.geoparquet'\n",
    "    p2 = f'{base}_grid_{YOUR_NAME}_metrics_6_7.geoparquet' \n",
    "    p3 = f'{base}_grid_metrics_8_9_{YOUR_NAME}.geoparquet'\n",
    "\n",
    "    # 2) Lazy-read  \n",
    "    g0 = dgpd.read_parquet(p0)\n",
    "    g1 = dgpd.read_parquet(p1)[['m1_std','m2_std','m1_raw','m2_raw','has_buildings']].drop(columns='geometry', errors='ignore')\n",
    "    g2 = dgpd.read_parquet(p2)[['m6_std','m7_std','m6_raw','m7_raw']].drop(columns='geometry', errors='ignore')\n",
    "    g3 = dgpd.read_parquet(p3)[['m8_std','m9_std','m8_raw','m9_raw']].drop(columns='geometry', errors='ignore')\n",
    "\n",
    "    # 3) Merge (keeps geometry)\n",
    "    df = (g0\n",
    "          .merge(g1, left_index=True, right_index=True, how='left')\n",
    "          .merge(g2, left_index=True, right_index=True, how='left')\n",
    "          .merge(g3, left_index=True, right_index=True, how='left'))\n",
    "\n",
    "    # 4) Mask\n",
    "    std_cols = [f'm{i}_std' for i in range(1,13)]\n",
    "    df['has_features'] = df['has_buildings'] | df['has_roads'] | df['has_intersections']\n",
    "    df[std_cols] = df[std_cols].where(df['has_features'], np.nan)\n",
    "\n",
    "    # 5) Persist & repartition\n",
    "    df = df.persist().repartition(npartitions=8)\n",
    "\n",
    "    all_metrics_columns_raw = ['m'+str(x)+'_raw' for x in range(1,13)]\n",
    "    all_metrics_columns_final = ['m'+str(x)+'_final' for x in range(1,13)]\n",
    "    all_metrics_columns_std = ['m'+str(x)+'_std' for x in range(1,13)]\n",
    "    all_metrics_columns_zc = ['m'+str(x)+'_zc' for x in range(1,13)]\n",
    "    \n",
    "    # 6) Compute global scalars on standardized metrics\n",
    "    means = df[all_metrics_columns_std].mean().compute()\n",
    "    stds  = df[all_metrics_columns_std].std().compute()\n",
    "    mins  = df[all_metrics_columns_std].min().compute()\n",
    "    maxs  = df[all_metrics_columns_std].max().compute()\n",
    "\n",
    "    # Convert those into z-mins / z-maxs and re-index to your zc_cols\n",
    "    zmin = (mins - means) / stds\n",
    "    zmax = (maxs - means) / stds\n",
    "    zmin.index = all_metrics_columns_zc\n",
    "    zmax.index = all_metrics_columns_zc\n",
    "\n",
    "    # 7) One-pass normalize + index\n",
    "    def normalize_partition(pdf, means, stds, zmin, zmax):\n",
    "        # zero-center -> z DataFrame\n",
    "        z = (pdf[all_metrics_columns_std] - means) / stds\n",
    "        z.columns = all_metrics_columns_zc\n",
    "        pdf[all_metrics_columns_zc] = z\n",
    "\n",
    "        # min–max on z -> f DataFrame\n",
    "        f = (z - zmin) / (zmax - zmin)\n",
    "        f.columns = all_metrics_columns_final\n",
    "        pdf[all_metrics_columns_final] = f\n",
    "\n",
    "        groupA = [f\"m{i}_final\" for i in range(1,10)]        # metrics 1–9\n",
    "        groupB = groupA + [\"m12_final\"]                      # metrics 1–9 plus 12\n",
    "        road_metrics = ['m1_final','m2_final','m3_final','m5_final','m7_final']\n",
    "        regularity_metrics = ['m6_final','m10_final','m12_final']\n",
    "\n",
    "        # … after pdf[all_metrics_columns_final] = f …\n",
    "        # compute two regularity indices and their NA‐counts\n",
    "        pdf[\"regularity_index_A\"] = f[groupA].mean(axis=1)\n",
    "        pdf[\"na_count_A\"] = f[groupA].isna().sum(axis=1)\n",
    "\n",
    "        pdf[\"regularity_index_B\"] = f[groupB].mean(axis=1)\n",
    "        pdf[\"na_count_B\"] = f[groupB].isna().sum(axis=1)\n",
    "\n",
    "        pdf['subdivisions_index'] = f[road_metrics].mean(axis=1)\n",
    "        pdf['second_stage'] = f[regularity_metrics].mean(axis=1)\n",
    "        \n",
    "        return pdf\n",
    "\n",
    "\n",
    "    # 8) Build meta and map_partitions\n",
    "    meta = df._meta.copy()\n",
    "    for c in (\n",
    "        all_metrics_columns_zc\n",
    "      + all_metrics_columns_final\n",
    "      + ['regularity_index_A','na_count_A','regularity_index_B','na_count_B','subdivisions_index','second_stage']\n",
    "    ):\n",
    "        meta[c] = pd.Series(dtype='float64')\n",
    "\n",
    "\n",
    "    df = df.map_partitions(\n",
    "        normalize_partition,\n",
    "        means, stds, zmin, zmax,\n",
    "        meta=meta\n",
    "    )\n",
    "\n",
    "    import geopandas as gpd\n",
    "    \n",
    "    out = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{grid_size}m_all_metrics_combined_{YOUR_NAME}.geoparquet'\n",
    "    # … after your df = df.map_partitions(...), instead of WKB+pd.to_parquet:\n",
    "    pdf = df.compute()\n",
    "\n",
    "    # rehydrate into a GeoDataFrame, borrow the original CRS:\n",
    "    gdf = gpd.GeoDataFrame(pdf, geometry='geometry', crs=g0.crs)\n",
    "\n",
    "    # write **one** parquet file, with embedded CRS and GeoArrow extension types:\n",
    "    gdf.to_parquet(out, engine='pyarrow', index=False)\n",
    "\n",
    "    '''\n",
    "    # 9) Compute into pandas, then convert geometry to WKB\n",
    "    pdf = df.compute()\n",
    "    pdf['geometry'] = pdf.geometry.apply(lambda geom: wkb.dumps(geom, hex=False))\n",
    "\n",
    "    # 10) Write one parquet file via pandas (supports single_file)\n",
    "    \n",
    "    pdf.to_parquet(out, engine='pyarrow', index=False)#single_file=True, \n",
    "    #df.to_parquet(out, engine='pyarrow', index=False)#\n",
    "    '''\n",
    "    return out\n",
    "\n",
    "# Dispatch in parallel:\n",
    "cities = [\"Nairobi\",\"Medellin\"]#\"Abidjan\",\"Accra\",\"Bamako\",\"Belo_Horizonte\",\"Bogota\",\"Campinas\",\"Cape_Town\",\"Luanda\",\n",
    "#cities = ['Medellin']\n",
    "futures = client.map(consolidate_irregularity_index, cities)\n",
    "results = client.gather(futures)\n",
    "print(\"Written outputs:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "#cities =[\"Abidjan\",\"Accra\",\"Bamako\",\"Belo_Horizonte\",\"Bogota\", \"Nairobi\",\"Campinas\",\"Cape_Town\",\"Luanda\",\"Medellin\"]\n",
    "cities = ['Medellin']\n",
    "paths = [f'{OUTPUT_PATH_RASTER}/{city}/{city}_{grid_size}m_all_metrics_combined_{YOUR_NAME}.geoparquet' \n",
    "         for city in cities]\n",
    "\n",
    "# Read into GeoPandas and concat\n",
    "gdfs = [gpd.read_parquet(path) for path in paths]\n",
    "combined = gpd.GeoDataFrame(pd.concat(gdfs, ignore_index=True), crs=gdfs[0].crs)\n",
    "\n",
    "# Write a single GeoPackage file\n",
    "combined.to_file('all_cities_combined.gpkg', driver='GPKG')\n",
    "print(\"Combined GeoPackage saved as all_cities_combined.gpkg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['m9_raw'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_combined.to_parquet('../Medellin_consolidated_index_results.parquet')#regularity_index.describe() #[['m1_raw','m2_raw','regularity_index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_combined.to_file('../Medellin_consolidated_index_results.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mde_grid = gpd.read_parquet('../Medellin_consolidated_index_results.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_combined[['m1_raw','m1_std','m1_zero-centered','m1_final']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_combined[['m2_raw','m2_std','m2_zero-centered','m2_final']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_combined[['m8_raw','m8_std','m8_zero-centered','m8_final']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mde_grid[['regularity_index']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subdivisions2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
