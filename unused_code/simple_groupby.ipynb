{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_NAME = 'sara'\n",
    "\n",
    "AWS_PROFILE = 'cities'\n",
    "\n",
    "'''\n",
    "# List of cities to process\n",
    "cities = [\"Belo Horizonte\", \"Campinas\"]#, \"Bogota\", \"Nairobi\", \"Bamako\", \n",
    "        #\"Lagos\", \"Accra\", \"Abidjan\", \"Mogadishu\", \"Cape Town\", \n",
    "        #\"Maputo\", \"Luanda\"]\n",
    "\n",
    "test_cities = [\"Belo Horizonte\"]\n",
    "#cities = test_cities\n",
    "\n",
    "cities = [city.replace(' ', '_') for city in cities]\n",
    "\n",
    "search_buffer_files = fs.ls(SEARCH_BUFFER_PATH)\n",
    "\n",
    "cities \n",
    "\n",
    "number_of_cities = len(cities)\n",
    "\n",
    "print(f'City count: {number_of_cities}')\n",
    "'''\n",
    "grid_size = 200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = \"s3://wri-cities-sandbox/identifyingLandSubdivisions/data\"\n",
    "INPUT_PATH = f'{MAIN_PATH}/input'\n",
    "CITY_INFO_PATH = f'{INPUT_PATH}/city_info'\n",
    "EXTENTS_PATH = f'{CITY_INFO_PATH}/extents'\n",
    "BUILDINGS_PATH = f'{INPUT_PATH}/buildings'\n",
    "ROADS_PATH = f'{INPUT_PATH}/roads'\n",
    "INTERSECTIONS_PATH = f'{INPUT_PATH}/intersections'\n",
    "GRIDS_PATH = f'{INPUT_PATH}/city_info/grids'\n",
    "SEARCH_BUFFER_PATH = f'{INPUT_PATH}/city_info/search_buffers'\n",
    "OUTPUT_PATH = f'{MAIN_PATH}/output'\n",
    "OUTPUT_PATH_CSV = f'{OUTPUT_PATH}/csv'\n",
    "OUTPUT_PATH_RASTER = f'{OUTPUT_PATH}/raster'\n",
    "OUTPUT_PATH_PNG = f'{OUTPUT_PATH}/png'\n",
    "OUTPUT_PATH_RAW = f'{OUTPUT_PATH}/raw_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'T4S4R9QXGQNM9D1F',\n",
       "  'HostId': 'KenjYedQDLTpBP3+KIFg9XicS+tABZTpug2/1rxqK/hId+682+e8TS7eGp/ZG7ngK0ILkQ/8bZ5GHLmQffpqFOftt/KGuunp',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'KenjYedQDLTpBP3+KIFg9XicS+tABZTpug2/1rxqK/hId+682+e8TS7eGp/ZG7ngK0ILkQ/8bZ5GHLmQffpqFOftt/KGuunp',\n",
       "   'x-amz-request-id': 'T4S4R9QXGQNM9D1F',\n",
       "   'date': 'Tue, 25 Mar 2025 03:57:22 GMT',\n",
       "   'content-type': 'application/xml',\n",
       "   'transfer-encoding': 'chunked',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'Buckets': [{'Name': 'aft-sandbox-540362055257',\n",
       "   'CreationDate': datetime.datetime(2022, 9, 13, 15, 12, 20, tzinfo=tzutc())},\n",
       "  {'Name': 'amplify-citiesindicatorsapi-dev-10508-deployment',\n",
       "   'CreationDate': datetime.datetime(2023, 8, 30, 5, 5, 13, tzinfo=tzutc())},\n",
       "  {'Name': 'cities-dev-sandbox',\n",
       "   'CreationDate': datetime.datetime(2025, 2, 7, 23, 18, 12, tzinfo=tzutc())},\n",
       "  {'Name': 'cities-heat',\n",
       "   'CreationDate': datetime.datetime(2023, 6, 1, 13, 22, 1, tzinfo=tzutc())},\n",
       "  {'Name': 'era5-brazil',\n",
       "   'CreationDate': datetime.datetime(2025, 2, 15, 19, 51, 14, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-athena-us-west-2',\n",
       "   'CreationDate': datetime.datetime(2024, 1, 12, 18, 45, 11, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-climate-hazards',\n",
       "   'CreationDate': datetime.datetime(2024, 1, 3, 16, 57, 31, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-data-api',\n",
       "   'CreationDate': datetime.datetime(2024, 7, 16, 8, 53, 31, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-heat',\n",
       "   'CreationDate': datetime.datetime(2024, 3, 25, 15, 46, 55, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-indicators',\n",
       "   'CreationDate': datetime.datetime(2024, 5, 13, 15, 50, 58, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-sandbox',\n",
       "   'CreationDate': datetime.datetime(2024, 7, 27, 0, 51, 38, tzinfo=tzutc())}],\n",
       " 'Owner': {'DisplayName': 'aws-cities',\n",
       "  'ID': 'df12253943982d72f60594f06c2cacf9a1ee3a9e738c1649c9fb96e5127f1a5c'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check s3 connection using AWS_PROFILE=CitiesUserPermissionSet profile \n",
    "import boto3\n",
    "\n",
    "session = boto3.Session(profile_name=AWS_PROFILE)\n",
    "s3 = session.client('s3')\n",
    "\n",
    "# export CitiesUserPermissionSet profile to use in the next cells\n",
    "import os\n",
    "os.environ['AWS_PROFILE'] = AWS_PROFILE\n",
    "\n",
    "\n",
    "s3.list_buckets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-24 22:57:26,398][INFO    ][coiled] Using existing cluster: 'ils-sara (id: 808133)'\n",
      "[2025-03-24 22:57:26,400][INFO    ][coiled] Attaching to existing cluster (name: ils-sara, https://cloud.coiled.io/clusters/808133?account=wri-cities-data )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started a new Dask client on Coiled. Dashboard is available at https://cluster-dkufr.dask.host/qpgfqSlVNtf2bAVE/status\n"
     ]
    }
   ],
   "source": [
    "import coiled\n",
    "\n",
    "cluster = coiled.Cluster(\n",
    "    workspace=\"wri-cities-data\",\n",
    "    name=f'ils-{YOUR_NAME}',\n",
    "    region=\"us-west-2\",\n",
    "    arm=True,\n",
    "    worker_vm_types=\"r8g.xlarge\",\n",
    "    spot_policy=\"spot\",\n",
    "    n_workers=8,\n",
    "    package_sync_ignore=[\"pyspark\", \"pypandoc\"]\n",
    ")\n",
    "client = cluster.get_client()\n",
    "\n",
    "print(f\"Started a new Dask client on Coiled. Dashboard is available at {client.dashboard_link}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import s3fs\n",
    "import fsspec\n",
    "import traceback\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "fs = s3fs.S3FileSystem(anon=False)\n",
    "search_buffer_files = fs.ls(SEARCH_BUFFER_PATH)\n",
    "\n",
    "cities = [x.split('/')[-1] for x in search_buffer_files]\n",
    "len(cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.ops import linemerge\n",
    "import dask_geopandas as dgpd\n",
    "from dask import delayed\n",
    "\n",
    "@delayed\n",
    "def compute_partial_union(partition):\n",
    "    \"\"\"Compute unary_union for a partition.\"\"\"\n",
    "    return partition.unary_union\n",
    "\n",
    "@delayed\n",
    "def merge_unions(partial_unions):\n",
    "    \"\"\"Merge partial unions into a single MultiLineString or LineString.\"\"\"\n",
    "    lines = []\n",
    "    for geom in partial_unions:\n",
    "        if geom and not geom.is_empty:\n",
    "            if geom.geom_type == \"LineString\":\n",
    "                lines.append(geom)\n",
    "            elif geom.geom_type == \"MultiLineString\":\n",
    "                lines.extend(geom.geoms)  # Extract individual LineStrings\n",
    "    \n",
    "    if len(lines) == 0:\n",
    "        return None  # Handle empty results\n",
    "    \n",
    "    return linemerge(lines)  # Final merge\n",
    "\n",
    "@delayed\n",
    "def get_unionized_roads(roads):\n",
    "    \"\"\"Efficient computation of unary_union for roads using Dask.\"\"\"\n",
    "    partitions = roads.to_delayed()  # This only works if `roads` is a Dask GeoDataFrame\n",
    "    partial_unions = [compute_partial_union(part) for part in partitions]\n",
    "    final_union = merge_unions(partial_unions)\n",
    "    return final_union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks completed in 22.97 seconds.\n"
     ]
    }
   ],
   "source": [
    "import dask_geopandas as dgpd\n",
    "import pandas as pd\n",
    "from dask import delayed, compute, visualize\n",
    "import geopandas as gpd\n",
    "from dask.diagnostics import ProgressBar\n",
    "%autoreload\n",
    "from citywide_calculation import get_utm_crs\n",
    "from metrics_calculation import calculate_minimum_distance_to_roads_option_B\n",
    "from shapely.geometry import MultiLineString, LineString, Point\n",
    "from shapely.ops import polygonize, nearest_points\n",
    "#from shapely.geometry import Polygon, LineString, Point, MultiPolygon, MultiLineString, GeometryCollection\n",
    "from scipy.optimize import fminbound, minimize\n",
    "\n",
    "@delayed\n",
    "def get_epsg(city_name):\n",
    "    search_buffer = f'{SEARCH_BUFFER_PATH}/{city_name}/{city_name}_search_buffer.geoparquet'\n",
    "    extent = dgpd.read_parquet(search_buffer)\n",
    "    geometry = extent.geometry[0].compute()\n",
    "    epsg = get_utm_crs(geometry)\n",
    "    print(f'{city_name} EPSG: {epsg}')\n",
    "    return epsg\n",
    "\n",
    "'''\n",
    "@delayed\n",
    "def load_dataset(path, epsg=None):\n",
    "    \"\"\"Load a single parquet dataset\"\"\"\n",
    "    dataset = dgpd.read_parquet(path, npartitions=2)\n",
    "    if epsg:\n",
    "        dataset = dataset.set_crs(\"EPSG:4326\", allow_override=True) \n",
    "        dataset = dataset.to_crs(epsg=epsg)\n",
    "    return dataset\n",
    "'''\n",
    "\n",
    "def load_dataset(path, epsg=None):\n",
    "    \"\"\"Load a single parquet dataset lazily\"\"\"\n",
    "    dataset = dgpd.read_parquet(path, npartitions=2)\n",
    "    if epsg:\n",
    "        dataset = dataset.set_crs(\"EPSG:4326\", allow_override=True).to_crs(epsg)\n",
    "    return dataset\n",
    "\n",
    "@delayed\n",
    "def row_count(dgdf):\n",
    "    \"\"\"Count the rows in a dataframe\"\"\"\n",
    "    row_count = dgdf.map_partitions(len).compute().sum()\n",
    "\n",
    "    return row_count\n",
    "\n",
    "\n",
    "def test_math(input):\n",
    "    return input + input\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "%autoreload\n",
    "from metrics_groupby import metrics\n",
    "\n",
    "@delayed\n",
    "def metrics(city_name,YOUR_NAME,grid_size):\n",
    "    grid_cell_count = 0\n",
    "    paths = {\n",
    "        'grid': f'{GRIDS_PATH}/{city_name}/{city_name}_{str(grid_size)}m_grid.geoparquet',\n",
    "        'buildings': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}.geoparquet',\n",
    "        'roads': f'{ROADS_PATH}/{city_name}/{city_name}_OSM_roads.geoparquet',\n",
    "        'intersections': f'{INTERSECTIONS_PATH}/{city_name}/{city_name}_OSM_intersections.geoparquet'\n",
    "    }\n",
    "    # Get EPSG\n",
    "    epsg = get_epsg(city_name).compute()\n",
    "    # Load grid\n",
    "    grid = load_dataset(paths['grid'], epsg=epsg)#.compute()\n",
    "    if 'geom' in grid.columns:\n",
    "        grid = grid.drop(columns=['geom'])\n",
    "    grid['cell_area'] = grid.geometry.area\n",
    "\n",
    "    cells = grid.index.size\n",
    "    grid_cell_count += cells\n",
    "\n",
    "    # Load buildings and perform relevant calculations on it\n",
    "    buildings = load_dataset(paths['buildings'], epsg=epsg)#.compute()\n",
    "    buildings['area'] = buildings.geometry.area\n",
    "    joined_buildings = dgpd.sjoin(buildings, grid, predicate='within')  \n",
    "    counts_buildings = joined_buildings.groupby('index_right').size()\n",
    "    grid['n_buildings'] = grid.index.map(counts_buildings).fillna(0).astype(int)\n",
    "    built_area_buildings = joined_buildings.groupby('index_right')['area'].sum()\n",
    "    grid['built_area'] = grid.index.map(built_area_buildings).fillna(0).astype(float)\n",
    "\n",
    "    #total_buildings = row_count(buildings).compute()\n",
    "    #print(total_buildings)\n",
    "    # Load roads\n",
    "    roads = load_dataset(paths['roads'], epsg=epsg)#.compute()\n",
    "    \n",
    "    #road_union = roads.unary_union.compute()\n",
    "    #roads = roads.compute()\n",
    "\n",
    "    # Load intersections\n",
    "    intersections = load_dataset(paths['intersections'], epsg=epsg)#.compute()\n",
    "\n",
    "    intersections_3plus = intersections[intersections.street_count >= 3]\n",
    "    intersections_4way = intersections[intersections.street_count == 4]\n",
    "\n",
    "    grid['cell_area_km2'] = grid['cell_area']/1000000.\n",
    "    \n",
    "    roads_grid_joined = dgpd.sjoin(roads, grid, predicate='within')\n",
    "    road_length_km = roads_grid_joined.groupby('index_right')['length'].sum()/1000.\n",
    "    grid['road_length'] = grid.index.map(road_length_km).fillna(0).astype(float)\n",
    "\n",
    "\n",
    "    joined_intersections_3plus = dgpd.sjoin(intersections_3plus, grid, predicate='within')\n",
    "    counts_intersections_3plus = joined_intersections_3plus.groupby('index_right').size()\n",
    "    grid['intersections_3plus'] = grid.index.map(counts_intersections_3plus).fillna(0).astype(int)\n",
    "\n",
    "    joined_intersections_4way = dgpd.sjoin(intersections_4way, grid, predicate='within')\n",
    "    counts_intersections_4way = joined_intersections_4way.groupby('index_right').size()\n",
    "    grid['intersections_4way'] = grid.index.map(counts_intersections_4way).fillna(0).astype(int)\n",
    "\n",
    "\n",
    "    grid['m3'] = grid['road_length']/grid['cell_area_km2']\n",
    "    grid['m4'] = grid['intersections_4way'] / grid['intersections_3plus']\n",
    "    grid['m5'] =  (1000.**2)*(grid['intersections_4way']/grid['cell_area']) #make sure this is equivalent to the meter calculation\n",
    "\n",
    "    \n",
    "\n",
    "    grid['m11'] = 1.0*grid['n_buildings'] / grid['cell_area'] # Building density\n",
    "    grid['m12'] = grid['built_area'] / grid['cell_area'] # Built area share\n",
    "    grid['m13'] = grid['built_area'] / grid['n_buildings'] # Average building area\n",
    "\n",
    "    path = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{str(grid_size)}m_grid_{YOUR_NAME}.geoparquet'\n",
    "\n",
    "    if 'geom' in grid.columns:\n",
    "        grid = grid.drop(columns='geom')\n",
    "\n",
    "    grid.to_parquet(path)\n",
    "    return grid_cell_count, path\n",
    "\n",
    "\n",
    "import time\n",
    "start_time = time.time()  # Start the timer\n",
    "\n",
    "cities = ['Nairobi']\n",
    "\n",
    "for city in cities:\n",
    "    metrics(city, YOUR_NAME, grid_size=grid_size).compute()\n",
    "\n",
    "end_time = time.time()  # End the timer\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Tasks completed in {elapsed_time:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of ['Intersection ID'] are in the columns\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mt/3n9j2kc92kv4psztx687vtd80000gn/T/ipykernel_7971/343425947.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;31m# Merge with street_count, then apply your “smallest angles” logic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0mintersection_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintersections_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'osmid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'street_count'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'geometry'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'osmid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m angles_merged = intersection_meta.merge(\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mdf_angles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Intersection ID'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inner'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[1;32m   6118\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6119\u001b[0m                         \u001b[0mmissing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\u001b[0m\u001b[0;34mNone of \u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m are in the columns\u001b[0m\u001b[0;34m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6124\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6125\u001b[0m             \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of ['Intersection ID'] are in the columns\""
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from shapely.geometry import Polygon, LineString, Point, MultiPolygon, MultiLineString, GeometryCollection\n",
    "\n",
    "# Function to calculate the angle between two vectors\n",
    "def calculate_angle(vector1, vector2):\n",
    "    angle = np.arctan2(vector2[1], vector2[0]) - np.arctan2(vector1[1], vector1[0])\n",
    "    angle = np.degrees(angle)\n",
    "    if angle < 0:\n",
    "        angle += 360\n",
    "    return angle\n",
    "\n",
    "def extract_coords(geometry):\n",
    "    if isinstance(geometry, (LineString, Polygon)):\n",
    "        # If it's a single geometry, return its coordinates\n",
    "        return list(geometry.coords)\n",
    "    elif isinstance(geometry, (MultiLineString, MultiPolygon)):\n",
    "        # If it's a multi-part geometry, iterate through the sub-geometries\n",
    "        coords = []\n",
    "        for part in geometry.geoms:  # Use .geoms to access each part of the MultiLineString/MultiPolygon\n",
    "            coords.extend(list(part.coords))\n",
    "        return coords\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported geometry type: {type(geometry)}\")\n",
    "\n",
    "def calculate_sequential_angles_option_A(intersections, roads):\n",
    "    records = []  # List to store angle records\n",
    "\n",
    "    # Iterate through each intersection\n",
    "    for _, intersection in intersections.iterrows():\n",
    "        intersection_id = intersection['osmid']\n",
    "        intersection_point = intersection.geometry\n",
    "        \n",
    "        # Get all roads connected to the intersection\n",
    "        connected_roads = roads[(roads['u'] == intersection_id) | (roads['v'] == intersection_id)]\n",
    "        vectors = []\n",
    "        \n",
    "        for _, road in connected_roads.iterrows():\n",
    "            #coords = list(road.geometry.coords)\n",
    "            coords = extract_coords(road.geometry)\n",
    "            \n",
    "            # Determine the vector for the road segment away from the intersection\n",
    "            if road['u'] == intersection_id:\n",
    "                vector = (coords[1][0] - coords[0][0], coords[1][1] - coords[0][1])\n",
    "            else:\n",
    "                vector = (coords[-2][0] - coords[-1][0], coords[-2][1] - coords[-1][1])\n",
    "            \n",
    "            vectors.append((vector, road['u'], road['v']))\n",
    "\n",
    "        # Sort vectors based on the angle relative to a fixed axis (e.g., x-axis)\n",
    "        vectors.sort(key=lambda v: np.arctan2(v[0][1], v[0][0]))\n",
    "\n",
    "        # Calculate the sequential angles between each pair of vectors\n",
    "        for i in range(len(vectors)):\n",
    "            vector1 = vectors[i][0]\n",
    "            vector2 = vectors[(i + 1) % len(vectors)][0]  # Next vector, looping back to the start\n",
    "            angle = calculate_angle(vector1, vector2)\n",
    "            \n",
    "            record = {\n",
    "                'Intersection ID': intersection_id,\n",
    "                'Segment 1': (vectors[i][1], vectors[i][2]),\n",
    "                'Segment 2': (vectors[(i + 1) % len(vectors)][1], vectors[(i + 1) % len(vectors)][2]),\n",
    "                'Angle': angle\n",
    "            }\n",
    "            records.append(record)\n",
    "\n",
    "    # Create a DataFrame from the records\n",
    "    df_angles = pd.DataFrame(records)\n",
    "    \n",
    "    return df_angles\n",
    "\n",
    "\n",
    "city_name = 'Nairobi'\n",
    "\n",
    "paths = {\n",
    "    'grid': f'{GRIDS_PATH}/{city_name}/{city_name}_{str(grid_size)}m_grid.geoparquet',\n",
    "    'buildings': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}.geoparquet',\n",
    "    'roads': f'{ROADS_PATH}/{city_name}/{city_name}_OSM_roads.geoparquet',\n",
    "    'intersections': f'{INTERSECTIONS_PATH}/{city_name}/{city_name}_OSM_intersections.geoparquet'\n",
    "}\n",
    "\n",
    "epsg = get_epsg(city_name).compute()\n",
    "\n",
    "roads = load_dataset(paths['roads'], epsg=epsg)#.compute()\n",
    "intersections = load_dataset(paths['intersections'], epsg=epsg)#.compute()\n",
    "grid = load_dataset(paths['grid'], epsg=epsg)\n",
    "\n",
    "# Compute intersection‑level angles (in memory)\n",
    "intersections_df = intersections.compute()\n",
    "roads_df         = roads.compute()\n",
    "grid_df          = grid.compute()\n",
    "\n",
    "# Build full angles DataFrame\n",
    "df_angles = calculate_sequential_angles_option_A(intersections_df, roads_df)\n",
    "\n",
    "# Merge with street_count, then apply your “smallest angles” logic\n",
    "intersection_meta = intersections_df[['osmid','street_count','geometry']].set_index('osmid')\n",
    "angles_merged = intersection_meta.merge(\n",
    "    df_angles.set_index('Intersection ID'),\n",
    "    left_index=True, right_index=True,\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# 3‑way → keep smallest; 4+ → keep two smallest\n",
    "keep3 = angles_merged[angles_merged.street_count == 3].groupby(level=0).Angle.min()\n",
    "keep4 = angles_merged[angles_merged.street_count >= 4].groupby(level=0).apply(lambda g: g.nsmallest(2, 'Angle')).reset_index(level=0, drop=True)\n",
    "\n",
    "kept = pd.concat([keep3, keep4]).reset_index().rename(columns={'index':'osmid','Angle':'Angle'})\n",
    "kept['angle_diff'] = (90 - kept['Angle']).abs()\n",
    "\n",
    "# Turn back into a GeoDataFrame\n",
    "kept_gdf = gpd.GeoDataFrame(\n",
    "    kept.merge(intersections_df[['osmid','geometry']], on='osmid'),\n",
    "    geometry='geometry', crs=intersections_df.crs\n",
    ")\n",
    "\n",
    "# Spatially join to grid & compute m10\n",
    "joined_m10 = gpd.sjoin(kept_gdf, grid, predicate='within')\n",
    "m10_by_cell = joined_m10.groupby('index_right')['angle_diff'].mean()\n",
    "#grid['m10'] = grid.index.map(m10_by_cell).fillna(0).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a real number, not 'GeoDataFrame'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 17\u001b[0m\n\u001b[1;32m     12\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtortuosity\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meuclid_dist\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m     15\u001b[0m roads \u001b[38;5;241m=\u001b[39m roads\u001b[38;5;241m.\u001b[39mmap_partitions(\n\u001b[1;32m     16\u001b[0m     add_tortuosity,\n\u001b[0;32m---> 17\u001b[0m     meta\u001b[38;5;241m=\u001b[39m\u001b[43mroads\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_meta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign\u001b[49m\u001b[43m(\u001b[49m\u001b[43meuclid_dist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtortuosity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# 3️⃣ Spatial join and aggregate into your grid\u001b[39;00m\n\u001b[1;32m     21\u001b[0m roads_grid \u001b[38;5;241m=\u001b[39m dgpd\u001b[38;5;241m.\u001b[39msjoin(roads, grid, predicate\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwithin\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pandas/core/frame.py:5239\u001b[0m, in \u001b[0;36mDataFrame.assign\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   5236\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   5238\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m-> 5239\u001b[0m     data[k] \u001b[38;5;241m=\u001b[39m \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_if_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pandas/core/common.py:384\u001b[0m, in \u001b[0;36mapply_if_callable\u001b[0;34m(maybe_callable, obj, **kwargs)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03mEvaluate possibly callable input using obj and kwargs if it is callable,\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03motherwise return as it is.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m**kwargs\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(maybe_callable):\n\u001b[0;32m--> 384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmaybe_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_callable\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'GeoDataFrame'"
     ]
    }
   ],
   "source": [
    "# Compute a small in‑memory map from intersection ID → geometry\n",
    "ints = intersections.compute()[['osmid','geometry']].set_index('osmid')['geometry']\n",
    "geom_map = ints.to_dict()\n",
    "\n",
    "# Partition‑wise tortuosity calculation\n",
    "def add_tortuosity(df):\n",
    "    # Safely compute straight‑line distance; avoid divide-by-zero\n",
    "    df['euclid_dist'] = df.apply(\n",
    "        lambda r: geom_map.get(r.u).distance(geom_map.get(r.v)) if (r.u in geom_map and r.v in geom_map) else None,\n",
    "        axis=1\n",
    "    )\n",
    "    df['tortuosity'] = df['length'] / df['euclid_dist']\n",
    "    return df\n",
    "\n",
    "roads = roads.map_partitions(\n",
    "    add_tortuosity,\n",
    "    meta=roads._meta.assign(euclid_dist=float, tortuosity=float)\n",
    ")\n",
    "\n",
    "# 3️⃣ Spatial join and aggregate into your grid\n",
    "roads_grid = dgpd.sjoin(roads, grid, predicate='within')\n",
    "mean_tortuosity = roads_grid.groupby('index_right')['tortuosity'].mean()\n",
    "grid['m10'] = grid.index.map(mean_tortuosity).fillna(0).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total grid cells: 462160\n"
     ]
    }
   ],
   "source": [
    "# Sum the total number of grid cells\n",
    "total_grid_cells = sum([grid_cells for grid_cells, path in calculated_grids])\n",
    "print(f'Total grid cells: {total_grid_cells}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subdivisions2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
