{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e5098a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_NAME = 'sara'\n",
    "\n",
    "AWS_PROFILE = 'cities'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca0cb8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09c11f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = \"s3://wri-cities-sandbox/identifyingLandSubdivisions/data\"\n",
    "INPUT_PATH = f'{MAIN_PATH}/input'\n",
    "CITY_INFO_PATH = f'{INPUT_PATH}/city_info'\n",
    "EXTENTS_PATH = f'{CITY_INFO_PATH}/extents'\n",
    "BUILDINGS_PATH = f'{INPUT_PATH}/buildings'\n",
    "ROADS_PATH = f'{INPUT_PATH}/roads'\n",
    "INTERSECTIONS_PATH = f'{INPUT_PATH}/intersections'\n",
    "GRIDS_PATH = f'{INPUT_PATH}/city_info/grids'\n",
    "SEARCH_BUFFER_PATH = f'{INPUT_PATH}/city_info/search_buffers'\n",
    "OUTPUT_PATH = f'{MAIN_PATH}/output'\n",
    "OUTPUT_PATH_CSV = f'{OUTPUT_PATH}/csv'\n",
    "OUTPUT_PATH_RASTER = f'{OUTPUT_PATH}/raster'\n",
    "OUTPUT_PATH_PNG = f'{OUTPUT_PATH}/png'\n",
    "OUTPUT_PATH_RAW = f'{OUTPUT_PATH}/raw_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54360109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'GYV6NG0EXYH6D3AV',\n",
       "  'HostId': '0Hlj7t2cGGRNUGhN8PcsGW7kVaqoSrDNudc0q/t1IetxF7erK8JPO8LJ/hjV8IFcsVCLDDHEfCw=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': '0Hlj7t2cGGRNUGhN8PcsGW7kVaqoSrDNudc0q/t1IetxF7erK8JPO8LJ/hjV8IFcsVCLDDHEfCw=',\n",
       "   'x-amz-request-id': 'GYV6NG0EXYH6D3AV',\n",
       "   'date': 'Sat, 17 Jan 2026 21:43:10 GMT',\n",
       "   'content-type': 'application/xml',\n",
       "   'transfer-encoding': 'chunked',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'Buckets': [{'Name': 'aft-sandbox-540362055257',\n",
       "   'CreationDate': datetime.datetime(2022, 9, 13, 15, 12, 20, tzinfo=tzutc()),\n",
       "   'BucketArn': 'arn:aws:s3:::aft-sandbox-540362055257'},\n",
       "  {'Name': 'amplify-citiesindicatorsapi-dev-10508-deployment',\n",
       "   'CreationDate': datetime.datetime(2023, 8, 30, 5, 5, 13, tzinfo=tzutc()),\n",
       "   'BucketArn': 'arn:aws:s3:::amplify-citiesindicatorsapi-dev-10508-deployment'},\n",
       "  {'Name': 'cities-heat',\n",
       "   'CreationDate': datetime.datetime(2023, 6, 1, 13, 22, 1, tzinfo=tzutc()),\n",
       "   'BucketArn': 'arn:aws:s3:::cities-heat'},\n",
       "  {'Name': 'cities-test-sandbox',\n",
       "   'CreationDate': datetime.datetime(2025, 4, 18, 19, 10, 49, tzinfo=tzutc()),\n",
       "   'BucketArn': 'arn:aws:s3:::cities-test-sandbox'},\n",
       "  {'Name': 'do-not-delete-ssm-diagnosis-540362055257-us-east-1-hac7u',\n",
       "   'CreationDate': datetime.datetime(2025, 7, 7, 9, 3, 18, tzinfo=tzutc()),\n",
       "   'BucketArn': 'arn:aws:s3:::do-not-delete-ssm-diagnosis-540362055257-us-east-1-hac7u'},\n",
       "  {'Name': 'era5-brazil',\n",
       "   'CreationDate': datetime.datetime(2025, 2, 15, 19, 51, 14, tzinfo=tzutc()),\n",
       "   'BucketArn': 'arn:aws:s3:::era5-brazil'},\n",
       "  {'Name': 'wri-cities-athena-us-east-1',\n",
       "   'CreationDate': datetime.datetime(2025, 7, 17, 13, 37, 24, tzinfo=tzutc()),\n",
       "   'BucketArn': 'arn:aws:s3:::wri-cities-athena-us-east-1'},\n",
       "  {'Name': 'wri-cities-athena-us-west-2',\n",
       "   'CreationDate': datetime.datetime(2024, 1, 12, 18, 45, 11, tzinfo=tzutc()),\n",
       "   'BucketArn': 'arn:aws:s3:::wri-cities-athena-us-west-2'},\n",
       "  {'Name': 'wri-cities-aws',\n",
       "   'CreationDate': datetime.datetime(2025, 7, 15, 18, 56, 37, tzinfo=tzutc()),\n",
       "   'BucketArn': 'arn:aws:s3:::wri-cities-aws'},\n",
       "  {'Name': 'wri-cities-climate-hazards',\n",
       "   'CreationDate': datetime.datetime(2024, 1, 3, 16, 57, 31, tzinfo=tzutc()),\n",
       "   'BucketArn': 'arn:aws:s3:::wri-cities-climate-hazards'},\n",
       "  {'Name': 'wri-cities-data-api',\n",
       "   'CreationDate': datetime.datetime(2024, 7, 16, 8, 53, 31, tzinfo=tzutc()),\n",
       "   'BucketArn': 'arn:aws:s3:::wri-cities-data-api'},\n",
       "  {'Name': 'wri-cities-heat',\n",
       "   'CreationDate': datetime.datetime(2024, 3, 25, 15, 46, 55, tzinfo=tzutc()),\n",
       "   'BucketArn': 'arn:aws:s3:::wri-cities-heat'},\n",
       "  {'Name': 'wri-cities-indicators',\n",
       "   'CreationDate': datetime.datetime(2024, 5, 13, 15, 50, 58, tzinfo=tzutc()),\n",
       "   'BucketArn': 'arn:aws:s3:::wri-cities-indicators'},\n",
       "  {'Name': 'wri-cities-sandbox',\n",
       "   'CreationDate': datetime.datetime(2024, 7, 27, 0, 51, 38, tzinfo=tzutc()),\n",
       "   'BucketArn': 'arn:aws:s3:::wri-cities-sandbox'},\n",
       "  {'Name': 'wri-cities-tcm',\n",
       "   'CreationDate': datetime.datetime(2025, 8, 29, 19, 35, 51, tzinfo=tzutc()),\n",
       "   'BucketArn': 'arn:aws:s3:::wri-cities-tcm'}],\n",
       " 'Owner': {'ID': 'df12253943982d72f60594f06c2cacf9a1ee3a9e738c1649c9fb96e5127f1a5c'}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check s3 connection using AWS_PROFILE=CitiesUserPermissionSet profile \n",
    "import boto3\n",
    "\n",
    "session = boto3.Session(profile_name=AWS_PROFILE)\n",
    "s3 = session.client('s3')\n",
    "\n",
    "# export CitiesUserPermissionSet profile to use in the next cells\n",
    "import os\n",
    "os.environ['AWS_PROFILE'] = AWS_PROFILE\n",
    "\n",
    "\n",
    "s3.list_buckets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f354422c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-17 16:43:09,669][INFO    ][coiled] Fetching latest package priorities...\n",
      "[2026-01-17 16:43:09,670][INFO    ][coiled.package_sync] Resolving your local subdivisions2 Python environment...\n",
      "[2026-01-17 16:43:10,096][INFO    ][coiled.package_sync] Scanning 446 conda packages...\n",
      "[2026-01-17 16:43:10,101][INFO    ][coiled.package_sync] Scanning 261 python packages...\n",
      "[2026-01-17 16:43:10,583][INFO    ][coiled] Running pip check...\n",
      "[2026-01-17 16:43:11,051][INFO    ][coiled] Validating environment...\n",
      "[2026-01-17 16:43:11,686][INFO    ][coiled] Creating wheel for ~/Documents/Identifying Land Subdivisions/identifyingLandSubdivisions...\n",
      "[2026-01-17 16:43:11,691][INFO    ][coiled] Creating wheel for /opt/spark-2.2.0/python...\n",
      "[2026-01-17 16:43:11,697][WARNING ][coiled.package_sync] Package - debugpy, debugpy~=1.8.16 has no install candidate for Python 3.12 linux-aarch64 on conda-forge\n",
      "[2026-01-17 16:43:11,697][INFO    ][coiled.package_sync] Package - aiobotocore, Pip check had the following issues that need resolving: \n",
      "aiobotocore 2.24.1 has requirement botocore<1.39.12,>=1.39.9, but you have botocore 1.40.16.\n",
      "[2026-01-17 16:43:11,697][INFO    ][coiled] Uploading coiled_local_identifyingLandSubdivisions...\n",
      "[2026-01-17 16:43:12,681][INFO    ][coiled] Uploading coiled_local_python...\n",
      "[2026-01-17 16:43:13,697][INFO    ][coiled] Requesting package sync build...\n",
      "[2026-01-17 16:43:19,565][INFO    ][coiled] Creating Cluster (name: ils-sara, https://cloud.coiled.io/clusters/1384504 ). This usually takes 1-2 minutes...\n",
      "/opt/anaconda3/envs/subdivisions2/lib/python3.12/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started a new Dask client on Coiled. Dashboard is available at https://cluster-rqtaa.dask.host/-c1C3tvp9cvQvzcz/status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/subdivisions2/lib/python3.12/site-packages/distributed/client.py:1590: VersionMismatchWarning: Mismatched versions found\n",
      "\n",
      "+---------+--------+-----------+---------+\n",
      "| Package | Client | Scheduler | Workers |\n",
      "+---------+--------+-----------+---------+\n",
      "| lz4     | 4.4.4  | 4.4.5     | 4.4.5   |\n",
      "+---------+--------+-----------+---------+\n",
      "  warnings.warn(version_module.VersionMismatchWarning(msg[0][\"warning\"]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-17 17:08:30,022 - distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client\n",
      "/opt/anaconda3/envs/subdivisions2/lib/python3.12/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import coiled\n",
    "\n",
    "cluster = coiled.Cluster(\n",
    "    workspace=\"wri-cities-data\",\n",
    "    name=f'ils-{YOUR_NAME}',\n",
    "    region=\"us-west-2\",\n",
    "    arm=True,\n",
    "    worker_vm_types=\"r8g.xlarge\",\n",
    "    spot_policy=\"spot\",\n",
    "    n_workers=2,\n",
    "    package_sync_ignore=[\"pyspark\", \"pypandoc\"],\n",
    "    worker_options={\"nthreads\": 1}\n",
    ")\n",
    "client = cluster.get_client()\n",
    "\n",
    "print(f\"Started a new Dask client on Coiled. Dashboard is available at {client.dashboard_link}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96fe15ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1239\n",
      "1238\n"
     ]
    }
   ],
   "source": [
    "import s3fs\n",
    "import fsspec\n",
    "import traceback\n",
    "\n",
    "import os\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import socket\n",
    "import traceback\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import pandas as pd\n",
    "from cloudpathlib import S3Path\n",
    "from gather_data_cities import *\n",
    "\n",
    "fs = s3fs.S3FileSystem(anon=False)\n",
    "search_buffer_files = fs.ls(SEARCH_BUFFER_PATH)\n",
    "\n",
    "cities = [x.split('/')[-1] for x in search_buffer_files]\n",
    "print(len(cities))\n",
    "cities =cities[1:]\n",
    "print(len(cities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5760044",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nn/3mdkp6sx1n3d955f3wqgdbb00000gn/T/ipykernel_83379/1327798151.py:19: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  RUN_ID = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import time\n",
    "import json\n",
    "import traceback\n",
    "import contextlib\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "from cloudpathlib import S3Path\n",
    "from dask.distributed import Semaphore\n",
    "\n",
    "# You already have these constants in gather_data_cities.py, but import them or redefine:\n",
    "# MAIN_PATH, INPUT_PATH, OUTPUT_PATH, OUTPUT_PATH_CSV, SEARCH_BUFFER_PATH, ROADS_PATH, INTERSECTIONS_PATH, NATURAL_FEATURES_PATH, BUILDINGS_PATH\n",
    "\n",
    "fs = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "RUN_ID = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "LOGS_S3_DIR = f\"{OUTPUT_PATH}/logs/gather_data/{RUN_ID}\"   # e.g. s3://.../output/logs/gather_data/20260115T....\n",
    "SUMMARY_S3_PATH = f\"{LOGS_S3_DIR}/summary.csv\"\n",
    "\n",
    "# ---- IMPORTANT: throttle Overpass concurrency ----\n",
    "# Overpass + AWS + 10 workers will often explode without this.\n",
    "# Try max_leases=2 first; if still unstable, use 1.\n",
    "overpass_sem = Semaphore(max_leases=2, name=\"overpass\")\n",
    "\n",
    "\n",
    "def _s3_exists(path: str) -> bool:\n",
    "    try:\n",
    "        return fs.exists(path)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _expected_outputs(city_clean: str) -> dict:\n",
    "    \"\"\"Return expected output paths + existence flags.\"\"\"\n",
    "    roads_path = f\"{ROADS_PATH}/{city_clean}/{city_clean}_OSM_roads.geoparquet\"\n",
    "    inter_path = f\"{INTERSECTIONS_PATH}/{city_clean}/{city_clean}_OSM_intersections.geoparquet\"\n",
    "    nat_path   = f\"{NATURAL_FEATURES_PATH}/{city_clean}/{city_clean}_OSM_natural_features_and_railroads.geoparquet\"\n",
    "\n",
    "    # Overture output path depends on your CLI output. In your code it's:\n",
    "    # os.path.join(BUILDINGS_PATH, city_clean) and filename Overture_building_<city>.geoparquet\n",
    "    bld_path   = f\"{BUILDINGS_PATH}/{city_clean}/Overture_building_{city_clean}.geoparquet\"\n",
    "\n",
    "    return {\n",
    "        \"roads_path\": roads_path,\n",
    "        \"intersections_path\": inter_path,\n",
    "        \"natural_path\": nat_path,\n",
    "        \"buildings_path\": bld_path,\n",
    "        \"exists_roads\": _s3_exists(roads_path),\n",
    "        \"exists_intersections\": _s3_exists(inter_path),\n",
    "        \"exists_natural\": _s3_exists(nat_path),\n",
    "        \"exists_buildings\": _s3_exists(bld_path),\n",
    "    }\n",
    "\n",
    "def gather_data_city_logged(city_name: str) -> dict:\n",
    "    from gather_data_cities import gather_data_city\n",
    "\n",
    "    city_clean = city_name.replace(\" \", \"_\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    lines = []\n",
    "    def log(msg: str):\n",
    "        lines.append(f\"[{city_clean}] {msg}\")\n",
    "\n",
    "    result = {\"city\": city_clean, \"status\": None, \"error\": \"\", \"elapsed_s\": None, \"log_s3\": \"\"}\n",
    "\n",
    "    try:\n",
    "        log(\"START\")\n",
    "        res = gather_data_city(city_clean)\n",
    "        log(f\"gather_data_city returned type={res.get('type')} status={res.get('status')}\")\n",
    "        result[\"gather_return_type\"] = res.get(\"type\", \"\")\n",
    "        result[\"gather_return_status\"] = res.get(\"status\", \"\")\n",
    "\n",
    "        outs = _expected_outputs(city_clean)\n",
    "        result.update(outs)\n",
    "\n",
    "        result[\"status\"] = \"OK\" if result[\"exists_roads\"] and result[\"exists_intersections\"] else \"PARTIAL\"\n",
    "\n",
    "    except Exception as e:\n",
    "        result[\"status\"] = \"FAIL\"\n",
    "        result[\"error\"] = \"\".join(traceback.format_exception(None, e, e.__traceback__))\n",
    "        log(f\"EXCEPTION: {e}\")\n",
    "\n",
    "        try:\n",
    "            result.update(_expected_outputs(city_clean))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    finally:\n",
    "        result[\"elapsed_s\"] = round(time.time() - t0, 2)\n",
    "        log(f\"END elapsed_s={result['elapsed_s']} status={result['status']}\")\n",
    "\n",
    "        # Upload log text (only this city’s lines)\n",
    "        log_text = \"\\n\".join(lines) + \"\\n\"\n",
    "        local_log = f\"/tmp/{city_clean}_{RUN_ID}.log\"\n",
    "        with open(local_log, \"w\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "            f.write(log_text)\n",
    "\n",
    "        log_s3 = f\"{LOGS_S3_DIR}/city_logs/{city_clean}.log\"\n",
    "        S3Path(log_s3).parent.mkdir(parents=True, exist_ok=True)\n",
    "        S3Path(log_s3).upload_from(local_log)\n",
    "        result[\"log_s3\"] = log_s3\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "063182cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def utc_now_iso():\n",
    "    return datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "\n",
    "def make_run_id(prefix=\"ils\"):\n",
    "    return f\"{prefix}-{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}-{uuid.uuid4().hex[:8]}\"\n",
    "\n",
    "\n",
    "def ensure_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "\n",
    "def write_text(path: str, text: str):\n",
    "    ensure_dir(os.path.dirname(path))\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "\n",
    "def safe_s3_upload(local_path: str, s3_uri: str):\n",
    "    # cloudpathlib handles mkdir + upload nicely\n",
    "    s3p = S3Path(s3_uri)\n",
    "    s3p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    s3p.upload_from(local_path)\n",
    "    return s3_uri\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56d22b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, socket, traceback\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def utc_now():\n",
    "    return datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "def append_log(path: str, msg: str):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(msg)\n",
    "\n",
    "def run_city(city_name: str, run_id: str, log_dir: str, fs) -> dict:\n",
    "    city_clean = city_name.replace(\" \", \"_\").strip()\n",
    "    host = socket.gethostname()\n",
    "    log_path = os.path.join(log_dir, f\"{city_clean}.log\")\n",
    "\n",
    "    append_log(log_path, f\"[{utc_now()}] START run_id={run_id} city={city_clean} host={host}\\n\")\n",
    "\n",
    "    # ---- SKIP logic (fast) ----\n",
    "    done, exists_map = is_city_done(city_clean, fs)\n",
    "    append_log(log_path, f\"[{utc_now()}] CHECK done={done} exists={exists_map}\\n\")\n",
    "\n",
    "    if done:\n",
    "        append_log(log_path, f\"[{utc_now()}] SKIP already_done\\n\")\n",
    "        return {\n",
    "            \"run_id\": run_id,\n",
    "            \"city\": city_clean,\n",
    "            \"status\": \"SKIP\",\n",
    "            \"elapsed_s\": 0.0,\n",
    "            \"host\": host,\n",
    "            \"exists_roads\": exists_map[\"roads\"],\n",
    "            \"exists_intersections\": exists_map[\"intersections\"],\n",
    "            \"exists_natural\": exists_map[\"natural\"],\n",
    "            \"error\": \"\",\n",
    "        }\n",
    "\n",
    "    # ---- Run gather ----\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        result = gather_data_city(city_clean)  # your function returns dict\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "\n",
    "        # re-check outputs after run\n",
    "        done2, exists_map2 = is_city_done(city_clean, fs)\n",
    "\n",
    "        append_log(log_path, f\"[{utc_now()}] DONE_CALL gather_result={result}\\n\")\n",
    "        append_log(log_path, f\"[{utc_now()}] POSTCHECK done={done2} exists={exists_map2}\\n\")\n",
    "        append_log(log_path, f\"[{utc_now()}] OK elapsed_s={elapsed:.2f}\\n\")\n",
    "\n",
    "        status = \"OK\" if done2 else \"PARTIAL\"\n",
    "\n",
    "        return {\n",
    "            \"run_id\": run_id,\n",
    "            \"city\": city_clean,\n",
    "            \"status\": status,\n",
    "            \"elapsed_s\": round(elapsed, 3),\n",
    "            \"host\": host,\n",
    "            \"exists_roads\": exists_map2[\"roads\"],\n",
    "            \"exists_intersections\": exists_map2[\"intersections\"],\n",
    "            \"exists_natural\": exists_map2[\"natural\"],\n",
    "            \"error\": \"\" if status != \"PARTIAL\" else \"Some expected outputs missing after run\",\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - t0\n",
    "        tb = \"\".join(traceback.format_exception(type(e), e, e.__traceback__))\n",
    "        append_log(log_path, f\"[{utc_now()}] FAIL elapsed_s={elapsed:.2f}\\n{tb}\\n\")\n",
    "\n",
    "        return {\n",
    "            \"run_id\": run_id,\n",
    "            \"city\": city_clean,\n",
    "            \"status\": \"FAIL\",\n",
    "            \"elapsed_s\": round(elapsed, 3),\n",
    "            \"host\": host,\n",
    "            \"exists_roads\": False,\n",
    "            \"exists_intersections\": False,\n",
    "            \"exists_natural\": False,\n",
    "            \"error\": str(e)[:2000],\n",
    "        }\n",
    "\n",
    "def expected_outputs(city_clean: str):\n",
    "    return {\n",
    "        \"roads\": f\"{ROADS_PATH}/{city_clean}/{city_clean}_OSM_roads.geoparquet\",\n",
    "        \"intersections\": f\"{INTERSECTIONS_PATH}/{city_clean}/{city_clean}_OSM_intersections.geoparquet\",\n",
    "        \"natural\": f\"{NATURAL_FEATURES_PATH}/{city_clean}/{city_clean}_OSM_natural_features_and_railroads.geoparquet\",\n",
    "        # buildings: not reliably on S3 with current code\n",
    "    }\n",
    "\n",
    "def is_city_done(city_clean: str, fs) -> tuple[bool, dict]:\n",
    "    paths = expected_outputs(city_clean)\n",
    "    exists = {k: fs.exists(p) for k, p in paths.items()}\n",
    "    done = all(exists.values())\n",
    "    return done, exists\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e8e4df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_all = pd.read_csv('../data/city_lists/cities_ssa_latam.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b57ab1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1238"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca56532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getting_1000_cities_grids import *\n",
    "\n",
    "\n",
    "\n",
    "all_cities_names = [s3_safe_token(x) for x in cities_all['city_name']]\n",
    "all_countries_names = [s3_safe_token(x) for x in cities_all['country_name']]\n",
    "\n",
    "all_cities = [f\"{a}__{b}\" for a, b in zip(all_cities_names, all_countries_names)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "863ec620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "search_buffer_files = fs.ls(ROADS_PATH)\n",
    "\n",
    "existing_features_cities = [x.split('/')[-1] for x in search_buffer_files]\n",
    "\n",
    "missing_cities = set(all_cities).difference(set(existing_features_cities))\n",
    "print(len(missing_cities))\n",
    "#print(missing_cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9c8130e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bugama__Nigeria',\n",
       " 'Mexico_City__Mexico',\n",
       " 'Mubi__Nigeria',\n",
       " 'Ngo__Nigeria',\n",
       " 'San_Pedro_de_Macoris__Dominican_Republic'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8b3d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing_cities_before = pd.read_csv('missing_cities_before.csv')\n",
    "#set(missing_cities_before['0']).difference(set(missing_cities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "73325bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status\n",
      "PARTIAL    5\n",
      "Name: count, dtype: int64\n",
      "✅ Summary written to: s3://wri-cities-sandbox/identifyingLandSubdivisions/data/output/logs/gather_data/20260117T214509Z/summary.csv\n"
     ]
    }
   ],
   "source": [
    "import dask.bag as db\n",
    "import pandas as pd\n",
    "from cloudpathlib import S3Path\n",
    "\n",
    "bag = db.from_sequence(missing_cities, partition_size=1)\n",
    "records = bag.map(gather_data_city_logged).compute()\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "print(df[\"status\"].value_counts(dropna=False))\n",
    "\n",
    "local_summary = f\"/tmp/summary_{RUN_ID}.csv\"\n",
    "df.to_csv(local_summary, index=False, sep=';')\n",
    "\n",
    "S3Path(LOGS_S3_DIR).mkdir(parents=True, exist_ok=True)\n",
    "S3Path(SUMMARY_S3_PATH).upload_from(local_summary)\n",
    "\n",
    "print(\"✅ Summary written to:\", SUMMARY_S3_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c53355",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c22482",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import dask.bag as db\n",
    "\n",
    "\n",
    "#cities = [\"Accra\", \"Abidjan\", \"Bamako\", \"Bogota\", \"Belo_Horizonte\", \"Campinas\", \"Cape_Town\", \"Lagos\",  \"Nairobi\", \"Luanda\"] #, Maputo, \"Mogadishu\",\n",
    "\n",
    "bag = db.from_sequence(cities, partition_size=1)\n",
    "results = bag.map(gather_data_city).compute()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650cbea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from pre_processing import *\n",
    "cities = [\"Belo_Horizonte\", \"Campinas\", \"Cape_Town\", \"Lagos\",  \"Nairobi\", \"Luanda\"] #, Maputo, \"Mogadishu\", #\"Accra\", \"Abidjan\", \"Bamako\", \"Bogota\", \n",
    "for city_name in cities:\n",
    "    produce_blocks(city_name,YOUR_NAME).compute()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dc62d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_creds():\n",
    "    import os\n",
    "    return {\n",
    "        \"AWS_ACCESS_KEY_ID\": bool(os.getenv(\"AWS_ACCESS_KEY_ID\")),\n",
    "        \"AWS_SECRET_ACCESS_KEY\": bool(os.getenv(\"AWS_SECRET_ACCESS_KEY\")),\n",
    "        \"AWS_SESSION_TOKEN\": bool(os.getenv(\"AWS_SESSION_TOKEN\")),\n",
    "        \"AWS_DEFAULT_REGION\": os.getenv(\"AWS_DEFAULT_REGION\"),\n",
    "    }\n",
    "\n",
    "CLIENT.run(check_creds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subdivisions2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
