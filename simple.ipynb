{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_NAME = 'chris'\n",
    "\n",
    "AWS_PROFILE = 'CitiesUserPermissionSet'\n",
    "\n",
    "\n",
    "# List of cities to process\n",
    "cities = [\"Belo Horizonte\", \"Campinas\", \"Bogota\", \"Nairobi\", \"Bamako\", \n",
    "        \"Lagos\", \"Accra\", \"Abidjan\", \"Mogadishu\", \"Cape Town\", \n",
    "        \"Maputo\", \"Luanda\"]\n",
    "\n",
    "test_cities = [\"Belo Horizonte\"]\n",
    "#cities = test_cities\n",
    "\n",
    "cities = [city.replace(' ', '_') for city in cities]\n",
    "\n",
    "number_of_cities = len(cities)\n",
    "\n",
    "print(f'City count: {number_of_cities}')\n",
    "\n",
    "grid_size = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = \"s3://wri-cities-sandbox/identifyingLandSubdivisions/data\"\n",
    "INPUT_PATH = f'{MAIN_PATH}/input'\n",
    "CITY_INFO_PATH = f'{INPUT_PATH}/city_info'\n",
    "EXTENTS_PATH = f'{CITY_INFO_PATH}/extents'\n",
    "BUILDINGS_PATH = f'{INPUT_PATH}/buildings'\n",
    "ROADS_PATH = f'{INPUT_PATH}/roads'\n",
    "INTERSECTIONS_PATH = f'{INPUT_PATH}/intersections'\n",
    "GRIDS_PATH = f'{INPUT_PATH}/city_info/grids'\n",
    "OUTPUT_PATH = f'{MAIN_PATH}/output'\n",
    "OUTPUT_PATH_CSV = f'{OUTPUT_PATH}/csv'\n",
    "OUTPUT_PATH_RASTER = f'{OUTPUT_PATH}/raster'\n",
    "OUTPUT_PATH_PNG = f'{OUTPUT_PATH}/png'\n",
    "OUTPUT_PATH_RAW = f'{OUTPUT_PATH}/raw_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check s3 connection using AWS_PROFILE=CitiesUserPermissionSet profile \n",
    "import boto3\n",
    "\n",
    "session = boto3.Session(profile_name=AWS_PROFILE)\n",
    "s3 = session.client('s3')\n",
    "\n",
    "# export CitiesUserPermissionSet profile to use in the next cells\n",
    "import os\n",
    "os.environ['AWS_PROFILE'] = AWS_PROFILE\n",
    "\n",
    "\n",
    "s3.list_buckets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coiled\n",
    "\n",
    "cluster = coiled.Cluster(\n",
    "    workspace=\"wri-cities-data\",\n",
    "    name=f'ils-{YOUR_NAME}',\n",
    "    region=\"us-west-2\",\n",
    "    arm=True,\n",
    "    worker_vm_types=\"r8g.xlarge\",\n",
    "    spot_policy=\"spot\",\n",
    "    n_workers=4,\n",
    ")\n",
    "client = cluster.get_client()\n",
    "\n",
    "print(f\"Started a new Dask client on Coiled. Dashboard is available at {client.dashboard_link}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_geopandas as dgpd\n",
    "from dask import delayed, compute, visualize\n",
    "from dask.diagnostics import ProgressBar\n",
    "%autoreload\n",
    "from citywide_calculation import get_utm_crs\n",
    "\n",
    "@delayed\n",
    "def get_epsg(city_name):\n",
    "    urban_extent = f'{EXTENTS_PATH}/{city_name}/{city_name}_urban_extent.geoparquet'\n",
    "    extent = dgpd.read_parquet(urban_extent)\n",
    "    geometry = extent.geometry[0].compute()\n",
    "    epsg = get_utm_crs(geometry)\n",
    "    print(f'{city_name} EPSG: {epsg}')\n",
    "    return epsg\n",
    "\n",
    "@delayed\n",
    "def load_dataset(path, epsg=None):\n",
    "    \"\"\"Load a single parquet dataset\"\"\"\n",
    "    dataset = dgpd.read_parquet(path, npartitions=2)\n",
    "    if epsg:\n",
    "        dataset = dataset.to_crs(epsg=epsg)\n",
    "    return dataset\n",
    "\n",
    "@delayed\n",
    "def row_count(df):\n",
    "    \"\"\"Count the rows in a dataframe\"\"\"\n",
    "    row_count = df.map_partitions(len).compute().sum()\n",
    "\n",
    "    return row_count\n",
    "\n",
    "\n",
    " \n",
    "%autoreload\n",
    "from metrics_calculation import metric_4_share_4way_intersections\n",
    "\n",
    "@delayed\n",
    "def metrics(city_name, grid, buildings, intersections):\n",
    "    # Building count\n",
    "    joined = dgpd.sjoin(buildings, grid, predicate='intersects')\n",
    "    del buildings\n",
    "    building_counts = joined.groupby('index_right').size()\n",
    "    del joined\n",
    "    grid['building_count'] = grid.index.map(building_counts).fillna(0).astype(int)\n",
    "    del building_counts\n",
    "    # # Metric 4\n",
    "    # joined = dgpd.sjoin(intersections, grid, predicate='within')\n",
    "    # # Group by the polygon index (automatically added as 'index_right')\n",
    "    # grouped = joined.groupby('index_right')\n",
    "    # del intersections\n",
    "    # m4 = grouped.apply(metric_4_share_4way_intersections, meta=('m4', 'float64'))\n",
    "    # del joined\n",
    "    # grid['m4'] = m4.index.map(m4).fillna(0)\n",
    "    # del m4\n",
    "    path = f'{OUTPUT_PATH}/city_info/grids/{city_name}/{city_name}_{str(grid_size)}m_grid_{YOUR_NAME}.geoparquet'\n",
    "    grid.to_parquet(path)\n",
    "    del grid\n",
    "    return path\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create delayed tasks for counting\n",
    "grid_calulations = []\n",
    "\n",
    "for city_name in cities:\n",
    "    # Define paths\n",
    "    paths = {\n",
    "        'grid': f'{GRIDS_PATH}/{city_name}/{city_name}_{str(grid_size)}m_grid.geoparquet',\n",
    "        'buildings': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}.geoparquet',\n",
    "        'roads': f'{ROADS_PATH}/{city_name}/{city_name}_OSM_roads.geoparquet',\n",
    "        'intersections': f'{INTERSECTIONS_PATH}/{city_name}/{city_name}_OSM_intersections.geoparquet'\n",
    "    }\n",
    "    # Get EPSG\n",
    "    epsg = get_epsg(city_name)\n",
    "    # Load buildings\n",
    "    buildings = load_dataset(paths['buildings'], epsg=epsg)\n",
    "    # Load roads\n",
    "    roads = load_dataset(paths['roads'], epsg=epsg)\n",
    "    # Load intersections\n",
    "    intersections = load_dataset(paths['intersections'], epsg=epsg)\n",
    "    # Load grid\n",
    "    grid = load_dataset(paths['grid'], epsg=epsg)\n",
    "    # Count buildings\n",
    "    grid_calc = metrics(city_name, grid, buildings, intersections)\n",
    "    # Write to geoportquet on S3\n",
    "    grid_calulations.append(grid_calc)\n",
    "\n",
    "#visualize(*grid_calulations)\n",
    "calculated_grids = compute(*grid_calulations)\n",
    "calculated_grids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subdivisions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
