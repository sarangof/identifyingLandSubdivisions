{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_NAME = 'sara'\n",
    "\n",
    "AWS_PROFILE = 'cities'\n",
    "\n",
    "'''\n",
    "# List of cities to process\n",
    "cities = [\"Belo Horizonte\", \"Campinas\"]#, \"Bogota\", \"Nairobi\", \"Bamako\", \n",
    "        #\"Lagos\", \"Accra\", \"Abidjan\", \"Mogadishu\", \"Cape Town\", \n",
    "        #\"Maputo\", \"Luanda\"]\n",
    "\n",
    "test_cities = [\"Belo Horizonte\"]\n",
    "#cities = test_cities\n",
    "\n",
    "cities = [city.replace(' ', '_') for city in cities]\n",
    "\n",
    "search_buffer_files = fs.ls(SEARCH_BUFFER_PATH)\n",
    "\n",
    "cities \n",
    "\n",
    "number_of_cities = len(cities)\n",
    "\n",
    "print(f'City count: {number_of_cities}')\n",
    "'''\n",
    "grid_size = 200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = \"s3://wri-cities-sandbox/identifyingLandSubdivisions/data\"\n",
    "INPUT_PATH = f'{MAIN_PATH}/input'\n",
    "CITY_INFO_PATH = f'{INPUT_PATH}/city_info'\n",
    "EXTENTS_PATH = f'{CITY_INFO_PATH}/extents'\n",
    "BUILDINGS_PATH = f'{INPUT_PATH}/buildings'\n",
    "BUILDINGS_DISTANCES_PATH = f'{INPUT_PATH}/buildings_with_distances'\n",
    "ROADS_PATH = f'{INPUT_PATH}/roads'\n",
    "INTERSECTIONS_PATH = f'{INPUT_PATH}/intersections'\n",
    "GRIDS_PATH = f'{INPUT_PATH}/city_info/grids'\n",
    "SEARCH_BUFFER_PATH = f'{INPUT_PATH}/city_info/search_buffers'\n",
    "BLOCKS_PATH = f'{INPUT_PATH}/blocks'\n",
    "OUTPUT_PATH = f'{MAIN_PATH}/output'\n",
    "OUTPUT_PATH_CSV = f'{OUTPUT_PATH}/csv'\n",
    "OUTPUT_PATH_RASTER = f'{OUTPUT_PATH}/raster'\n",
    "OUTPUT_PATH_PNG = f'{OUTPUT_PATH}/png'\n",
    "OUTPUT_PATH_RAW = f'{OUTPUT_PATH}/raw_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'BVNTN4SF3RHMH175',\n",
       "  'HostId': 'TbKdRgJsq1+glKjTz1Vf8l/lnKjmoRWgML9qh0bHsaxMxwc36LQDTKvB6uv/npSUk0D4JQVt8Ac=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'TbKdRgJsq1+glKjTz1Vf8l/lnKjmoRWgML9qh0bHsaxMxwc36LQDTKvB6uv/npSUk0D4JQVt8Ac=',\n",
       "   'x-amz-request-id': 'BVNTN4SF3RHMH175',\n",
       "   'date': 'Wed, 26 Mar 2025 23:00:58 GMT',\n",
       "   'content-type': 'application/xml',\n",
       "   'transfer-encoding': 'chunked',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'Buckets': [{'Name': 'aft-sandbox-540362055257',\n",
       "   'CreationDate': datetime.datetime(2022, 9, 13, 15, 12, 20, tzinfo=tzutc())},\n",
       "  {'Name': 'amplify-citiesindicatorsapi-dev-10508-deployment',\n",
       "   'CreationDate': datetime.datetime(2023, 8, 30, 5, 5, 13, tzinfo=tzutc())},\n",
       "  {'Name': 'cities-dev-sandbox',\n",
       "   'CreationDate': datetime.datetime(2025, 2, 7, 23, 18, 12, tzinfo=tzutc())},\n",
       "  {'Name': 'cities-heat',\n",
       "   'CreationDate': datetime.datetime(2023, 6, 1, 13, 22, 1, tzinfo=tzutc())},\n",
       "  {'Name': 'era5-brazil',\n",
       "   'CreationDate': datetime.datetime(2025, 2, 15, 19, 51, 14, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-athena-us-west-2',\n",
       "   'CreationDate': datetime.datetime(2024, 1, 12, 18, 45, 11, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-climate-hazards',\n",
       "   'CreationDate': datetime.datetime(2024, 1, 3, 16, 57, 31, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-data-api',\n",
       "   'CreationDate': datetime.datetime(2024, 7, 16, 8, 53, 31, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-heat',\n",
       "   'CreationDate': datetime.datetime(2024, 3, 25, 15, 46, 55, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-indicators',\n",
       "   'CreationDate': datetime.datetime(2024, 5, 13, 15, 50, 58, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-sandbox',\n",
       "   'CreationDate': datetime.datetime(2024, 7, 27, 0, 51, 38, tzinfo=tzutc())}],\n",
       " 'Owner': {'DisplayName': 'aws-cities',\n",
       "  'ID': 'df12253943982d72f60594f06c2cacf9a1ee3a9e738c1649c9fb96e5127f1a5c'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check s3 connection using AWS_PROFILE=CitiesUserPermissionSet profile \n",
    "import boto3\n",
    "\n",
    "session = boto3.Session(profile_name=AWS_PROFILE)\n",
    "s3 = session.client('s3')\n",
    "\n",
    "# export CitiesUserPermissionSet profile to use in the next cells\n",
    "import os\n",
    "os.environ['AWS_PROFILE'] = AWS_PROFILE\n",
    "\n",
    "\n",
    "s3.list_buckets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-26 18:01:00,521][INFO    ][coiled] Fetching latest package priorities...\n",
      "[2025-03-26 18:01:00,523][INFO    ][coiled.package_sync] Resolving your local subdivisions2 Python environment...\n",
      "[2025-03-26 18:01:01,396][INFO    ][coiled.package_sync] Scanning 444 conda packages...\n",
      "[2025-03-26 18:01:01,405][INFO    ][coiled.package_sync] Scanning 259 python packages...\n",
      "[2025-03-26 18:01:02,391][INFO    ][coiled] Running pip check...\n",
      "[2025-03-26 18:01:04,019][INFO    ][coiled] Validating environment...\n",
      "[2025-03-26 18:01:06,186][INFO    ][coiled] Creating wheel for ~/Documents/Identifying Land Subdivisions/identifyingLandSubdivisions...\n",
      "[2025-03-26 18:01:06,486][WARNING ][coiled.package_sync] Package - libopenvino-intel-cpu-plugin, libopenvino-intel-cpu-plugin~=2025.0.0 has no install candidate for Python 3.12 linux-aarch64 on conda-forge\n",
      "[2025-03-26 18:01:06,488][INFO    ][coiled] Uploading coiled_local_identifyingLandSubdivisions...\n",
      "[2025-03-26 18:01:07,438][INFO    ][coiled] Requesting package sync build...\n",
      "[2025-03-26 18:01:08,240][INFO    ][coiled] Creating Cluster (name: ils-sara, https://cloud.coiled.io/clusters/810792?account=wri-cities-data ). This usually takes 1-2 minutes...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started a new Dask client on Coiled. Dashboard is available at https://cluster-ntylm.dask.host/atafD5pdumjdrwos/status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in callback None()\n",
      "handle: <Handle cancelled>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 1367, in _do_ssl_handshake\n",
      "    self.socket.do_handshake()\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/ssl.py\", line 1319, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "ssl.SSLEOFError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1010)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 202, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 691, in _handle_events\n",
      "    self._handle_read()\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 1427, in _handle_read\n",
      "    self._do_ssl_handshake()\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 1376, in _do_ssl_handshake\n",
      "    return self.close(exc_info=err)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 606, in close\n",
      "    self._signal_closed()\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 636, in _signal_closed\n",
      "    self._ssl_connect_future.exception()\n",
      "asyncio.exceptions.CancelledError\n",
      "Exception in callback None()\n",
      "handle: <Handle cancelled>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 1367, in _do_ssl_handshake\n",
      "    self.socket.do_handshake()\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/ssl.py\", line 1319, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "ssl.SSLEOFError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1010)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 202, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 691, in _handle_events\n",
      "    self._handle_read()\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 1427, in _handle_read\n",
      "    self._do_ssl_handshake()\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 1376, in _do_ssl_handshake\n",
      "    return self.close(exc_info=err)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 606, in close\n",
      "    self._signal_closed()\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 636, in _signal_closed\n",
      "    self._ssl_connect_future.exception()\n",
      "asyncio.exceptions.CancelledError\n",
      "2025-03-26 18:55:58,055 - distributed.deploy.cluster - WARNING - Failed to sync cluster info multiple times - perhaps there's a connection issue? Error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 547, in connect\n",
      "    stream = await self.client.connect(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/tcpclient.py\", line 292, in connect\n",
      "    stream = await stream.start_tls(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 1367, in _do_ssl_handshake\n",
      "    self.socket.do_handshake()\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/ssl.py\", line 1319, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "ssl.SSLEOFError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1010)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/distributed/core.py\", line 1176, in send_recv_from_rpc\n",
      "    comm = await self.live_comm()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/distributed/core.py\", line 1135, in live_comm\n",
      "    comm = await connect(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/distributed/comm/core.py\", line 342, in connect\n",
      "    comm = await wait_for(\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/distributed/utils.py\", line 1910, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 567, in connect\n",
      "    raise FatalCommClosedError() from err\n",
      "distributed.comm.core.FatalCommClosedError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/distributed/deploy/cluster.py\", line 165, in _sync_cluster_info\n",
      "    await self.scheduler_comm.set_metadata(\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/distributed/core.py\", line 1185, in send_recv_from_rpc\n",
      "    raise type(e)(\n",
      "distributed.comm.core.FatalCommClosedError: Exception while trying to call remote method 'set_metadata' before comm was established.\n",
      "Exception in callback None()\n",
      "handle: <Handle cancelled>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 1367, in _do_ssl_handshake\n",
      "    self.socket.do_handshake()\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/ssl.py\", line 1319, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "ssl.SSLEOFError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1010)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 202, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 691, in _handle_events\n",
      "    self._handle_read()\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 1427, in _handle_read\n",
      "    self._do_ssl_handshake()\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 1376, in _do_ssl_handshake\n",
      "    return self.close(exc_info=err)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 606, in close\n",
      "    self._signal_closed()\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/tornado/iostream.py\", line 636, in _signal_closed\n",
      "    self._ssl_connect_future.exception()\n",
      "asyncio.exceptions.CancelledError\n",
      "2025-03-26 18:56:02,742 - distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client\n"
     ]
    }
   ],
   "source": [
    "import coiled\n",
    "\n",
    "cluster = coiled.Cluster(\n",
    "    workspace=\"wri-cities-data\",\n",
    "    name=f'ils-{YOUR_NAME}',\n",
    "    region=\"us-west-2\",\n",
    "    arm=True,\n",
    "    worker_vm_types=\"r8g.xlarge\",\n",
    "    spot_policy=\"spot\",\n",
    "    n_workers=8,\n",
    "    package_sync_ignore=[\"pyspark\", \"pypandoc\"]\n",
    ")\n",
    "client = cluster.get_client()\n",
    "\n",
    "print(f\"Started a new Dask client on Coiled. Dashboard is available at {client.dashboard_link}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import s3fs\n",
    "import fsspec\n",
    "import traceback\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "fs = s3fs.S3FileSystem(anon=False)\n",
    "search_buffer_files = fs.ls(SEARCH_BUFFER_PATH)\n",
    "\n",
    "cities = [x.split('/')[-1] for x in search_buffer_files]\n",
    "len(cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 1650.1406290531158\n"
     ]
    }
   ],
   "source": [
    "import dask_geopandas as dgpd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape, Point, LineString, Polygon, MultiPolygon, MultiLineString\n",
    "from shapely.geometry.base import BaseGeometry\n",
    "from shapely.strtree import STRtree\n",
    "from shapely.errors import ShapelyError\n",
    "import dask\n",
    "from dask import delayed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.wkb import loads as wkb_loads\n",
    "from dask import compute\n",
    "from citywide_calculation import get_utm_crs\n",
    "\n",
    "@delayed\n",
    "def get_epsg(city_name):\n",
    "    search_buffer = f'{SEARCH_BUFFER_PATH}/{city_name}/{city_name}_search_buffer.geoparquet'\n",
    "    extent = dgpd.read_parquet(search_buffer)\n",
    "    geometry = extent.geometry[0].compute()\n",
    "    epsg = get_utm_crs(geometry)\n",
    "    print(f'{city_name} EPSG: {epsg}')\n",
    "    return epsg\n",
    "\n",
    "def load_dataset(path, epsg=None):\n",
    "    dataset = dgpd.read_parquet(path, npartitions=4)\n",
    "    \n",
    "    # Only assign if the file has no CRS\n",
    "    if epsg:\n",
    "        if dataset.crs is None:\n",
    "            dataset = dataset.set_crs(\"EPSG:4326\")  # assume WGS84 if missing\n",
    "        dataset = dataset.to_crs(epsg)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "max_distance = 200\n",
    "default_distance = 500\n",
    "\n",
    "def compute_distance_partition(buildings_df, roads_geom_list, max_distance, default_distance):\n",
    "    tree = STRtree(roads_geom_list)\n",
    "\n",
    "    def distance_fn(bgeom):\n",
    "        try:\n",
    "            bgeom = shape(bgeom) if not isinstance(bgeom, BaseGeometry) else bgeom\n",
    "            nearby_indices = tree.query(bgeom.buffer(max_distance))\n",
    "            if nearby_indices is None or len(nearby_indices) == 0:\n",
    "                return default_distance\n",
    "            nearby_geoms = [roads_geom_list[i] for i in nearby_indices]\n",
    "            return min(bgeom.distance(road) for road in nearby_geoms)\n",
    "        except Exception:\n",
    "            return default_distance\n",
    "\n",
    "    buildings_df = buildings_df.copy()\n",
    "    buildings_df['geometry'] = buildings_df['geometry'].apply(shape)  # extra safe\n",
    "    buildings_df[\"distance_to_nearest_road\"] = buildings_df.geometry.apply(distance_fn)\n",
    "    return buildings_df\n",
    "\n",
    "\n",
    "@delayed\n",
    "def calculate_building_distances_to_roads(city_name):\n",
    "    paths = {\n",
    "    'grid': f'{GRIDS_PATH}/{city_name}/{city_name}_{grid_size}m_grid.geoparquet',\n",
    "    'buildings': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}.geoparquet',\n",
    "    'roads': f'{ROADS_PATH}/{city_name}/{city_name}_OSM_roads.geoparquet'\n",
    "    }\n",
    "    epsg = get_epsg(city_name).compute()  \n",
    "    # Load and prepare roads for spatial index\n",
    "    roads = load_dataset(paths['roads'], epsg=epsg).compute()\n",
    "    roads_geom_list = [geom for geom in roads.geometry]\n",
    "\n",
    "    # Load buildings lazily\n",
    "    buildings = load_dataset(paths['buildings'], epsg=epsg)\n",
    "\n",
    "    meta = buildings._meta.assign(distance_to_nearest_road='f8')\n",
    "\n",
    "    # Apply distance computation per partition\n",
    "    buildings_with_dist = buildings.map_partitions(\n",
    "        compute_distance_partition,\n",
    "        roads_geom_list,\n",
    "        max_distance,\n",
    "        default_distance,\n",
    "        meta=meta\n",
    "    )\n",
    "\n",
    "    # Write output\n",
    "    columns_to_keep = ['id', 'geometry','distance_to_nearest_road']\n",
    "    buildings_with_dist = buildings_with_dist[columns_to_keep].set_index('id')\n",
    "    out_path = paths['buildings'].replace(\".geoparquet\", \"_with_distances.geoparquet\")\n",
    "    buildings_with_dist.to_parquet(out_path)\n",
    "    return out_path\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "city_list = [\"Nairobi\",\"Belo_Horizonte\"]\n",
    "\n",
    "def run_all_cities(city_list):\n",
    "    delayed_jobs = [delayed(calculate_building_distances_to_roads)(city) for city in city_list]\n",
    "    results = compute(*delayed_jobs)\n",
    "    return results\n",
    "\n",
    "run_all_cities(city_list)\n",
    "\n",
    "print(\"Elapsed:\", time.time() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_name = 'Nairobi'\n",
    "nai_buildings = dgpd.read_parquet(f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}_with_distances.geoparquet', npartitions=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 88.6606342792511\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import t, sem, entropy\n",
    "\n",
    "def calculate_azimuth(buildings_partition, bin_width_degrees):\n",
    "        \n",
    "    buildings_partition.loc[:, 'azimuth'] = buildings_partition['geometry'].apply(lambda x: calculate_azimuth(buildings_partition(x))  % 90. )\n",
    "\n",
    "    # Extract azimuths (assuming they are in degrees)\n",
    "    azimuths = buildings_partition['azimuth'].values\n",
    "\n",
    "    # Define the number of bins (e.g., 0-360 degrees, based on bin width)\n",
    "    num_bins = int(90 / bin_width_degrees)\n",
    "\n",
    "    # Create a histogram of azimuths (observed distribution P)\n",
    "    histogram, bin_edges = np.histogram(azimuths, bins=num_bins, range=(0, 90))\n",
    "    \n",
    "    # Normalize the histogram to make it a probability distribution P\n",
    "    P = histogram / histogram.sum()\n",
    "\n",
    "    # Create a uniform distribution Q with the same number of bins\n",
    "    Q = np.ones(num_bins) / num_bins\n",
    "\n",
    "    # Calculate the KL divergence\n",
    "    kl_divergence = entropy(P, Q)\n",
    "\n",
    "    # Standardize the KL divergence by dividing by log(n)\n",
    "    max_kl_divergence = np.log(num_bins)\n",
    "    standardized_kl_divergence = kl_divergence / max_kl_divergence\n",
    "\n",
    "\n",
    "def building_distance_metrics(city_name):\n",
    "\n",
    "    grid_cell_count = 0\n",
    "    paths = {\n",
    "        'grid': f'{GRIDS_PATH}/{city_name}/{city_name}_{str(grid_size)}m_grid.geoparquet',\n",
    "        'buildings': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}.geoparquet',\n",
    "        'buildings_with_distances': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}_with_distances.geoparquet',\n",
    "        'roads': f'{ROADS_PATH}/{city_name}/{city_name}_OSM_roads.geoparquet',\n",
    "        'intersections': f'{INTERSECTIONS_PATH}/{city_name}/{city_name}_OSM_intersections.geoparquet'\n",
    "    }\n",
    "    # Get EPSG\n",
    "    epsg = get_epsg(city_name).compute()\n",
    "    # Load grid\n",
    "    grid = load_dataset(paths['grid'], epsg=epsg)#.compute()\n",
    "    if 'geom' in grid.columns:\n",
    "        grid = grid.drop(columns=['geom'])\n",
    "    \n",
    "    buildings = load_dataset(paths['buildings_with_distances'], epsg=epsg)#.compute()\n",
    "    buildings['distance_to_nearest_road'] = buildings['distance_to_nearest_road'].astype(float)\n",
    "    buildings['area'] = buildings.geometry.area\n",
    "    joined_buildings = dgpd.sjoin(buildings, grid, predicate='intersects')  \n",
    "    counts_buildings = joined_buildings.groupby('index_right').size()\n",
    "    grid['n_buildings'] = grid.index.map(counts_buildings).fillna(0).astype(int)\n",
    "    average_distance = joined_buildings.groupby('index_right')['distance_to_nearest_road'].mean()\n",
    "    grid['average_distance_nearest_building'] = grid.index.map(average_distance).fillna(0).astype(float)\n",
    " \n",
    "\n",
    "    buildings_closer_than_20m = buildings[buildings['distance_to_nearest_road'] <= 20]\n",
    "    joined_buildings_closer_than_20m = dgpd.sjoin(buildings_closer_than_20m, grid, predicate='intersects') \n",
    "    n_buildings_closer_than_20m = joined_buildings_closer_than_20m.groupby('index_right').size()\n",
    "    grid['n_buildings_closer_than_20m'] = grid.index.map(n_buildings_closer_than_20m).fillna(0).astype(float)\n",
    "\n",
    "    grid['m1'] = grid['n_buildings_closer_than_20m'] / grid['n_buildings']\n",
    "    grid['m2'] = grid['average_distance_nearest_building']\n",
    "\n",
    "    path = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{str(grid_size)}m_grid_{YOUR_NAME}_metrics_1_2.geoparquet'\n",
    "\n",
    "    if 'geom' in grid.columns:\n",
    "        grid = grid.drop(columns='geom')\n",
    "\n",
    "    grid.to_parquet(path)\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "city_list = [\"Nairobi\",\"Belo_Horizonte\"]\n",
    "\n",
    "def run_all_cities_metrics1_2(city_list):\n",
    "    delayed_jobs = [delayed(building_distance_metrics)(city) for city in city_list]\n",
    "    results = compute(*delayed_jobs)\n",
    "    return results\n",
    "\n",
    "run_all_cities_metrics1_2(city_list)\n",
    "\n",
    "print(\"Elapsed:\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nai_buildings.distance_to_nearest_road = nai_buildings.distance_to_nearest_road.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mt/3n9j2kc92kv4psztx687vtd80000gn/T/ipykernel_33008/1568468617.py:1: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  nai_buildings.to_file('nairobi_buildings_with_distance.shp')\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'distance_to_nearest_road' to 'distance_t'\n",
      "  ogr_write(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnai_buildings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnairobi_buildings_with_distance.shp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/geopandas/geodataframe.py:1536\u001b[0m, in \u001b[0;36mGeoDataFrame.to_file\u001b[0;34m(self, filename, driver, schema, index, **kwargs)\u001b[0m\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Write the ``GeoDataFrame`` to a file.\u001b[39;00m\n\u001b[1;32m   1442\u001b[0m \n\u001b[1;32m   1443\u001b[0m \u001b[38;5;124;03mBy default, an ESRI shapefile is written, but any OGR data source\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1532\u001b[0m \n\u001b[1;32m   1533\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1534\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgeopandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _to_file\n\u001b[0;32m-> 1536\u001b[0m \u001b[43m_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/geopandas/io/file.py:686\u001b[0m, in \u001b[0;36m_to_file\u001b[0;34m(df, filename, driver, schema, index, mode, crs, engine, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    683\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m should be one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    685\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyogrio\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 686\u001b[0m     \u001b[43m_to_file_pyogrio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiona\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    688\u001b[0m     _to_file_fiona(df, filename, driver, schema, crs, mode, metadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/geopandas/io/file.py:748\u001b[0m, in \u001b[0;36m_to_file_pyogrio\u001b[0;34m(df, filename, driver, schema, crs, mode, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique:\n\u001b[1;32m    746\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeoDataFrame cannot contain duplicated column names.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 748\u001b[0m \u001b[43mpyogrio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/geopandas.py:662\u001b[0m, in \u001b[0;36mwrite_dataframe\u001b[0;34m(df, path, layer, driver, encoding, geometry_type, promote_to_multi, nan_as_null, append, use_arrow, dataset_metadata, layer_metadata, metadata, dataset_options, layer_options, **kwargs)\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m geometry_column \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    660\u001b[0m     geometry \u001b[38;5;241m=\u001b[39m to_wkb(geometry\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m--> 662\u001b[0m \u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeometry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfield_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfield_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfield_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfield_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeometry_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeometry_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpromote_to_multi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpromote_to_multi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnan_as_null\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnan_as_null\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mappend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgdal_tz_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgdal_tz_offsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pyogrio/raw.py:723\u001b[0m, in \u001b[0;36mwrite\u001b[0;34m(path, geometry, field_data, fields, field_mask, layer, driver, geometry_type, crs, encoding, promote_to_multi, nan_as_null, append, dataset_metadata, layer_metadata, metadata, dataset_options, layer_options, gdal_tz_offsets, **kwargs)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;66;03m# preprocess kwargs and split in dataset and layer creation options\u001b[39;00m\n\u001b[1;32m    719\u001b[0m dataset_kwargs, layer_kwargs \u001b[38;5;241m=\u001b[39m _preprocess_options_kwargs(\n\u001b[1;32m    720\u001b[0m     driver, dataset_options, layer_options, kwargs\n\u001b[1;32m    721\u001b[0m )\n\u001b[0;32m--> 723\u001b[0m \u001b[43mogr_write\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeometry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeometry_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeometry_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfield_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfield_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfield_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfield_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpromote_to_multi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpromote_to_multi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnan_as_null\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnan_as_null\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mappend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgdal_tz_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgdal_tz_offsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32mpyogrio/_io.pyx:2504\u001b[0m, in \u001b[0;36mpyogrio._io.ogr_write\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nai_buildings.to_file('nairobi_buildings_with_distance.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subdivisions2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
