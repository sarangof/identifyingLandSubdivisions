{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_NAME = 'sara'\n",
    "\n",
    "AWS_PROFILE = 'cities'\n",
    "\n",
    "'''\n",
    "# List of cities to process\n",
    "cities = [\"Belo Horizonte\", \"Campinas\"]#, \"Bogota\", \"Nairobi\", \"Bamako\", \n",
    "        #\"Lagos\", \"Accra\", \"Abidjan\", \"Mogadishu\", \"Cape Town\", \n",
    "        #\"Maputo\", \"Luanda\"]\n",
    "\n",
    "test_cities = [\"Belo Horizonte\"]\n",
    "#cities = test_cities\n",
    "\n",
    "cities = [city.replace(' ', '_') for city in cities]\n",
    "\n",
    "search_buffer_files = fs.ls(SEARCH_BUFFER_PATH)\n",
    "\n",
    "cities \n",
    "\n",
    "number_of_cities = len(cities)\n",
    "\n",
    "print(f'City count: {number_of_cities}')\n",
    "'''\n",
    "grid_size = 200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = \"s3://wri-cities-sandbox/identifyingLandSubdivisions/data\"\n",
    "INPUT_PATH = f'{MAIN_PATH}/input'\n",
    "CITY_INFO_PATH = f'{INPUT_PATH}/city_info'\n",
    "EXTENTS_PATH = f'{CITY_INFO_PATH}/extents'\n",
    "BUILDINGS_PATH = f'{INPUT_PATH}/buildings'\n",
    "BUILDINGS_DISTANCES_PATH = f'{INPUT_PATH}/buildings_with_distances'\n",
    "ROADS_PATH = f'{INPUT_PATH}/roads'\n",
    "INTERSECTIONS_PATH = f'{INPUT_PATH}/intersections'\n",
    "GRIDS_PATH = f'{INPUT_PATH}/city_info/grids'\n",
    "SEARCH_BUFFER_PATH = f'{INPUT_PATH}/city_info/search_buffers'\n",
    "BLOCKS_PATH = f'{INPUT_PATH}/blocks'\n",
    "OUTPUT_PATH = f'{MAIN_PATH}/output'\n",
    "OUTPUT_PATH_CSV = f'{OUTPUT_PATH}/csv'\n",
    "OUTPUT_PATH_RASTER = f'{OUTPUT_PATH}/raster'\n",
    "OUTPUT_PATH_PNG = f'{OUTPUT_PATH}/png'\n",
    "OUTPUT_PATH_RAW = f'{OUTPUT_PATH}/raw_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'SYG4E8TB8ZBYVQR8',\n",
       "  'HostId': '41lbiGSGloX3AnNa3myfH9JKfHOqVhyllMqBLUBw5yp+YniuilE8oybWNA5epcbI5el+ie937W8=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': '41lbiGSGloX3AnNa3myfH9JKfHOqVhyllMqBLUBw5yp+YniuilE8oybWNA5epcbI5el+ie937W8=',\n",
       "   'x-amz-request-id': 'SYG4E8TB8ZBYVQR8',\n",
       "   'date': 'Thu, 27 Mar 2025 16:31:04 GMT',\n",
       "   'content-type': 'application/xml',\n",
       "   'transfer-encoding': 'chunked',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'Buckets': [{'Name': 'aft-sandbox-540362055257',\n",
       "   'CreationDate': datetime.datetime(2022, 9, 13, 15, 12, 20, tzinfo=tzutc())},\n",
       "  {'Name': 'amplify-citiesindicatorsapi-dev-10508-deployment',\n",
       "   'CreationDate': datetime.datetime(2023, 8, 30, 5, 5, 13, tzinfo=tzutc())},\n",
       "  {'Name': 'cities-dev-sandbox',\n",
       "   'CreationDate': datetime.datetime(2025, 2, 7, 23, 18, 12, tzinfo=tzutc())},\n",
       "  {'Name': 'cities-heat',\n",
       "   'CreationDate': datetime.datetime(2023, 6, 1, 13, 22, 1, tzinfo=tzutc())},\n",
       "  {'Name': 'era5-brazil',\n",
       "   'CreationDate': datetime.datetime(2025, 2, 15, 19, 51, 14, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-athena-us-west-2',\n",
       "   'CreationDate': datetime.datetime(2024, 1, 12, 18, 45, 11, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-climate-hazards',\n",
       "   'CreationDate': datetime.datetime(2024, 1, 3, 16, 57, 31, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-data-api',\n",
       "   'CreationDate': datetime.datetime(2024, 7, 16, 8, 53, 31, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-heat',\n",
       "   'CreationDate': datetime.datetime(2024, 3, 25, 15, 46, 55, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-indicators',\n",
       "   'CreationDate': datetime.datetime(2024, 5, 13, 15, 50, 58, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-sandbox',\n",
       "   'CreationDate': datetime.datetime(2024, 7, 27, 0, 51, 38, tzinfo=tzutc())}],\n",
       " 'Owner': {'DisplayName': 'aws-cities',\n",
       "  'ID': 'df12253943982d72f60594f06c2cacf9a1ee3a9e738c1649c9fb96e5127f1a5c'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check s3 connection using AWS_PROFILE=CitiesUserPermissionSet profile \n",
    "import boto3\n",
    "\n",
    "session = boto3.Session(profile_name=AWS_PROFILE)\n",
    "s3 = session.client('s3')\n",
    "\n",
    "# export CitiesUserPermissionSet profile to use in the next cells\n",
    "import os\n",
    "os.environ['AWS_PROFILE'] = AWS_PROFILE\n",
    "\n",
    "\n",
    "s3.list_buckets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-27 11:31:07,275][INFO    ][coiled] Fetching latest package priorities...\n",
      "[2025-03-27 11:31:07,276][INFO    ][coiled.package_sync] Resolving your local subdivisions2 Python environment...\n",
      "[2025-03-27 11:31:08,201][INFO    ][coiled.package_sync] Scanning 444 conda packages...\n",
      "[2025-03-27 11:31:08,209][INFO    ][coiled.package_sync] Scanning 259 python packages...\n",
      "[2025-03-27 11:31:09,207][INFO    ][coiled] Running pip check...\n",
      "[2025-03-27 11:31:10,681][INFO    ][coiled] Validating environment...\n",
      "[2025-03-27 11:31:12,939][INFO    ][coiled] Creating wheel for ~/Documents/Identifying Land Subdivisions/identifyingLandSubdivisions...\n",
      "[2025-03-27 11:31:13,191][WARNING ][coiled.package_sync] Package - libopenvino-intel-cpu-plugin, libopenvino-intel-cpu-plugin~=2025.0.0 has no install candidate for Python 3.12 linux-aarch64 on conda-forge\n",
      "[2025-03-27 11:31:13,192][INFO    ][coiled] Uploading coiled_local_identifyingLandSubdivisions...\n",
      "[2025-03-27 11:31:14,116][INFO    ][coiled] Requesting package sync build...\n",
      "[2025-03-27 11:31:14,827][INFO    ][coiled] Creating Cluster (name: ils-sara, https://cloud.coiled.io/clusters/812060?account=wri-cities-data ). This usually takes 1-2 minutes...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started a new Dask client on Coiled. Dashboard is available at https://cluster-mxmgm.dask.host/0SXtF1t0_MxnecAm/status\n"
     ]
    }
   ],
   "source": [
    "import coiled\n",
    "\n",
    "cluster = coiled.Cluster(\n",
    "    workspace=\"wri-cities-data\",\n",
    "    name=f'ils-{YOUR_NAME}',\n",
    "    region=\"us-west-2\",\n",
    "    arm=True,\n",
    "    worker_vm_types=\"r8g.xlarge\",\n",
    "    spot_policy=\"spot\",\n",
    "    n_workers=8,\n",
    "    package_sync_ignore=[\"pyspark\", \"pypandoc\"]\n",
    ")\n",
    "client = cluster.get_client()\n",
    "\n",
    "print(f\"Started a new Dask client on Coiled. Dashboard is available at {client.dashboard_link}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_geopandas as dgpd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape, Point, LineString, Polygon, MultiPolygon, MultiLineString\n",
    "from shapely.geometry.base import BaseGeometry\n",
    "from shapely.strtree import STRtree\n",
    "from shapely.errors import ShapelyError\n",
    "import dask\n",
    "from dask import delayed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.wkb import loads as wkb_loads\n",
    "from dask import compute\n",
    "from citywide_calculation import get_utm_crs\n",
    "import s3fs\n",
    "import fsspec\n",
    "import traceback\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs = s3fs.S3FileSystem(anon=False)\n",
    "search_buffer_files = fs.ls(SEARCH_BUFFER_PATH)\n",
    "\n",
    "cities = [x.split('/')[-1] for x in search_buffer_files]\n",
    "len(cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def get_epsg(city_name):\n",
    "    search_buffer = f'{SEARCH_BUFFER_PATH}/{city_name}/{city_name}_search_buffer.geoparquet'\n",
    "    extent = dgpd.read_parquet(search_buffer)\n",
    "    geometry = extent.geometry[0].compute()\n",
    "    epsg = get_utm_crs(geometry)\n",
    "    print(f'{city_name} EPSG: {epsg}')\n",
    "    return epsg\n",
    "\n",
    "def load_dataset(path, epsg=None):\n",
    "    dataset = dgpd.read_parquet(path, npartitions=4)\n",
    "    \n",
    "    # Only assign if the file has no CRS\n",
    "    if epsg:\n",
    "        if dataset.crs is None:\n",
    "            dataset = dataset.set_crs(\"EPSG:4326\")  # assume WGS84 if missing\n",
    "        dataset = dataset.to_crs(epsg)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 1650.1406290531158\n"
     ]
    }
   ],
   "source": [
    "max_distance = 200\n",
    "default_distance = 500\n",
    "\n",
    "def compute_distance_partition(buildings_df, roads_geom_list, max_distance, default_distance):\n",
    "    tree = STRtree(roads_geom_list)\n",
    "\n",
    "    def distance_fn(bgeom):\n",
    "        try:\n",
    "            bgeom = shape(bgeom) if not isinstance(bgeom, BaseGeometry) else bgeom\n",
    "            nearby_indices = tree.query(bgeom.buffer(max_distance))\n",
    "            if nearby_indices is None or len(nearby_indices) == 0:\n",
    "                return default_distance\n",
    "            nearby_geoms = [roads_geom_list[i] for i in nearby_indices]\n",
    "            return min(bgeom.distance(road) for road in nearby_geoms)\n",
    "        except Exception:\n",
    "            return default_distance\n",
    "\n",
    "    buildings_df = buildings_df.copy()\n",
    "    buildings_df['geometry'] = buildings_df['geometry'].apply(shape)  # extra safe\n",
    "    buildings_df[\"distance_to_nearest_road\"] = buildings_df.geometry.apply(distance_fn)\n",
    "    return buildings_df\n",
    "\n",
    "\n",
    "@delayed\n",
    "def calculate_building_distances_to_roads(city_name):\n",
    "    paths = {\n",
    "    'grid': f'{GRIDS_PATH}/{city_name}/{city_name}_{grid_size}m_grid.geoparquet',\n",
    "    'buildings': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}.geoparquet',\n",
    "    'roads': f'{ROADS_PATH}/{city_name}/{city_name}_OSM_roads.geoparquet'\n",
    "    }\n",
    "    epsg = get_epsg(city_name).compute()  \n",
    "    # Load and prepare roads for spatial index\n",
    "    roads = load_dataset(paths['roads'], epsg=epsg).compute()\n",
    "    roads_geom_list = [geom for geom in roads.geometry]\n",
    "\n",
    "    # Load buildings lazily\n",
    "    buildings = load_dataset(paths['buildings'], epsg=epsg)\n",
    "\n",
    "    meta = buildings._meta.assign(distance_to_nearest_road='f8')\n",
    "\n",
    "    # Apply distance computation per partition\n",
    "    buildings_with_dist = buildings.map_partitions(\n",
    "        compute_distance_partition,\n",
    "        roads_geom_list,\n",
    "        max_distance,\n",
    "        default_distance,\n",
    "        meta=meta\n",
    "    )\n",
    "\n",
    "    # Write output\n",
    "    columns_to_keep = ['id', 'geometry','distance_to_nearest_road']\n",
    "    buildings_with_dist = buildings_with_dist[columns_to_keep].set_index('id')\n",
    "    out_path = paths['buildings'].replace(\".geoparquet\", \"_with_distances.geoparquet\")\n",
    "    buildings_with_dist.to_parquet(out_path)\n",
    "    return out_path\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "city_list = [\"Nairobi\",\"Belo_Horizonte\"]\n",
    "\n",
    "def run_all_cities(city_list):\n",
    "    delayed_jobs = [delayed(calculate_building_distances_to_roads)(city) for city in city_list]\n",
    "    results = compute(*delayed_jobs)\n",
    "    return results\n",
    "\n",
    "run_all_cities(city_list)\n",
    "\n",
    "print(\"Elapsed:\", time.time() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 354.11 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py:4735: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 367.83961296081543\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import t, sem, entropy\n",
    "\n",
    "def calculate_standardized_kl_azimuth(buildings_df, bin_width_degrees=5):\n",
    "    azimuths = buildings_df['azimuth'].to_numpy()\n",
    "    num_bins = int(90 / bin_width_degrees)\n",
    "    histogram, _ = np.histogram(azimuths, bins=num_bins, range=(0, 90))\n",
    "    P = histogram / histogram.sum() if histogram.sum() > 0 else np.ones(num_bins) / num_bins\n",
    "    Q = np.ones(num_bins) / num_bins\n",
    "    kl_divergence = entropy(P, Q)\n",
    "    max_kl_divergence = np.log(num_bins)\n",
    "    return kl_divergence / max_kl_divergence\n",
    "\n",
    "def compute_azimuth_partition(df):\n",
    "    def azimuth(geom):\n",
    "        if geom is None or geom.is_empty:\n",
    "            return np.nan\n",
    "        oriented = geom.minimum_rotated_rectangle\n",
    "        coords = list(oriented.exterior.coords)\n",
    "        edge = LineString([coords[0], coords[1]])\n",
    "        dx, dy = edge.xy[0][1] - edge.xy[0][0], edge.xy[1][1] - edge.xy[1][0]\n",
    "        angle = np.degrees(np.arctan2(dy, dx)) % 180\n",
    "        return angle % 90\n",
    "\n",
    "    df = df.copy()\n",
    "    df['azimuth'] = df['geometry'].map(azimuth)\n",
    "    return df\n",
    "\n",
    "@delayed\n",
    "def compute_block_kl_metrics(buildings_blocks):\n",
    "    grouped = buildings_blocks.groupby('block_id')\n",
    "    kl_data = grouped.apply(lambda g: pd.Series({\n",
    "        'standardized_kl': calculate_standardized_kl_azimuth(g),\n",
    "        'n_buildings': len(g),\n",
    "    })).reset_index()\n",
    "    return kl_data\n",
    "\n",
    "def compute_block_grid_weights(blocks, grid):\n",
    "    \"\"\"\n",
    "    Computes the proportional overlap of blocks in each grid cell.\n",
    "    Returns a Dask DataFrame containing block_id, index_right (grid ID), and area_weight.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    #blocks = blocks.rename_axis(index='block_id').reset_index()\n",
    "    grid = grid.rename_axis(index='grid_id').reset_index()\n",
    "\n",
    "    def overlay_partition(blocks_df, grid_df):\n",
    "        \"\"\"Computes intersection between blocks and grid.\"\"\"\n",
    "        return gpd.overlay(blocks_df, grid_df, how='intersection')\n",
    "\n",
    "    #meta = blocks._meta.merge(grid._meta, how=\"outer\")\n",
    "\n",
    "    block_grid_overlap = blocks.map_partitions(overlay_partition, grid)#, meta=meta\n",
    "\n",
    "\n",
    "    # Step 2: Compute area for each block-grid overlap\n",
    "    block_grid_overlap = block_grid_overlap.assign(\n",
    "        overlap_area=block_grid_overlap.map_partitions(lambda df: df.geometry.area, meta=('overlap_area', 'f8'))\n",
    "    )\n",
    "\n",
    "    # Step 3: Compute the total area of each grid cell\n",
    "    grid_areas = grid.assign(grid_area=grid.map_partitions(lambda df: df.geometry.area, meta=('grid_area', 'f8')))\n",
    "\n",
    "\n",
    "    # Step 4: Merge grid cell areas into block-grid overlap\n",
    "    block_grid_overlap = block_grid_overlap.merge(grid_areas[['grid_id','grid_area']], left_on='grid_id', right_on='grid_id', how='left')\n",
    "\n",
    "    # Step 5: Compute area weight as the ratio of overlap to grid cell area\n",
    "    block_grid_overlap = block_grid_overlap.assign(\n",
    "        area_weight=block_grid_overlap['overlap_area'] / block_grid_overlap['grid_area']\n",
    "    )\n",
    "    block_grid_overlap = block_grid_overlap.map_partitions(\n",
    "        lambda df: df.assign(\n",
    "            area_weight=df['area_weight'] / df.groupby(df['grid_id'])['area_weight'].transform('sum')\n",
    "        ),\n",
    "        meta=block_grid_overlap._meta  # Preserve original structure\n",
    "    )\n",
    "\n",
    "    return block_grid_overlap[['block_id', 'optimal_point', 'max_radius', 'grid_id', 'geometry', 'overlap_area', 'grid_area', 'area_weight']]\n",
    "\n",
    "\n",
    "def aggregate_m6(kl_df, overlap_df):\n",
    "    df = overlap_df.merge(kl_df, on='block_id', how='left')\n",
    "    df = df.dropna(subset=['standardized_kl'])\n",
    "    df['weight'] = df['area_weight'] * df['n_buildings']\n",
    "    df['weighted_kl'] = df['standardized_kl'] * df['weight']\n",
    "\n",
    "    group = df.groupby('block_id').agg(\n",
    "        m6_weighted_kl_sum=('weighted_kl', 'sum'),\n",
    "        total_weight=('weight', 'sum')\n",
    "    )\n",
    "    group['m6'] = group['m6_weighted_kl_sum'] / group['total_weight']\n",
    "    return group['m6']\n",
    "\n",
    "\n",
    "def building_distance_metrics(city_name):\n",
    "\n",
    "     grid_cell_count = 0\n",
    "     paths = {\n",
    "         'grid': f'{GRIDS_PATH}/{city_name}/{city_name}_{str(grid_size)}m_grid.geoparquet',\n",
    "         'buildings': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}.geoparquet',\n",
    "         'buildings_with_distances': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}_with_distances.geoparquet',\n",
    "         'roads': f'{ROADS_PATH}/{city_name}/{city_name}_OSM_roads.geoparquet',\n",
    "         'intersections': f'{INTERSECTIONS_PATH}/{city_name}/{city_name}_OSM_intersections.geoparquet'\n",
    "     }\n",
    "     # Get EPSG\n",
    "     epsg = get_epsg(city_name).compute()\n",
    "     # Load grid\n",
    "     grid = load_dataset(paths['grid'], epsg=epsg)#.compute()\n",
    "     if 'geom' in grid.columns:\n",
    "         grid = grid.drop(columns=['geom'])\n",
    "     \n",
    "     buildings = load_dataset(paths['buildings_with_distances'], epsg=epsg)#.compute()\n",
    "     buildings['distance_to_nearest_road'] = buildings['distance_to_nearest_road'].astype(float)\n",
    "     buildings['area'] = buildings.geometry.area\n",
    "     joined_buildings = dgpd.sjoin(buildings, grid, predicate='intersects')  \n",
    "     counts_buildings = joined_buildings.groupby('index_right').size()\n",
    "     grid['n_buildings'] = grid.index.map(counts_buildings).fillna(0).astype(int)\n",
    "     average_distance = joined_buildings.groupby('index_right')['distance_to_nearest_road'].mean()\n",
    "     grid['average_distance_nearest_building'] = grid.index.map(average_distance).fillna(0).astype(float)\n",
    "    \n",
    "    \n",
    "     buildings_closer_than_20m = buildings[buildings['distance_to_nearest_road'] <= 20]\n",
    "     joined_buildings_closer_than_20m = dgpd.sjoin(buildings_closer_than_20m, grid, predicate='intersects') \n",
    "     n_buildings_closer_than_20m = joined_buildings_closer_than_20m.groupby('index_right').size()\n",
    "     grid['n_buildings_closer_than_20m'] = grid.index.map(n_buildings_closer_than_20m).fillna(0).astype(float)\n",
    "    \n",
    "     grid['m1'] = grid['n_buildings_closer_than_20m'] / grid['n_buildings']\n",
    "     grid['m2'] = grid['average_distance_nearest_building']\n",
    "    \n",
    "     path = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{str(grid_size)}m_grid_{YOUR_NAME}_metrics_1_2.geoparquet'\n",
    "    \n",
    "     if 'geom' in grid.columns:\n",
    "         grid = grid.drop(columns='geom')\n",
    "    \n",
    "     grid.to_parquet(path)\n",
    "\n",
    "def building_orientation_metrics(city_name):\n",
    "    paths = {\n",
    "        'grid': f'{GRIDS_PATH}/{city_name}/{city_name}_{str(grid_size)}m_grid.geoparquet',\n",
    "        'blocks': f'{BLOCKS_PATH}/{city_name}/{city_name}_blocks_{YOUR_NAME}.geoparquet',\n",
    "        'buildings_with_distances': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}_with_distances.geoparquet',\n",
    "    }\n",
    "\n",
    "    epsg = get_epsg(city_name).compute()\n",
    "    grid = load_dataset(paths['grid'], epsg=epsg)\n",
    "    blocks = load_dataset(paths['blocks'], epsg=epsg)\n",
    "    buildings = load_dataset(paths['buildings_with_distances'], epsg=epsg)\n",
    "\n",
    "    if 'geom' in grid.columns:\n",
    "        grid = grid.drop(columns=['geom'])\n",
    "\n",
    "    blocks['block_id'] = blocks.index\n",
    "\n",
    "    meta = buildings._meta.copy()\n",
    "    meta['azimuth'] = 'f8'\n",
    "    buildings = buildings.map_partitions(compute_azimuth_partition, meta=meta)\n",
    "\n",
    "    # Fix `sjoin` issues by computing before\n",
    "    buildings_blocks = dgpd.sjoin(buildings.compute(), blocks.compute(), predicate='intersects')\n",
    "    buildings_blocks = buildings_blocks[['block_id', 'geometry', 'azimuth']]\n",
    "\n",
    "    kl_df = compute_block_kl_metrics(buildings_blocks)\n",
    "\n",
    "    # Keep `block_grid_overlap` lazy\n",
    "    block_grid_overlap = compute_block_grid_weights(blocks, grid)\n",
    "\n",
    "    # Aggregate `m6`\n",
    "    m6_series = aggregate_m6(kl_df.compute(), block_grid_overlap.compute())\n",
    "    grid['m6'] = grid.index.map(m6_series).fillna(0)\n",
    "\n",
    "    out_path = paths['gri'].replace(\".geoparquet\", \"_with_distances.geoparquet\")\n",
    "    buildings_with_dist.to_parquet(out_path)\n",
    "    return out_path\n",
    "\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "city_list = [\"Nairobi\"]\n",
    "grid = building_orientation_metrics('Nairobi')\n",
    "\n",
    "print(\"Elapsed:\", time.time() - start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_name</th>\n",
       "      <th>geom</th>\n",
       "      <th>geometry</th>\n",
       "      <th>m6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3424</th>\n",
       "      <td>Nairobi</td>\n",
       "      <td>b'\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x05\\x00...</td>\n",
       "      <td>POLYGON ((276653.479 9883364.725, 276839.021 9...</td>\n",
       "      <td>0.109158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3425</th>\n",
       "      <td>Nairobi</td>\n",
       "      <td>b'\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x05\\x00...</td>\n",
       "      <td>POLYGON ((276839.021 9883364.845, 277024.562 9...</td>\n",
       "      <td>0.327423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3426</th>\n",
       "      <td>Nairobi</td>\n",
       "      <td>b'\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x05\\x00...</td>\n",
       "      <td>POLYGON ((277024.562 9883364.964, 277210.103 9...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3427</th>\n",
       "      <td>Nairobi</td>\n",
       "      <td>b'\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x05\\x00...</td>\n",
       "      <td>POLYGON ((277210.103 9883365.084, 277395.644 9...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3428</th>\n",
       "      <td>Nairobi</td>\n",
       "      <td>b'\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x05\\x00...</td>\n",
       "      <td>POLYGON ((277395.644 9883365.203, 277581.185 9...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56206</th>\n",
       "      <td>Nairobi</td>\n",
       "      <td>b'\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x05\\x00...</td>\n",
       "      <td>POLYGON ((272430.048 9828982.878, 272615.558 9...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56207</th>\n",
       "      <td>Nairobi</td>\n",
       "      <td>b'\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x05\\x00...</td>\n",
       "      <td>POLYGON ((272615.558 9828983.056, 272801.068 9...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56208</th>\n",
       "      <td>Nairobi</td>\n",
       "      <td>b'\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x05\\x00...</td>\n",
       "      <td>POLYGON ((272801.068 9828983.235, 272986.577 9...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56209</th>\n",
       "      <td>Nairobi</td>\n",
       "      <td>b'\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x05\\x00...</td>\n",
       "      <td>POLYGON ((272986.577 9828983.413, 273172.086 9...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56210</th>\n",
       "      <td>Nairobi</td>\n",
       "      <td>b'\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x05\\x00...</td>\n",
       "      <td>POLYGON ((273172.086 9828983.592, 273357.596 9...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52787 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      city_name                                               geom  \\\n",
       "3424    Nairobi  b'\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x05\\x00...   \n",
       "3425    Nairobi  b'\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x05\\x00...   \n",
       "3426    Nairobi  b'\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x05\\x00...   \n",
       "3427    Nairobi  b'\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x05\\x00...   \n",
       "3428    Nairobi  b'\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x05\\x00...   \n",
       "...         ...                                                ...   \n",
       "56206   Nairobi  b'\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x05\\x00...   \n",
       "56207   Nairobi  b'\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x05\\x00...   \n",
       "56208   Nairobi  b'\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x05\\x00...   \n",
       "56209   Nairobi  b'\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x05\\x00...   \n",
       "56210   Nairobi  b'\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x05\\x00...   \n",
       "\n",
       "                                                geometry        m6  \n",
       "3424   POLYGON ((276653.479 9883364.725, 276839.021 9...  0.109158  \n",
       "3425   POLYGON ((276839.021 9883364.845, 277024.562 9...  0.327423  \n",
       "3426   POLYGON ((277024.562 9883364.964, 277210.103 9...  0.000000  \n",
       "3427   POLYGON ((277210.103 9883365.084, 277395.644 9...  1.000000  \n",
       "3428   POLYGON ((277395.644 9883365.203, 277581.185 9...  0.000000  \n",
       "...                                                  ...       ...  \n",
       "56206  POLYGON ((272430.048 9828982.878, 272615.558 9...  0.000000  \n",
       "56207  POLYGON ((272615.558 9828983.056, 272801.068 9...  0.000000  \n",
       "56208  POLYGON ((272801.068 9828983.235, 272986.577 9...  0.000000  \n",
       "56209  POLYGON ((272986.577 9828983.413, 273172.086 9...  0.000000  \n",
       "56210  POLYGON ((273172.086 9828983.592, 273357.596 9...  0.000000  \n",
       "\n",
       "[52787 rows x 4 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grid_nairobi = grid.compute()\n",
    "grid_nairobi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_name = 'Nairobi'\n",
    "paths = {\n",
    "    'grid': f'{GRIDS_PATH}/{city_name}/{city_name}_{str(grid_size)}m_grid.geoparquet',\n",
    "    'blocks': f'{BLOCKS_PATH}/{city_name}/{city_name}_blocks_{YOUR_NAME}.geoparquet',\n",
    "    'buildings_with_distances': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}_with_distances.geoparquet',\n",
    "}\n",
    "\n",
    "epsg = get_epsg(city_name).compute()\n",
    "grid = load_dataset(paths['grid'], epsg=epsg)\n",
    "blocks = load_dataset(paths['blocks'], epsg=epsg)\n",
    "buildings = load_dataset(paths['buildings_with_distances'], epsg=epsg)\n",
    "\n",
    "blocks['block_id'] = blocks.index\n",
    "\n",
    "meta = buildings._meta.copy()\n",
    "meta['azimuth'] = 'f8'\n",
    "buildings = buildings.map_partitions(compute_azimuth_partition, meta=meta)\n",
    "\n",
    "# Fix `sjoin` issues by computing before\n",
    "buildings_blocks = dgpd.sjoin(buildings.compute(), blocks.compute(), predicate='intersects')\n",
    "buildings_blocks = buildings_blocks[['block_id', 'geometry', 'azimuth']]\n",
    "\n",
    "kl_df = compute_block_kl_metrics(buildings_blocks)\n",
    "\n",
    "# Keep `block_grid_overlap` lazy\n",
    "block_grid_overlap = compute_block_grid_weights(blocks, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 354.11 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "kl_df_df = kl_df.compute()\n",
    "block_grid_overlap_df = block_grid_overlap.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py:4735: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/IPython/core/formatters.py:770\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    763\u001b[0m stream \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[1;32m    764\u001b[0m printer \u001b[38;5;241m=\u001b[39m pretty\u001b[38;5;241m.\u001b[39mRepresentationPrinter(stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnewline,\n\u001b[1;32m    766\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_length,\n\u001b[1;32m    767\u001b[0m     singleton_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingleton_printers,\n\u001b[1;32m    768\u001b[0m     type_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_printers,\n\u001b[1;32m    769\u001b[0m     deferred_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeferred_printers)\n\u001b[0;32m--> 770\u001b[0m \u001b[43mprinter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    771\u001b[0m printer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/IPython/lib/pretty.py:419\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    408\u001b[0m                         \u001b[38;5;28;01mreturn\u001b[39;00m meth(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    409\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    410\u001b[0m                     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\n\u001b[1;32m    411\u001b[0m                     \u001b[38;5;66;03m# check if cls defines __repr__\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    417\u001b[0m                     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(_safe_getattr(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__repr__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    418\u001b[0m                 ):\n\u001b[0;32m--> 419\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_repr_pprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_pprint(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/IPython/lib/pretty.py:794\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;66;03m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[0;32m--> 794\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    795\u001b[0m lines \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgroup():\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py:4692\u001b[0m, in \u001b[0;36mIndex.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4691\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 4692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<dask_expr.expr.Index: expr=\u001b[39m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpr\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/_expr.py:76\u001b[0m, in \u001b[0;36mExpr.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 76\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_operands_for_repr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/dataframe/dask_expr/_expr.py:147\u001b[0m, in \u001b[0;36mExpr._operands_for_repr\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param, operand \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperands):\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(operand, Expr) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(operand, (pd\u001b[38;5;241m.\u001b[39mSeries, pd\u001b[38;5;241m.\u001b[39mDataFrame))\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m operand \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_defaults\u001b[38;5;241m.\u001b[39mget(param)\n\u001b[1;32m    146\u001b[0m     ):\n\u001b[0;32m--> 147\u001b[0m         to_include\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43moperand\u001b[49m\u001b[38;5;132;43;01m!r}\u001b[39;49;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m to_include\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/_expr.py:80\u001b[0m, in \u001b[0;36mExpr.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/_expr.py:76\u001b[0m, in \u001b[0;36mExpr.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 76\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_operands_for_repr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/dataframe/dask_expr/_expr.py:145\u001b[0m, in \u001b[0;36mExpr._operands_for_repr\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m to_include \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param, operand \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperands):\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(operand, Expr) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(operand, (pd\u001b[38;5;241m.\u001b[39mSeries, pd\u001b[38;5;241m.\u001b[39mDataFrame))\n\u001b[0;32m--> 145\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m operand \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_defaults\u001b[38;5;241m.\u001b[39mget(param)\n\u001b[1;32m    146\u001b[0m     ):\n\u001b[1;32m    147\u001b[0m         to_include\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperand\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m to_include\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "m6_series = aggregate_m6(kl_df_df, block_grid_overlap_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask_expr.expr.Index: expr=Index(frame=MapPartitions(to_crs))>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py:4735: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/IPython/core/formatters.py:770\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    763\u001b[0m stream \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[1;32m    764\u001b[0m printer \u001b[38;5;241m=\u001b[39m pretty\u001b[38;5;241m.\u001b[39mRepresentationPrinter(stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnewline,\n\u001b[1;32m    766\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_length,\n\u001b[1;32m    767\u001b[0m     singleton_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingleton_printers,\n\u001b[1;32m    768\u001b[0m     type_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_printers,\n\u001b[1;32m    769\u001b[0m     deferred_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeferred_printers)\n\u001b[0;32m--> 770\u001b[0m \u001b[43mprinter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    771\u001b[0m printer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/IPython/lib/pretty.py:419\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    408\u001b[0m                         \u001b[38;5;28;01mreturn\u001b[39;00m meth(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    409\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    410\u001b[0m                     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\n\u001b[1;32m    411\u001b[0m                     \u001b[38;5;66;03m# check if cls defines __repr__\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    417\u001b[0m                     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(_safe_getattr(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__repr__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    418\u001b[0m                 ):\n\u001b[0;32m--> 419\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_repr_pprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_pprint(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/IPython/lib/pretty.py:794\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;66;03m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[0;32m--> 794\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    795\u001b[0m lines \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgroup():\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py:4692\u001b[0m, in \u001b[0;36mIndex.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4691\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 4692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<dask_expr.expr.Index: expr=\u001b[39m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpr\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/_expr.py:76\u001b[0m, in \u001b[0;36mExpr.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 76\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_operands_for_repr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/dataframe/dask_expr/_expr.py:145\u001b[0m, in \u001b[0;36mExpr._operands_for_repr\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m to_include \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param, operand \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperands):\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(operand, Expr) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(operand, (pd\u001b[38;5;241m.\u001b[39mSeries, pd\u001b[38;5;241m.\u001b[39mDataFrame))\n\u001b[0;32m--> 145\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m operand \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_defaults\u001b[38;5;241m.\u001b[39mget(param)\n\u001b[1;32m    146\u001b[0m     ):\n\u001b[1;32m    147\u001b[0m         to_include\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperand\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m to_include\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "#m6_series = aggregate_m6(kl_df_df, block_grid_overlap_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid['m6'] = grid.index.map(m6_series).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_geopandas as dgpd\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from shapely.geometry import Polygon\n",
    "from dask import delayed\n",
    "\n",
    "def compute_block_grid_weights(blocks, grid):\n",
    "    \"\"\"\n",
    "    Computes the proportional overlap of blocks in each grid cell.\n",
    "    Returns a Dask DataFrame containing block_id, index_right (grid ID), and area_weight.\n",
    "    \"\"\"\n",
    "\n",
    "    if 'geom' in grid.columns:\n",
    "        grid = grid.drop(columns=['geom'])\n",
    "\n",
    "    blocks = blocks.rename_axis(index='block_id').reset_index()\n",
    "    grid = grid.rename_axis(index='grid_id').reset_index()\n",
    "\n",
    "    def overlay_partition(blocks_df, grid_df):\n",
    "        \"\"\"Computes intersection between blocks and grid.\"\"\"\n",
    "        return gpd.overlay(blocks_df, grid_df, how='intersection')\n",
    "\n",
    "    #meta = blocks._meta.merge(grid._meta, how=\"outer\")\n",
    "\n",
    "    block_grid_overlap = blocks.map_partitions(overlay_partition, grid)#, meta=meta\n",
    "\n",
    "\n",
    "    # Step 2: Compute area for each block-grid overlap\n",
    "    block_grid_overlap = block_grid_overlap.assign(\n",
    "        overlap_area=block_grid_overlap.map_partitions(lambda df: df.geometry.area, meta=('overlap_area', 'f8'))\n",
    "    )\n",
    "\n",
    "    # Step 3: Compute the total area of each grid cell\n",
    "    grid_areas = grid.assign(grid_area=grid.map_partitions(lambda df: df.geometry.area, meta=('grid_area', 'f8')))\n",
    "\n",
    "\n",
    "    # Step 4: Merge grid cell areas into block-grid overlap\n",
    "    block_grid_overlap = block_grid_overlap.merge(grid_areas[['grid_id','grid_area']], left_on='grid_id', right_on='grid_id', how='left')\n",
    "\n",
    "    # Step 5: Compute area weight as the ratio of overlap to grid cell area\n",
    "    block_grid_overlap = block_grid_overlap.assign(\n",
    "        area_weight=block_grid_overlap['overlap_area'] / block_grid_overlap['grid_area']\n",
    "    )\n",
    "    block_grid_overlap = block_grid_overlap.map_partitions(\n",
    "        lambda df: df.assign(\n",
    "            area_weight=df['area_weight'] / df.groupby(df['grid_id'])['area_weight'].transform('sum')\n",
    "        ),\n",
    "        meta=block_grid_overlap._meta  # Preserve original structure\n",
    "    )\n",
    "\n",
    "    return block_grid_overlap[['block_id', 'optimal_point', 'max_radius', 'grid_id', 'geometry', 'overlap_area', 'grid_area', 'area_weight']]\n",
    "\n",
    "\n",
    "city_list = [\"Nairobi\", \"Belo_Horizonte\"]  # Add more cities here\n",
    "\n",
    "@delayed\n",
    "def create_city_blocks_grid_overlap(city_name):\n",
    "    \"\"\"Loads data and computes block-grid weights for a city.\"\"\"\n",
    "    paths = {\n",
    "        'grid': f'{GRIDS_PATH}/{city_name}/{city_name}_{str(grid_size)}m_grid.geoparquet',\n",
    "        'blocks': f'{BLOCKS_PATH}/{city_name}/{city_name}_blocks_{YOUR_NAME}.geoparquet',\n",
    "        'buildings_with_distances': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}_with_distances.geoparquet',\n",
    "    }\n",
    "\n",
    "    epsg = get_epsg(city_name).compute()\n",
    "    grid = load_dataset(paths['grid'], epsg=epsg)\n",
    "    blocks = load_dataset(paths['blocks'], epsg=epsg)\n",
    "\n",
    "    block_grid_weights = compute_block_grid_weights(blocks, grid)\n",
    "\n",
    "    return block_grid_weights.compute()\n",
    "\n",
    "delayed_jobs = [create_city_blocks_grid_overlap(city) for city in city_list]\n",
    "\n",
    "results = compute(*delayed_jobs)  \n",
    "\n",
    "city_results = {city: result for city, result in zip(city_list, results)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subdivisions2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
