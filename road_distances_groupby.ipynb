{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_NAME = 'sara'\n",
    "\n",
    "AWS_PROFILE = 'cities'\n",
    "\n",
    "'''\n",
    "# List of cities to process\n",
    "cities = [\"Belo Horizonte\", \"Campinas\"]#, \"Bogota\", \"Nairobi\", \"Bamako\", \n",
    "        #\"Lagos\", \"Accra\", \"Abidjan\", \"Mogadishu\", \"Cape Town\", \n",
    "        #\"Maputo\", \"Luanda\"]\n",
    "\n",
    "test_cities = [\"Belo Horizonte\"]\n",
    "#cities = test_cities\n",
    "\n",
    "cities = [city.replace(' ', '_') for city in cities]\n",
    "\n",
    "search_buffer_files = fs.ls(SEARCH_BUFFER_PATH)\n",
    "\n",
    "cities \n",
    "\n",
    "number_of_cities = len(cities)\n",
    "\n",
    "print(f'City count: {number_of_cities}')\n",
    "'''\n",
    "grid_size = 200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = \"s3://wri-cities-sandbox/identifyingLandSubdivisions/data\"\n",
    "INPUT_PATH = f'{MAIN_PATH}/input'\n",
    "CITY_INFO_PATH = f'{INPUT_PATH}/city_info'\n",
    "EXTENTS_PATH = f'{CITY_INFO_PATH}/extents'\n",
    "BUILDINGS_PATH = f'{INPUT_PATH}/buildings'\n",
    "BUILDINGS_DISTANCES_PATH = f'{INPUT_PATH}/buildings_with_distances'\n",
    "ROADS_PATH = f'{INPUT_PATH}/roads'\n",
    "INTERSECTIONS_PATH = f'{INPUT_PATH}/intersections'\n",
    "GRIDS_PATH = f'{INPUT_PATH}/city_info/grids'\n",
    "SEARCH_BUFFER_PATH = f'{INPUT_PATH}/city_info/search_buffers'\n",
    "BLOCKS_PATH = f'{INPUT_PATH}/blocks'\n",
    "OUTPUT_PATH = f'{MAIN_PATH}/output'\n",
    "OUTPUT_PATH_CSV = f'{OUTPUT_PATH}/csv'\n",
    "OUTPUT_PATH_RASTER = f'{OUTPUT_PATH}/raster'\n",
    "OUTPUT_PATH_PNG = f'{OUTPUT_PATH}/png'\n",
    "OUTPUT_PATH_RAW = f'{OUTPUT_PATH}/raw_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'F62522GEAWZPC7AT',\n",
       "  'HostId': 'hxY+SIz4/FYIgyjAjPOwwJTka4LDIu7v/E8MRUAtsIpivJeblJj15eSLdlZT8xByvUQVgelb+Bw=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'hxY+SIz4/FYIgyjAjPOwwJTka4LDIu7v/E8MRUAtsIpivJeblJj15eSLdlZT8xByvUQVgelb+Bw=',\n",
       "   'x-amz-request-id': 'F62522GEAWZPC7AT',\n",
       "   'date': 'Thu, 27 Mar 2025 21:52:18 GMT',\n",
       "   'content-type': 'application/xml',\n",
       "   'transfer-encoding': 'chunked',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'Buckets': [{'Name': 'aft-sandbox-540362055257',\n",
       "   'CreationDate': datetime.datetime(2022, 9, 13, 15, 12, 20, tzinfo=tzutc())},\n",
       "  {'Name': 'amplify-citiesindicatorsapi-dev-10508-deployment',\n",
       "   'CreationDate': datetime.datetime(2023, 8, 30, 5, 5, 13, tzinfo=tzutc())},\n",
       "  {'Name': 'cities-dev-sandbox',\n",
       "   'CreationDate': datetime.datetime(2025, 2, 7, 23, 18, 12, tzinfo=tzutc())},\n",
       "  {'Name': 'cities-heat',\n",
       "   'CreationDate': datetime.datetime(2023, 6, 1, 13, 22, 1, tzinfo=tzutc())},\n",
       "  {'Name': 'era5-brazil',\n",
       "   'CreationDate': datetime.datetime(2025, 2, 15, 19, 51, 14, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-athena-us-west-2',\n",
       "   'CreationDate': datetime.datetime(2024, 1, 12, 18, 45, 11, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-climate-hazards',\n",
       "   'CreationDate': datetime.datetime(2024, 1, 3, 16, 57, 31, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-data-api',\n",
       "   'CreationDate': datetime.datetime(2024, 7, 16, 8, 53, 31, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-heat',\n",
       "   'CreationDate': datetime.datetime(2024, 3, 25, 15, 46, 55, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-indicators',\n",
       "   'CreationDate': datetime.datetime(2024, 5, 13, 15, 50, 58, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-sandbox',\n",
       "   'CreationDate': datetime.datetime(2024, 7, 27, 0, 51, 38, tzinfo=tzutc())}],\n",
       " 'Owner': {'DisplayName': 'aws-cities',\n",
       "  'ID': 'df12253943982d72f60594f06c2cacf9a1ee3a9e738c1649c9fb96e5127f1a5c'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check s3 connection using AWS_PROFILE=CitiesUserPermissionSet profile \n",
    "import boto3\n",
    "\n",
    "session = boto3.Session(profile_name=AWS_PROFILE)\n",
    "s3 = session.client('s3')\n",
    "\n",
    "# export CitiesUserPermissionSet profile to use in the next cells\n",
    "import os\n",
    "os.environ['AWS_PROFILE'] = AWS_PROFILE\n",
    "\n",
    "\n",
    "s3.list_buckets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-27 16:52:22,045][INFO    ][coiled] Fetching latest package priorities...\n",
      "[2025-03-27 16:52:22,046][INFO    ][coiled.package_sync] Resolving your local subdivisions2 Python environment...\n",
      "[2025-03-27 16:52:22,937][INFO    ][coiled.package_sync] Scanning 444 conda packages...\n",
      "[2025-03-27 16:52:22,945][INFO    ][coiled.package_sync] Scanning 259 python packages...\n",
      "[2025-03-27 16:52:23,735][INFO    ][coiled] Running pip check...\n",
      "[2025-03-27 16:52:25,215][INFO    ][coiled] Validating environment...\n",
      "[2025-03-27 16:52:27,434][INFO    ][coiled] Creating wheel for ~/Documents/Identifying Land Subdivisions/identifyingLandSubdivisions...\n",
      "[2025-03-27 16:52:27,713][WARNING ][coiled.package_sync] Package - libopenvino-intel-cpu-plugin, libopenvino-intel-cpu-plugin~=2025.0.0 has no install candidate for Python 3.12 linux-aarch64 on conda-forge\n",
      "[2025-03-27 16:52:27,714][INFO    ][coiled] Uploading coiled_local_identifyingLandSubdivisions...\n",
      "[2025-03-27 16:52:28,636][INFO    ][coiled] Requesting package sync build...\n",
      "[2025-03-27 16:52:29,419][INFO    ][coiled] Creating Cluster (name: ils-sara, https://cloud.coiled.io/clusters/812439?account=wri-cities-data ). This usually takes 1-2 minutes...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started a new Dask client on Coiled. Dashboard is available at https://cluster-pwsua.dask.host/GSUq14YS9YNiASnN/status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-2344' coro=<Client._gather.<locals>.wait() done, defined at /Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/distributed/client.py:2394> exception=AllExit()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/distributed/client.py\", line 2403, in wait\n",
      "    raise AllExit()\n",
      "distributed.client.AllExit\n"
     ]
    }
   ],
   "source": [
    "import coiled\n",
    "\n",
    "cluster = coiled.Cluster(\n",
    "    workspace=\"wri-cities-data\",\n",
    "    name=f'ils-{YOUR_NAME}',\n",
    "    region=\"us-west-2\",\n",
    "    arm=True,\n",
    "    worker_vm_types=\"r8g.xlarge\",\n",
    "    spot_policy=\"spot\",\n",
    "    n_workers=8,\n",
    "    package_sync_ignore=[\"pyspark\", \"pypandoc\"]\n",
    ")\n",
    "client = cluster.get_client()\n",
    "\n",
    "print(f\"Started a new Dask client on Coiled. Dashboard is available at {client.dashboard_link}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_geopandas as dgpd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape, Point, LineString, Polygon, MultiPolygon, MultiLineString\n",
    "from shapely.geometry.base import BaseGeometry\n",
    "from shapely.strtree import STRtree\n",
    "from shapely.errors import ShapelyError\n",
    "import dask\n",
    "from dask import delayed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.wkb import loads as wkb_loads\n",
    "from dask import compute\n",
    "from citywide_calculation import get_utm_crs\n",
    "import s3fs\n",
    "import fsspec\n",
    "import traceback\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs = s3fs.S3FileSystem(anon=False)\n",
    "search_buffer_files = fs.ls(SEARCH_BUFFER_PATH)\n",
    "\n",
    "cities = [x.split('/')[-1] for x in search_buffer_files]\n",
    "len(cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def get_epsg(city_name):\n",
    "    search_buffer = f'{SEARCH_BUFFER_PATH}/{city_name}/{city_name}_search_buffer.geoparquet'\n",
    "    extent = dgpd.read_parquet(search_buffer)\n",
    "    geometry = extent.geometry[0].compute()\n",
    "    epsg = get_utm_crs(geometry)\n",
    "    print(f'{city_name} EPSG: {epsg}')\n",
    "    return epsg\n",
    "\n",
    "def load_dataset(path, epsg=None):\n",
    "    dataset = dgpd.read_parquet(path, npartitions=4)\n",
    "    \n",
    "    # Only assign if the file has no CRS\n",
    "    if epsg:\n",
    "        if dataset.crs is None:\n",
    "            dataset = dataset.set_crs(\"EPSG:4326\")  # assume WGS84 if missing\n",
    "        dataset = dataset.to_crs(epsg)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 1650.1406290531158\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "THIS IS PRE-PROCESSING\n",
    "'''\n",
    "\n",
    "max_distance = 200.\n",
    "default_distance = 500.\n",
    "\n",
    "def compute_distance_partition(buildings_df, roads_geom_list, max_distance, default_distance):\n",
    "    tree = STRtree(roads_geom_list)\n",
    "\n",
    "    def distance_fn(bgeom):\n",
    "        try:\n",
    "            bgeom = shape(bgeom) if not isinstance(bgeom, BaseGeometry) else bgeom\n",
    "            nearby_indices = tree.query(bgeom.buffer(max_distance))\n",
    "            if nearby_indices is None or len(nearby_indices) == 0:\n",
    "                return default_distance\n",
    "            nearby_geoms = [roads_geom_list[i] for i in nearby_indices]\n",
    "            return min(bgeom.distance(road) for road in nearby_geoms)\n",
    "        except Exception:\n",
    "            return default_distance\n",
    "\n",
    "    buildings_df = buildings_df.copy()\n",
    "    buildings_df['geometry'] = buildings_df['geometry'].apply(shape)  # extra safe\n",
    "    buildings_df[\"distance_to_nearest_road\"] = buildings_df.geometry.apply(distance_fn)\n",
    "    return buildings_df\n",
    "\n",
    "\n",
    "@delayed\n",
    "def calculate_building_distances_to_roads(city_name):\n",
    "    paths = {\n",
    "    'grid': f'{GRIDS_PATH}/{city_name}/{city_name}_{grid_size}m_grid.geoparquet',\n",
    "    'buildings': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}.geoparquet',\n",
    "    'roads': f'{ROADS_PATH}/{city_name}/{city_name}_OSM_roads.geoparquet'\n",
    "    }\n",
    "    epsg = get_epsg(city_name).compute()  \n",
    "    # Load and prepare roads for spatial index\n",
    "    roads = load_dataset(paths['roads'], epsg=epsg).compute()\n",
    "    roads_geom_list = [geom for geom in roads.geometry]\n",
    "\n",
    "    # Load buildings lazily\n",
    "    buildings = load_dataset(paths['buildings'], epsg=epsg)\n",
    "\n",
    "    meta = buildings._meta.assign(distance_to_nearest_road='f8')\n",
    "\n",
    "    # Apply distance computation per partition\n",
    "    buildings_with_dist = buildings.map_partitions(\n",
    "        compute_distance_partition,\n",
    "        roads_geom_list,\n",
    "        max_distance,\n",
    "        default_distance,\n",
    "        meta=meta\n",
    "    )\n",
    "\n",
    "    # Write output\n",
    "    columns_to_keep = ['id', 'geometry','distance_to_nearest_road']\n",
    "    buildings_with_dist = buildings_with_dist[columns_to_keep].set_index('id')\n",
    "    out_path = paths['buildings'].replace(\".geoparquet\", \"_with_distances.geoparquet\")\n",
    "    buildings_with_dist.to_parquet(out_path)\n",
    "    return out_path\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "city_list = [\"Nairobi\",\"Belo_Horizonte\"]\n",
    "\n",
    "def run_all_cities(city_list):\n",
    "    delayed_jobs = [delayed(calculate_building_distances_to_roads)(city) for city in city_list]\n",
    "    results = compute(*delayed_jobs)\n",
    "    return results\n",
    "\n",
    "run_all_cities(city_list)\n",
    "\n",
    "print(\"Elapsed:\", time.time() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/distributed/client.py:3370: UserWarning: Sending large graph of size 354.10 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 487.3804180622101\n"
     ]
    }
   ],
   "source": [
    "def building_distance_metrics(city_name):\n",
    "\n",
    "     grid_cell_count = 0\n",
    "     paths = {\n",
    "         'grid': f'{GRIDS_PATH}/{city_name}/{city_name}_{str(grid_size)}m_grid.geoparquet',\n",
    "         'buildings': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}.geoparquet',\n",
    "         'buildings_with_distances': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}_with_distances.geoparquet',\n",
    "         'roads': f'{ROADS_PATH}/{city_name}/{city_name}_OSM_roads.geoparquet',\n",
    "         'intersections': f'{INTERSECTIONS_PATH}/{city_name}/{city_name}_OSM_intersections.geoparquet'\n",
    "     }\n",
    "     # Get EPSG\n",
    "     epsg = get_epsg(city_name).compute()\n",
    "     # Load grid\n",
    "     grid = load_dataset(paths['grid'], epsg=epsg)#.compute()\n",
    "     if 'geom' in grid.columns:\n",
    "         grid = grid.drop(columns=['geom'])\n",
    "     \n",
    "     buildings = load_dataset(paths['buildings_with_distances'], epsg=epsg)#.compute()\n",
    "     buildings['distance_to_nearest_road'] = buildings['distance_to_nearest_road'].astype(float)\n",
    "     buildings['area'] = buildings.geometry.area\n",
    "     joined_buildings = dgpd.sjoin(buildings, grid, predicate='intersects')  \n",
    "     counts_buildings = joined_buildings.groupby('index_right').size()\n",
    "     grid['n_buildings'] = grid.index.map(counts_buildings).fillna(0).astype(int)\n",
    "     average_distance = joined_buildings.groupby('index_right')['distance_to_nearest_road'].mean()\n",
    "     grid['average_distance_nearest_building'] = grid.index.map(average_distance).fillna(0).astype(float)\n",
    "    \n",
    "    \n",
    "     buildings_closer_than_20m = buildings[buildings['distance_to_nearest_road'] <= 20]\n",
    "     joined_buildings_closer_than_20m = dgpd.sjoin(buildings_closer_than_20m, grid, predicate='intersects') \n",
    "     n_buildings_closer_than_20m = joined_buildings_closer_than_20m.groupby('index_right').size()\n",
    "     grid['n_buildings_closer_than_20m'] = grid.index.map(n_buildings_closer_than_20m).fillna(0).astype(float)\n",
    "    \n",
    "     grid['m1'] = grid['n_buildings_closer_than_20m'] / grid['n_buildings']\n",
    "     grid['m2'] = grid['average_distance_nearest_building']\n",
    "    \n",
    "     path = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{str(grid_size)}m_grid_{YOUR_NAME}_metrics_1_2.geoparquet'\n",
    "    \n",
    "     if 'geom' in grid.columns:\n",
    "         grid = grid.drop(columns='geom')\n",
    "    \n",
    "     grid.to_parquet(path)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "city_list = [\"Nairobi\",\"Belo_Horizonte\"]\n",
    "\n",
    "def run_all_cities(city_list):\n",
    "    delayed_jobs = [delayed(building_distance_metrics)(city) for city in city_list]\n",
    "    results = compute(*delayed_jobs)\n",
    "    return results\n",
    "\n",
    "run_all_cities(city_list)\n",
    "\n",
    "print(\"Elapsed:\", time.time() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subdivisions2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
