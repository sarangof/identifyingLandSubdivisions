{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_NAME = 'sara'\n",
    "\n",
    "AWS_PROFILE = 'cities'\n",
    "\n",
    "'''\n",
    "# List of cities to process\n",
    "cities = [\"Belo Horizonte\", \"Campinas\"]#, \"Bogota\", \"Nairobi\", \"Bamako\", \n",
    "        #\"Lagos\", \"Accra\", \"Abidjan\", \"Mogadishu\", \"Cape Town\", \n",
    "        #\"Maputo\", \"Luanda\"]\n",
    "\n",
    "test_cities = [\"Belo Horizonte\"]\n",
    "#cities = test_cities\n",
    "\n",
    "cities = [city.replace(' ', '_') for city in cities]\n",
    "\n",
    "search_buffer_files = fs.ls(SEARCH_BUFFER_PATH)\n",
    "\n",
    "cities \n",
    "\n",
    "number_of_cities = len(cities)\n",
    "\n",
    "print(f'City count: {number_of_cities}')\n",
    "'''\n",
    "grid_size = 200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = \"s3://wri-cities-sandbox/identifyingLandSubdivisions/data\"\n",
    "INPUT_PATH = f'{MAIN_PATH}/input'\n",
    "CITY_INFO_PATH = f'{INPUT_PATH}/city_info'\n",
    "EXTENTS_PATH = f'{CITY_INFO_PATH}/extents'\n",
    "BUILDINGS_PATH = f'{INPUT_PATH}/buildings'\n",
    "BLOCKS_PATH = f'{INPUT_PATH}/blocks'\n",
    "ROADS_PATH = f'{INPUT_PATH}/roads'\n",
    "INTERSECTIONS_PATH = f'{INPUT_PATH}/intersections'\n",
    "GRIDS_PATH = f'{INPUT_PATH}/city_info/grids'\n",
    "SEARCH_BUFFER_PATH = f'{INPUT_PATH}/city_info/search_buffers'\n",
    "OUTPUT_PATH = f'{MAIN_PATH}/output'\n",
    "OUTPUT_PATH_CSV = f'{OUTPUT_PATH}/csv'\n",
    "OUTPUT_PATH_RASTER = f'{OUTPUT_PATH}/raster'\n",
    "OUTPUT_PATH_PNG = f'{OUTPUT_PATH}/png'\n",
    "OUTPUT_PATH_RAW = f'{OUTPUT_PATH}/raw_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '26T9JK5P364MWS9P',\n",
       "  'HostId': 'l81uvTxG7BaKakYsRmSW+VO/rs4uRa4x962jAKcIQ4EdZf98sj+xYoN3RNLLiZgEYApEcblK6vs=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'l81uvTxG7BaKakYsRmSW+VO/rs4uRa4x962jAKcIQ4EdZf98sj+xYoN3RNLLiZgEYApEcblK6vs=',\n",
       "   'x-amz-request-id': '26T9JK5P364MWS9P',\n",
       "   'date': 'Fri, 28 Mar 2025 01:54:17 GMT',\n",
       "   'content-type': 'application/xml',\n",
       "   'transfer-encoding': 'chunked',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'Buckets': [{'Name': 'aft-sandbox-540362055257',\n",
       "   'CreationDate': datetime.datetime(2022, 9, 13, 15, 12, 20, tzinfo=tzutc())},\n",
       "  {'Name': 'amplify-citiesindicatorsapi-dev-10508-deployment',\n",
       "   'CreationDate': datetime.datetime(2023, 8, 30, 5, 5, 13, tzinfo=tzutc())},\n",
       "  {'Name': 'cities-dev-sandbox',\n",
       "   'CreationDate': datetime.datetime(2025, 2, 7, 23, 18, 12, tzinfo=tzutc())},\n",
       "  {'Name': 'cities-heat',\n",
       "   'CreationDate': datetime.datetime(2023, 6, 1, 13, 22, 1, tzinfo=tzutc())},\n",
       "  {'Name': 'era5-brazil',\n",
       "   'CreationDate': datetime.datetime(2025, 2, 15, 19, 51, 14, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-athena-us-west-2',\n",
       "   'CreationDate': datetime.datetime(2024, 1, 12, 18, 45, 11, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-climate-hazards',\n",
       "   'CreationDate': datetime.datetime(2024, 1, 3, 16, 57, 31, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-data-api',\n",
       "   'CreationDate': datetime.datetime(2024, 7, 16, 8, 53, 31, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-heat',\n",
       "   'CreationDate': datetime.datetime(2024, 3, 25, 15, 46, 55, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-indicators',\n",
       "   'CreationDate': datetime.datetime(2024, 5, 13, 15, 50, 58, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-sandbox',\n",
       "   'CreationDate': datetime.datetime(2024, 7, 27, 0, 51, 38, tzinfo=tzutc())}],\n",
       " 'Owner': {'DisplayName': 'aws-cities',\n",
       "  'ID': 'df12253943982d72f60594f06c2cacf9a1ee3a9e738c1649c9fb96e5127f1a5c'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check s3 connection using AWS_PROFILE=CitiesUserPermissionSet profile \n",
    "import boto3\n",
    "\n",
    "session = boto3.Session(profile_name=AWS_PROFILE)\n",
    "s3 = session.client('s3')\n",
    "\n",
    "# export CitiesUserPermissionSet profile to use in the next cells\n",
    "import os\n",
    "os.environ['AWS_PROFILE'] = AWS_PROFILE\n",
    "\n",
    "\n",
    "s3.list_buckets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-27 20:54:19,746][INFO    ][coiled] Fetching latest package priorities...\n",
      "[2025-03-27 20:54:19,749][INFO    ][coiled.package_sync] Resolving your local subdivisions2 Python environment...\n",
      "[2025-03-27 20:54:20,679][INFO    ][coiled.package_sync] Scanning 444 conda packages...\n",
      "[2025-03-27 20:54:20,687][INFO    ][coiled.package_sync] Scanning 259 python packages...\n",
      "[2025-03-27 20:54:21,797][INFO    ][coiled] Running pip check...\n",
      "[2025-03-27 20:54:23,505][INFO    ][coiled] Validating environment...\n",
      "[2025-03-27 20:54:25,804][INFO    ][coiled] Creating wheel for ~/Documents/Identifying Land Subdivisions/identifyingLandSubdivisions...\n",
      "[2025-03-27 20:54:26,101][WARNING ][coiled.package_sync] Package - libopenvino-intel-cpu-plugin, libopenvino-intel-cpu-plugin~=2025.0.0 has no install candidate for Python 3.12 linux-aarch64 on conda-forge\n",
      "[2025-03-27 20:54:26,102][INFO    ][coiled] Uploading coiled_local_identifyingLandSubdivisions...\n",
      "[2025-03-27 20:54:27,073][INFO    ][coiled] Requesting package sync build...\n",
      "[2025-03-27 20:54:28,152][INFO    ][coiled] Creating Cluster (name: ils-sara, https://cloud.coiled.io/clusters/812658?account=wri-cities-data ). This usually takes 1-2 minutes...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started a new Dask client on Coiled. Dashboard is available at https://cluster-dlfhq.dask.host/hpQtmlKdvbAcrFFL/status\n"
     ]
    }
   ],
   "source": [
    "import coiled\n",
    "\n",
    "cluster = coiled.Cluster(\n",
    "    workspace=\"wri-cities-data\",\n",
    "    name=f'ils-{YOUR_NAME}',\n",
    "    region=\"us-west-2\",\n",
    "    arm=True,\n",
    "    worker_vm_types=\"r8g.xlarge\",\n",
    "    spot_policy=\"spot\",\n",
    "    n_workers=8,\n",
    "    package_sync_ignore=[\"pyspark\", \"pypandoc\"]\n",
    ")\n",
    "client = cluster.get_client()\n",
    "\n",
    "print(f\"Started a new Dask client on Coiled. Dashboard is available at {client.dashboard_link}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import s3fs\n",
    "import fsspec\n",
    "import traceback\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "fs = s3fs.S3FileSystem(anon=False)\n",
    "search_buffer_files = fs.ls(SEARCH_BUFFER_PATH)\n",
    "\n",
    "cities = [x.split('/')[-1] for x in search_buffer_files]\n",
    "len(cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstart_time = time.time()  \\n\\n#cities = [\\'Nairobi\\',\\'Belo_Horizonte\\']\\ncities = [\"Belo Horizonte\", \\'Nairobi\\'] #\"Campinas\", \"Bogota\", \"Nairobi\", \"Bamako\", \"Lagos\", \"Accra\", \"Abidjan\", \"Cape Town\", \"Luanda\"] #\"Maputo\",\"Mogadishu\", \\ncities = [city.replace(\\' \\', \\'_\\') for city in cities]\\n\\ntasks = [produce_blocks(city) for city in cities]\\nresults = compute(*tasks)\\n\\nend_time = time.time()  \\nelapsed_time = end_time - start_time\\n\\nprint(f\"Tasks completed in {elapsed_time:.2f} seconds.\")\\'\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "THIS IS PRE-PROCESSING\n",
    "'''\n",
    "\n",
    "import dask_geopandas as dgpd\n",
    "import pandas as pd\n",
    "from dask import delayed, compute, visualize\n",
    "import geopandas as gpd\n",
    "from dask.diagnostics import ProgressBar\n",
    "from citywide_calculation import get_utm_crs\n",
    "from metrics_calculation import calculate_minimum_distance_to_roads_option_B\n",
    "from shapely.geometry import MultiLineString, LineString, Point\n",
    "from shapely.ops import polygonize, nearest_points\n",
    "#from shapely.geometry import Polygon, LineString, Point, MultiPolygon, MultiLineString, GeometryCollection\n",
    "from scipy.optimize import fminbound, minimize\n",
    "from shapely.ops import unary_union, polygonize\n",
    "import geopandas as gpd\n",
    "from polylabel import polylabel\n",
    "from shapely.geometry import mapping\n",
    "from shapely.geometry import mapping, Point\n",
    "from polylabel import polylabel\n",
    "\n",
    "def to_geojson_dict(geom):\n",
    "    \"\"\"\n",
    "    Convert a Shapely geometry to a GeoJSON-like dict with lists instead of tuples.\n",
    "    \"\"\"\n",
    "    geojson = mapping(geom)\n",
    "    def recursive_convert(obj):\n",
    "        if isinstance(obj, tuple):\n",
    "            return list(obj)\n",
    "        elif isinstance(obj, list):\n",
    "            return [recursive_convert(item) for item in obj]\n",
    "        elif isinstance(obj, dict):\n",
    "            return {k: recursive_convert(v) for k, v in obj.items()}\n",
    "        else:\n",
    "            return obj\n",
    "    return recursive_convert(geojson)\n",
    "\n",
    "def compute_largest_inscribed_circle(geom):\n",
    "    \"\"\"\n",
    "    Compute the largest inscribed circle for a given polygon or multipolygon.\n",
    "\n",
    "    Parameters:\n",
    "      geom (shapely.geometry): A Polygon or MultiPolygon.\n",
    "    \n",
    "    Returns:\n",
    "      tuple: (optimal_point, max_radius) where optimal_point is a shapely Point and max_radius is a float.\n",
    "    \"\"\"\n",
    "    if geom is None or geom.is_empty:\n",
    "        return None, None\n",
    "\n",
    "    if geom.geom_type == 'Polygon':\n",
    "        geojson_poly = to_geojson_dict(geom)\n",
    "        # Pass in the coordinates list instead of the entire dict.\n",
    "        optimal_coords = polylabel(geojson_poly[\"coordinates\"])\n",
    "        optimal = Point(optimal_coords)\n",
    "        radius = geom.boundary.distance(optimal)\n",
    "        return optimal, radius\n",
    "\n",
    "    elif geom.geom_type == 'MultiPolygon':\n",
    "        best_point = None\n",
    "        best_radius = 0\n",
    "        for poly in geom.geoms:\n",
    "            geojson_poly = to_geojson_dict(poly)\n",
    "            optimal_coords = polylabel(geojson_poly[\"coordinates\"])\n",
    "            candidate = Point(optimal_coords)\n",
    "            radius = poly.boundary.distance(candidate)\n",
    "            if radius > best_radius:\n",
    "                best_radius = radius\n",
    "                best_point = candidate\n",
    "        return best_point, best_radius\n",
    "\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "def add_inscribed_circle_info(blocks_gdf):\n",
    "    \"\"\"\n",
    "    Adds two new columns to a blocks GeoDataFrame: 'optimal_point' and 'max_radius'\n",
    "    which indicate the center and radius of the largest inscribed circle for each block.\n",
    "    Converts the optimal_point geometries to WKT strings for Parquet compatibility.\n",
    "    \n",
    "    Parameters:\n",
    "      blocks_gdf (GeoDataFrame): A GeoDataFrame with block polygons.\n",
    "      \n",
    "    Returns:\n",
    "      GeoDataFrame: The input GeoDataFrame with two new columns.\n",
    "    \"\"\"\n",
    "    # Apply the computation for each geometry\n",
    "    results = blocks_gdf.geometry.apply(lambda geom: compute_largest_inscribed_circle(geom))\n",
    "    \n",
    "    # Unpack the tuple results into two new columns\n",
    "    blocks_gdf[\"optimal_point\"] = results.apply(lambda x: x[0])\n",
    "    blocks_gdf[\"max_radius\"] = results.apply(lambda x: x[1])\n",
    "    \n",
    "    # Convert the 'optimal_point' column from Shapely objects to WKT strings\n",
    "    blocks_gdf[\"optimal_point\"] = blocks_gdf[\"optimal_point\"].apply(\n",
    "        lambda geom: geom.wkt if geom is not None else None\n",
    "    )\n",
    "    \n",
    "    return blocks_gdf\n",
    "\n",
    "\n",
    "def get_blocks(roads):\n",
    "    \"\"\"\n",
    "    Create urban blocks from a grid and road network.\n",
    "\n",
    "    Parameters:\n",
    "      grid (GeoDataFrame): A GeoDataFrame of grid polygons defining the city extent.\n",
    "      roads (GeoDataFrame): A GeoDataFrame of road line geometries.\n",
    "    \n",
    "    Returns:\n",
    "      GeoDataFrame: A GeoDataFrame of block polygons.\n",
    "    \"\"\"\n",
    "    # Merge all road geometries into a single geometry\n",
    "    roads_union = unary_union(roads.geometry)\n",
    "    \n",
    "    # Polygonize the road network to generate blocks.\n",
    "    # The polygonize function returns an iterator of Polygons.\n",
    "    blocks_polygons = list(polygonize(roads_union))\n",
    "    \n",
    "    # Create a GeoDataFrame for blocks\n",
    "    blocks_gdf = gpd.GeoDataFrame(geometry=blocks_polygons, crs=roads.crs)\n",
    "    \n",
    "    # Remove any empty geometries resulting from the intersection.\n",
    "    blocks_gdf = blocks_gdf[~blocks_gdf.is_empty]\n",
    "    \n",
    "    return blocks_gdf\n",
    "\n",
    "@delayed\n",
    "def produce_blocks(city_name):\n",
    "    # Construct file paths for the city\n",
    "    paths = {\n",
    "        'grid': f'{GRIDS_PATH}/{city_name}/{city_name}_{str(grid_size)}m_grid.geoparquet',\n",
    "        'buildings': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}.geoparquet',\n",
    "        'roads': f'{ROADS_PATH}/{city_name}/{city_name}_OSM_roads.geoparquet',\n",
    "        'intersections': f'{INTERSECTIONS_PATH}/{city_name}/{city_name}_OSM_intersections.geoparquet'\n",
    "    }\n",
    "    \n",
    "    epsg = get_epsg(city_name)\n",
    "    \n",
    "    roads = load_dataset(paths['roads'], epsg=epsg)\n",
    "    \n",
    "    blocks = get_blocks(roads.compute())\n",
    "\n",
    "    # Now add the inscribed circle information.\n",
    "    blocks = add_inscribed_circle_info(blocks)\n",
    "    \n",
    "    # Define the output path for the blocks geoparquet\n",
    "    path_blocks = f'{BLOCKS_PATH}/{city_name}/{city_name}_blocks_{YOUR_NAME}.geoparquet'\n",
    "\n",
    "    blocks = blocks.set_crs(epsg.compute())\n",
    "\n",
    "    # Convert the geometry column to WKT before saving\n",
    "    #blocks[\"geometry\"] = blocks[\"geometry\"].apply(lambda geom: geom.wkt if geom is not None else None)\n",
    "    \n",
    "    # Save the blocks dataset. \n",
    "    blocks.to_parquet(path_blocks)\n",
    "    \n",
    "    # Optionally, return the output path or any summary info.\n",
    "    return blocks\n",
    "\n",
    "\n",
    "import time\n",
    "'''\n",
    "start_time = time.time()  \n",
    "\n",
    "#cities = ['Nairobi','Belo_Horizonte']\n",
    "cities = [\"Belo Horizonte\", 'Nairobi'] #\"Campinas\", \"Bogota\", \"Nairobi\", \"Bamako\", \"Lagos\", \"Accra\", \"Abidjan\", \"Cape Town\", \"Luanda\"] #\"Maputo\",\"Mogadishu\", \n",
    "cities = [city.replace(' ', '_') for city in cities]\n",
    "\n",
    "tasks = [produce_blocks(city) for city in cities]\n",
    "results = compute(*tasks)\n",
    "\n",
    "end_time = time.time()  \n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Tasks completed in {elapsed_time:.2f} seconds.\")'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t, sem, entropy\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "Auxiliary functions for metric 6\n",
    "'''\n",
    "\n",
    "def calculate_standardized_kl_azimuth(buildings_df, bin_width_degrees=5):\n",
    "    azimuths = buildings_df['azimuth'].to_numpy()\n",
    "    num_bins = int(90 / bin_width_degrees)\n",
    "    histogram, _ = np.histogram(azimuths, bins=num_bins, range=(0, 90))\n",
    "    P = histogram / histogram.sum() if histogram.sum() > 0 else np.ones(num_bins) / num_bins\n",
    "    Q = np.ones(num_bins) / num_bins\n",
    "    kl_divergence = entropy(P, Q)\n",
    "    max_kl_divergence = np.log(num_bins)\n",
    "    return kl_divergence / max_kl_divergence\n",
    "\n",
    "def compute_azimuth_partition(df):\n",
    "    def azimuth(geom):\n",
    "        if geom is None or geom.is_empty:\n",
    "            return np.nan\n",
    "        oriented = geom.minimum_rotated_rectangle\n",
    "        coords = list(oriented.exterior.coords)\n",
    "        edge = LineString([coords[0], coords[1]])\n",
    "        dx, dy = edge.xy[0][1] - edge.xy[0][0], edge.xy[1][1] - edge.xy[1][0]\n",
    "        angle = np.degrees(np.arctan2(dy, dx)) % 180\n",
    "        return angle % 90\n",
    "\n",
    "    df = df.copy()\n",
    "    df['azimuth'] = df['geometry'].map(azimuth)\n",
    "    return df\n",
    "\n",
    "@delayed\n",
    "def compute_block_kl_metrics(buildings_blocks):\n",
    "    grouped = buildings_blocks.groupby('block_id')\n",
    "    kl_data = grouped.apply(lambda g: pd.Series({\n",
    "        'standardized_kl': calculate_standardized_kl_azimuth(g),\n",
    "        'n_buildings': len(g),\n",
    "    })).reset_index()\n",
    "    return kl_data\n",
    "\n",
    "def compute_block_grid_weights(blocks, grid):\n",
    "    \"\"\"\n",
    "    Computes the proportional overlap of blocks in each grid cell.\n",
    "    Returns a Dask DataFrame containing block_id, index_right (grid ID), and area_weight.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    #blocks = blocks.rename_axis(index='block_id').reset_index()\n",
    "    grid = grid.rename_axis(index='grid_id').reset_index()\n",
    "\n",
    "    def overlay_partition(blocks_df, grid_df):\n",
    "        \"\"\"Computes intersection between blocks and grid.\"\"\"\n",
    "        return gpd.overlay(blocks_df, grid_df, how='intersection')\n",
    "\n",
    "    #meta = blocks._meta.merge(grid._meta, how=\"outer\")\n",
    "\n",
    "    block_grid_overlap = blocks.map_partitions(overlay_partition, grid)#, meta=meta\n",
    "\n",
    "\n",
    "    # Step 2: Compute area for each block-grid overlap\n",
    "    block_grid_overlap = block_grid_overlap.assign(\n",
    "        overlap_area=block_grid_overlap.map_partitions(lambda df: df.geometry.area, meta=('overlap_area', 'f8'))\n",
    "    )\n",
    "\n",
    "    # Step 3: Compute the total area of each grid cell\n",
    "    grid_areas = grid.assign(grid_area=grid.map_partitions(lambda df: df.geometry.area, meta=('grid_area', 'f8')))\n",
    "\n",
    "\n",
    "    # Step 4: Merge grid cell areas into block-grid overlap\n",
    "    block_grid_overlap = block_grid_overlap.merge(grid_areas[['grid_id','grid_area']], left_on='grid_id', right_on='grid_id', how='left')\n",
    "\n",
    "    # Step 5: Compute area weight as the ratio of overlap to grid cell area\n",
    "    block_grid_overlap = block_grid_overlap.assign(\n",
    "        area_weight=block_grid_overlap['overlap_area'] / block_grid_overlap['grid_area']\n",
    "    )\n",
    "    block_grid_overlap = block_grid_overlap.map_partitions(\n",
    "        lambda df: df.assign(\n",
    "            area_weight=df['area_weight'] / df.groupby(df['grid_id'])['area_weight'].transform('sum')\n",
    "        ),\n",
    "        meta=block_grid_overlap._meta  # Preserve original structure\n",
    "    )\n",
    "\n",
    "    return block_grid_overlap[['block_id', 'optimal_point', 'max_radius', 'grid_id', 'geometry', 'overlap_area', 'grid_area', 'area_weight']]\n",
    "\n",
    "\n",
    "def aggregate_m6(kl_df, overlap_df):\n",
    "    df = overlap_df.merge(kl_df, on='block_id', how='left')\n",
    "    df = df.dropna(subset=['standardized_kl'])\n",
    "\n",
    "    # Compute weights\n",
    "    df['weight'] = df['area_weight'] * df['n_buildings']\n",
    "    df['weighted_kl'] = df['standardized_kl'] * df['weight']\n",
    "\n",
    "    # Aggregate directly at the GRID level\n",
    "    grid_aggregated = df.groupby('grid_id').agg(\n",
    "        total_weighted_kl=('weighted_kl', 'sum'),\n",
    "        total_weight=('weight', 'sum')\n",
    "    )\n",
    "\n",
    "    # Compute final KL divergence for each grid cell\n",
    "    grid_aggregated['m6'] = grid_aggregated['total_weighted_kl'] / grid_aggregated['total_weight']\n",
    "\n",
    "    return grid_aggregated[['m6']]\n",
    "\n",
    "\n",
    "\n",
    "def building_orientation_metrics(city_name):\n",
    "    paths = {\n",
    "        'grid': f'{GRIDS_PATH}/{city_name}/{city_name}_{str(grid_size)}m_grid.geoparquet',\n",
    "        'blocks': f'{BLOCKS_PATH}/{city_name}/{city_name}_blocks_{YOUR_NAME}.geoparquet',\n",
    "        'buildings_with_distances': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}_with_distances.geoparquet',\n",
    "    }\n",
    "\n",
    "    epsg = get_epsg(city_name).compute()\n",
    "    grid = load_dataset(paths['grid'], epsg=epsg)\n",
    "    blocks = load_dataset(paths['blocks'], epsg=epsg)\n",
    "    buildings = load_dataset(paths['buildings_with_distances'], epsg=epsg)\n",
    "\n",
    "    if 'geom' in grid.columns:\n",
    "        grid = grid.drop(columns=['geom'])\n",
    "\n",
    "    blocks['block_id'] = blocks.index\n",
    "\n",
    "    meta = buildings._meta.copy()\n",
    "    meta['azimuth'] = 'f8'\n",
    "    buildings = buildings.map_partitions(compute_azimuth_partition, meta=meta)\n",
    "\n",
    "    # Fix `sjoin` issues by computing before\n",
    "    buildings_blocks = dgpd.sjoin(buildings.compute(), blocks.compute(), predicate='intersects')\n",
    "    buildings_blocks = buildings_blocks[['block_id', 'geometry', 'azimuth']]\n",
    "\n",
    "    kl_df = compute_block_kl_metrics(buildings_blocks)\n",
    "\n",
    "    # Keep `block_grid_overlap` lazy\n",
    "    block_grid_overlap = compute_block_grid_weights(blocks, grid)\n",
    "\n",
    "    # Aggregate `m6`\n",
    "    m6_grid = aggregate_m6(kl_df.compute(), block_grid_overlap.compute())\n",
    "    grid = grid.merge(m6_grid, left_index=True, right_index=True, how='left')\n",
    "    grid['m6'] = grid['m6'].fillna(0)\n",
    "\n",
    "\n",
    "    path = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{str(grid_size)}m_grid_{YOUR_NAME}_metric_6.geoparquet'\n",
    "    grid.to_parquet(path)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport time\\n\\nstart_time = time.time()  # Start the timer\\n\\n#cities = [\\'Nairobi\\',\\'Belo_Horizonte\\']\\ncities = [\"Belo Horizonte\", \"Campinas\", \"Bogota\", \"Nairobi\", \"Bamako\", \"Lagos\", \"Accra\", \"Abidjan\", \"Cape Town\", \"Luanda\"] #\"Maputo\",\"Mogadishu\", \\ncities = [city.replace(\\' \\', \\'_\\') for city in cities]\\n\\ntasks = [block_metrics(city,YOUR_NAME,grid_size) for city in cities]\\nresults = compute(*tasks)\\n\\nend_time = time.time()  # End the timer\\nelapsed_time = end_time - start_time\\n\\nresults\\n\\nprint(f\"Tasks completed in {elapsed_time:.2f} seconds.\")\\'\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask_geopandas as dgpd\n",
    "import pandas as pd\n",
    "from dask import delayed, compute, visualize\n",
    "import geopandas as gpd\n",
    "from dask.diagnostics import ProgressBar\n",
    "from citywide_calculation import get_utm_crs\n",
    "from metrics_calculation import calculate_minimum_distance_to_roads_option_B\n",
    "from shapely.geometry import MultiLineString, LineString, Point\n",
    "from shapely.ops import polygonize, nearest_points\n",
    "#from shapely.geometry import Polygon, LineString, Point, MultiPolygon, MultiLineString, GeometryCollection\n",
    "from scipy.optimize import fminbound, minimize\n",
    "import shapely.wkt\n",
    "\n",
    "@delayed\n",
    "def get_epsg(city_name):\n",
    "    search_buffer = f'{SEARCH_BUFFER_PATH}/{city_name}/{city_name}_search_buffer.geoparquet'\n",
    "    extent = dgpd.read_parquet(search_buffer)\n",
    "    geometry = extent.geometry[0].compute()\n",
    "    epsg = get_utm_crs(geometry)\n",
    "    print(f'{city_name} EPSG: {epsg}')\n",
    "    return epsg\n",
    "\n",
    "\n",
    "def load_dataset(path, epsg=None):\n",
    "    dataset = dgpd.read_parquet(path, npartitions=4)\n",
    "    \n",
    "    # Only assign if the file has no CRS\n",
    "    if epsg:\n",
    "        if dataset.crs is None:\n",
    "            dataset = dataset.set_crs(\"EPSG:4326\")  # assume WGS84 if missing\n",
    "        dataset = dataset.to_crs(epsg)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "@delayed\n",
    "def block_metrics(city_name, YOUR_NAME, grid_size):\n",
    "    paths = {\n",
    "        'grid': f'{GRIDS_PATH}/{city_name}/{city_name}_{str(grid_size)}m_grid.geoparquet',\n",
    "        'buildings': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}.geoparquet',\n",
    "        'roads': f'{ROADS_PATH}/{city_name}/{city_name}_OSM_roads.geoparquet',\n",
    "        'intersections': f'{INTERSECTIONS_PATH}/{city_name}/{city_name}_OSM_intersections.geoparquet',\n",
    "        'blocks' : f'{BLOCKS_PATH}/{city_name}/{city_name}_blocks_{YOUR_NAME}.geoparquet'\n",
    "    }\n",
    "    # Get EPSG (you may still use a delayed get_epsg, but ensure you compute it if necessary)\n",
    "    epsg = get_epsg(city_name).compute()\n",
    "    \n",
    "    grid = load_dataset(paths['grid'], epsg=epsg)\n",
    "    blocks = load_dataset(paths['blocks'])\n",
    "\n",
    "    if 'geom' in grid.columns:\n",
    "        grid = grid.drop(columns='geom')\n",
    "    if 'geom' in blocks.columns:\n",
    "        blocks = blocks.drop(columns='geom')\n",
    "\n",
    "    # Ensure that the active geometry is set correctly\n",
    "    grid = grid.set_geometry(\"geometry\")\n",
    "    blocks = blocks.set_geometry(\"geometry\")\n",
    "\n",
    "    grid = grid.persist()\n",
    "    blocks = blocks.persist()\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    grid_sample = grid.head()\n",
    "    blocks_sample = blocks.head()\n",
    "\n",
    "    ax = grid_sample.plot(edgecolor='black', facecolor='none', figsize=(8, 8))\n",
    "    blocks_sample.plot(ax=ax, color='red', alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Perform the spatial join with dask-geopandas GeoDataFrames\n",
    "    blocks_grid_joined = dgpd.sjoin(blocks, grid, predicate='intersects')\n",
    "    average_block_maxradius = blocks_grid_joined.groupby('index_right')['max_radius'].mean().astype(float)\n",
    "    \n",
    "    grid['m7'] = grid.index.map(average_block_maxradius).fillna(0).astype(float)\n",
    "    \n",
    "    path = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{str(grid_size)}m_grid_{YOUR_NAME}.geoparquet'\n",
    "    \n",
    "    # Remove problematic column if present\n",
    "    if 'geom' in grid.columns:\n",
    "        grid = grid.drop(columns='geom')\n",
    "    \n",
    "    grid.to_parquet(path)\n",
    "    return path\n",
    "\n",
    "'''\n",
    "import time\n",
    "\n",
    "start_time = time.time()  # Start the timer\n",
    "\n",
    "#cities = ['Nairobi','Belo_Horizonte']\n",
    "cities = [\"Belo Horizonte\", \"Campinas\", \"Bogota\", \"Nairobi\", \"Bamako\", \"Lagos\", \"Accra\", \"Abidjan\", \"Cape Town\", \"Luanda\"] #\"Maputo\",\"Mogadishu\", \n",
    "cities = [city.replace(' ', '_') for city in cities]\n",
    "\n",
    "tasks = [block_metrics(city,YOUR_NAME,grid_size) for city in cities]\n",
    "results = compute(*tasks)\n",
    "\n",
    "end_time = time.time()  # End the timer\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "results\n",
    "\n",
    "print(f\"Tasks completed in {elapsed_time:.2f} seconds.\")'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nimport time\\nfrom dask import compute\\n\\nstart_time = time.time()  # Start the timer\\n\\ncities = [\\'Nairobi\\', \\'Belo_Horizonte\\']\\ncities = [city.replace(\\' \\', \\'_\\') for city in cities]\\n\\ntasks = [compute_m6_m7_m8(city, YOUR_NAME, grid_size) for city in cities]\\nresults = compute(*tasks)  \\n\\nend_time = time.time()  # End the timer\\nelapsed_time = end_time - start_time\\n\\nprint(f\"Tasks completed in {elapsed_time:.2f} seconds.\")\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from shapely.geometry import Polygon\n",
    "\n",
    "def get_internal_buffer_with_target_area(row, target_area, tolerance=1e-6, max_iter=100):\n",
    "    \"\"\"\n",
    "    Iteratively finds an internal buffer that results in the target area.\n",
    "    \n",
    "    Parameters:\n",
    "    - row: GeoDataFrame row containing a 'geometry' column.\n",
    "    - target_area: Desired area for the internal buffer.\n",
    "    - tolerance: Error tolerance for area difference.\n",
    "    - max_iter: Maximum iterations to refine buffer.\n",
    "\n",
    "    Returns:\n",
    "    - Buffered geometry (Polygon) or empty geometry if not possible.\n",
    "    \"\"\"\n",
    "    geom = row.geometry\n",
    "\n",
    "    if geom.is_empty or geom.area <= target_area:\n",
    "        return Polygon()  # Return empty if no valid buffer can be made\n",
    "\n",
    "    # Initial guess for buffer distance (negative means internal buffer)\n",
    "    buffer_dist = -0.1 * (geom.area ** 0.5)  # Start with a fraction of block size\n",
    "    iteration = 0\n",
    "\n",
    "    while iteration < max_iter:\n",
    "        buffered_geom = geom.buffer(buffer_dist)\n",
    "\n",
    "        if buffered_geom.is_empty:\n",
    "            return Polygon()  # If buffering fails, return empty\n",
    "\n",
    "        new_area = buffered_geom.area\n",
    "        area_diff = abs(new_area - target_area)\n",
    "\n",
    "        if area_diff < tolerance:\n",
    "            return buffered_geom  # Found a good enough buffer\n",
    "\n",
    "        # Adjust buffer distance using a binary search-like approach\n",
    "        if new_area > target_area:\n",
    "            buffer_dist *= 1.1  # Increase buffer distance\n",
    "        else:\n",
    "            buffer_dist *= 0.9  # Decrease buffer distance\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    return buffered_geom  # Return best-found buffer\n",
    "\n",
    "\n",
    "def compute_building_to_buffer_ratio(blocks_df, buildings_df, buffer_col):\n",
    "    \"\"\"Helper function to compute building area ratio within a buffer.\"\"\"\n",
    "    # Ensure that the selected buffer column is recognized as geometry\n",
    "    blocks_df = blocks_df.set_geometry(buffer_col)\n",
    "\n",
    "    # Clip buildings **fully contained** in each buffer\n",
    "    buildings_within = dgpd.sjoin(buildings_df, blocks_df[['block_id', buffer_col]], predicate='within')\n",
    "\n",
    "    # Compute total building area within each block's buffer\n",
    "    building_area_within = buildings_within.groupby('block_id')['geometry'].area.sum()\n",
    "\n",
    "    # Get the buffer area\n",
    "    buffer_area = blocks_df.set_index('block_id')[buffer_col].area\n",
    "\n",
    "    return (building_area_within / buffer_area).fillna(0)\n",
    "\n",
    "\n",
    "def apply_internal_buffer(row, epsilon):\n",
    "    block_area = row.geometry.area\n",
    "    \n",
    "    # Epsilon buffer: very small internal buffer\n",
    "    epsilon_target_area = block_area * (1.0 - epsilon)\n",
    "    epsilon_buffer = get_internal_buffer_with_target_area(row, epsilon_target_area)\n",
    "\n",
    "    # 50%-width buffer\n",
    "    half_width_buffer = row['max_radius'] * 0.5\n",
    "    width_buffer = row.geometry.buffer(-half_width_buffer)\n",
    "\n",
    "    return epsilon_buffer, width_buffer\n",
    "\n",
    "@delayed\n",
    "def compute_m6_m7_m8(city_name, YOUR_NAME, grid_size, epsilon=0.01):\n",
    "    \"\"\"\n",
    "    Computes m6 (building orientation KL divergence),\n",
    "    m7 (average block width), and\n",
    "    m8 (building density within internal buffers) in a single function.\n",
    "    \n",
    "    Ensures shared datasets are only loaded once and operations are correctly ordered.\n",
    "    \"\"\"\n",
    "\n",
    "    paths = {\n",
    "        'grid': f'{GRIDS_PATH}/{city_name}/{city_name}_{str(grid_size)}m_grid.geoparquet',\n",
    "        'blocks': f'{BLOCKS_PATH}/{city_name}/{city_name}_blocks_{YOUR_NAME}.geoparquet',\n",
    "        'buildings_with_distances': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}_with_distances.geoparquet',\n",
    "    }\n",
    "\n",
    "    # Step 1: Load common datasets\n",
    "    epsg = get_epsg(city_name).compute()\n",
    "    grid = load_dataset(paths['grid'], epsg=epsg)\n",
    "    blocks = load_dataset(paths['blocks'], epsg=epsg)\n",
    "    buildings = load_dataset(paths['buildings_with_distances'], epsg=epsg)\n",
    "\n",
    "    if 'geom' in grid.columns:\n",
    "        grid = grid.drop(columns=['geom'])\n",
    "\n",
    "    blocks['block_id'] = blocks.index\n",
    "\n",
    "    buffer_results = blocks.apply(apply_internal_buffer, epsilon=0.001, axis=1)\n",
    "    blocks['epsilon_buffer'] = buffer_results.apply(lambda x: x[0])\n",
    "    blocks['width_buffer'] = buffer_results.apply(lambda x: x[1])\n",
    "\n",
    "    # Step 3: Compute M6 (Building Orientation KL Divergence)\n",
    "    meta = buildings._meta.copy()\n",
    "    meta['azimuth'] = 'f8'\n",
    "    buildings = buildings.map_partitions(compute_azimuth_partition, meta=meta)\n",
    "\n",
    "    buildings_blocks = dgpd.sjoin(buildings.compute(), blocks.compute(), predicate='intersects')\n",
    "    buildings_blocks = buildings_blocks[['block_id', 'geometry', 'azimuth']]\n",
    "\n",
    "    kl_df = compute_block_kl_metrics(buildings_blocks)\n",
    "    block_grid_overlap = compute_block_grid_weights(blocks, grid)\n",
    "\n",
    "    # Compute `block_grid_overlap` once BEFORE using it in `m6`, `m7`, and `m8`\n",
    "    block_grid_overlap = block_grid_overlap.compute()\n",
    "\n",
    "    m6_grid = aggregate_m6(kl_df.compute(), block_grid_overlap)\n",
    "\n",
    "    # Step 4: Compute M7 (Block Width)\n",
    "    block_grid_overlap['weighted_max_radius'] = (\n",
    "        block_grid_overlap['max_radius'] * block_grid_overlap['area_weight']\n",
    "    )\n",
    "\n",
    "    grid_m7 = block_grid_overlap.groupby('grid_id').agg(\n",
    "        total_weighted_max_radius=('weighted_max_radius', 'sum'),\n",
    "        total_weight=('area_weight', 'sum')\n",
    "    )\n",
    "\n",
    "    grid_m7['m7'] = grid_m7['total_weighted_max_radius'] / grid_m7['total_weight']\n",
    "\n",
    "    # Compute A and B\n",
    "    blocks['A'] = compute_building_to_buffer_ratio(blocks, buildings, 'epsilon_buffer')\n",
    "    blocks['B'] = compute_building_to_buffer_ratio(blocks, buildings, 'width_buffer')\n",
    "\n",
    "    # Compute M8 as A / B\n",
    "    blocks['m8'] = (blocks['A'] / blocks['B']).fillna(0)\n",
    "\n",
    "    # Assign M8 to Grid Cells (Weighted by Block Contribution)\n",
    "    block_grid_overlap['weighted_m8'] = block_grid_overlap['area_weight'] * blocks.set_index('block_id')['m8']\n",
    "\n",
    "    grid_m8 = block_grid_overlap.groupby('grid_id').agg(\n",
    "        total_weighted_m8=('weighted_m8', 'sum'),\n",
    "        total_weight=('area_weight', 'sum')\n",
    "    )\n",
    "\n",
    "    grid_m8['m8'] = grid_m8['total_weighted_m8'] / grid_m8['total_weight']\n",
    "\n",
    "    #  Step 6: Merge M6, M7, and M8 into Grid\n",
    "    grid = grid.merge(m6_grid, left_index=True, right_index=True, how='left')\n",
    "    grid = grid.merge(grid_m7[['m7']], left_index=True, right_index=True, how='left')\n",
    "    grid = grid.merge(grid_m8[['m8']], left_index=True, right_index=True, how='left')\n",
    "\n",
    "    # Fill NaNs\n",
    "    grid['m6'] = grid['m6'].fillna(0)\n",
    "    grid['m7'] = grid['m7'].fillna(0)\n",
    "    grid['m8'] = grid['m8'].fillna(0)\n",
    "\n",
    "    #  Step 7: Save Output\n",
    "    grid = grid.compute() \n",
    "    path = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{str(grid_size)}m_grid_{YOUR_NAME}_metrics_6_7_8.geoparquet'\n",
    "    grid.to_parquet(path)\n",
    "\n",
    "    return path\n",
    "\n",
    "'''\n",
    "\n",
    "import time\n",
    "from dask import compute\n",
    "\n",
    "start_time = time.time()  # Start the timer\n",
    "\n",
    "cities = ['Nairobi', 'Belo_Horizonte']\n",
    "cities = [city.replace(' ', '_') for city in cities]\n",
    "\n",
    "tasks = [compute_m6_m7_m8(city, YOUR_NAME, grid_size) for city in cities]\n",
    "results = compute(*tasks)  \n",
    "\n",
    "end_time = time.time()  # End the timer\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Tasks completed in {elapsed_time:.2f} seconds.\")\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "blocks['block_id'] = blocks.index\n",
    "\n",
    "blocks['epsilon_buffer'] = blocks.geometry.buffer(-blocks.geometry.area * 0.001)\n",
    "blocks['width_buffer'] = blocks.geometry.buffer(-blocks['max_radius'] * 0.5)\n",
    "\n",
    "# Step 3: Compute M6 (Building Orientation KL Divergence)\n",
    "meta = buildings._meta.copy()\n",
    "meta['azimuth'] = 'f8'\n",
    "buildings = buildings.map_partitions(compute_azimuth_partition, meta=meta)\n",
    "\n",
    "buildings_blocks = dgpd.sjoin(buildings.compute(), blocks.compute(), predicate='intersects')\n",
    "buildings_blocks = buildings_blocks[['block_id', 'geometry', 'azimuth']]\n",
    "\n",
    "#kl_df = compute_block_kl_metrics(buildings_blocks)\n",
    "block_grid_overlap = compute_block_grid_weights(blocks, grid)\n",
    "\n",
    "# Compute `block_grid_overlap` once BEFORE using it in `m6`, `m7`, and `m8`\n",
    "block_grid_overlap = block_grid_overlap.compute()\n",
    "\n",
    "#m6_grid = aggregate_m6(kl_df.compute(), block_grid_overlap)\n",
    "\n",
    "# Step 4: Compute M7 (Block Width)\n",
    "block_grid_overlap['weighted_max_radius'] = (\n",
    "    block_grid_overlap['max_radius'] * block_grid_overlap['area_weight']\n",
    ")\n",
    "\n",
    "# Compute buffers\n",
    "blocks['epsilon_buffer'] = blocks.geometry.buffer(-blocks.geometry.area * 0.001)  # Internal buffer (1% of block area)\n",
    "blocks['width_buffer'] = blocks.geometry.buffer(-blocks['max_radius'] * 0.5)  # Internal buffer (50% of block width)\n",
    "\n",
    "# Ensure buffers are recognized as geometries\n",
    "epsilon_gdf = blocks[['block_id', 'epsilon_buffer']].copy()  # Copy to avoid modifying `blocks`\n",
    "epsilon_gdf = epsilon_gdf.set_geometry('epsilon_buffer')  # Set active geometry\n",
    "\n",
    "width_gdf = blocks[['block_id', 'width_buffer']].copy()  # Copy to avoid modifying `blocks`\n",
    "width_gdf = width_gdf.set_geometry('width_buffer')  # Set active geometry\n",
    "\n",
    "# Compute explicit buffer areas **before** performing joins\n",
    "blocks['epsilon_area'] = epsilon_gdf.geometry.area\n",
    "blocks['width_area'] = width_gdf.geometry.area\n",
    "\n",
    "# Perform spatial joins ONCE for each buffer\n",
    "buildings_within_epsilon = dgpd.sjoin(buildings, epsilon_gdf, predicate='within')\n",
    "buildings_within_width = dgpd.sjoin(buildings, width_gdf, predicate='within')\n",
    "\n",
    "# Compute area explicitly using map_partitions\n",
    "buildings_within_epsilon['building_area'] = buildings_within_epsilon.map_partitions(\n",
    "    lambda gdf: gdf.geometry.area, meta=('building_area', 'f8')\n",
    ")\n",
    "buildings_within_width['building_area'] = buildings_within_width.map_partitions(\n",
    "    lambda gdf: gdf.geometry.area, meta=('building_area', 'f8')\n",
    ")\n",
    "\n",
    "# Aggregate total building area within each buffer\n",
    "building_area_epsilon = buildings_within_epsilon.groupby('block_id')['building_area'].sum()\n",
    "building_area_width = buildings_within_width.groupby('block_id')['building_area'].sum()\n",
    "\n",
    "# Compute A and B using precomputed buffer areas\n",
    "blocks['A'] = (building_area_epsilon / blocks['epsilon_area']).fillna(0)\n",
    "blocks['B'] = (building_area_width / blocks['width_area']).fillna(0)\n",
    "\n",
    "# Compute M8 as A / B\n",
    "blocks['m8'] = (blocks['A'] / blocks['B']).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assign M8 to Grid Cells (Weighted by Block Contribution)\n",
    "# Compute `m8` column first before using it\n",
    "blocks = blocks.compute()  # Ensure blocks is fully computed before indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pandas/core/nanops.py:1016: RuntimeWarning: invalid value encountered in subtract\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    3.126200e+04\n",
       "mean              inf\n",
       "std               NaN\n",
       "min      0.000000e+00\n",
       "25%      0.000000e+00\n",
       "50%      8.969876e-01\n",
       "75%      1.679932e+00\n",
       "max               inf\n",
       "Name: m8, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks['m8'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8969876"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8.969876e-01\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Then perform the operation\n",
    "block_grid_overlap['weighted_m8'] = block_grid_overlap['area_weight'] * blocks.set_index('block_id')['m8']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'area'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Assign M8 to Grid Cells (Weighted by Block Contribution)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Compute `m8` column first before using it\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m blocks \u001b[38;5;241m=\u001b[39m \u001b[43mblocks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Ensure blocks is fully computed before indexing\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Then perform the operation\u001b[39;00m\n\u001b[1;32m      6\u001b[0m block_grid_overlap[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted_m8\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m block_grid_overlap[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marea_weight\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m blocks\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblock_id\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm8\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/dataframe/dask_expr/_collection.py:489\u001b[0m, in \u001b[0;36mFrameBase.compute\u001b[0;34m(self, fuse, concatenate, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mrepartition(npartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    488\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39moptimize(fuse\u001b[38;5;241m=\u001b[39mfuse)\n\u001b[0;32m--> 489\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDaskMethodsMixin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/base.py:374\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    351\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 374\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/dask/base.py:662\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    659\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 662\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m/opt/coiled/env/lib/python3.12/site-packages/pandas/core/generic.py:6299\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'area'"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Column(s) ['weighted_m8'] do not exist\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m grid_m8 \u001b[38;5;241m=\u001b[39m \u001b[43mblock_grid_overlap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgrid_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_weighted_m8\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweighted_m8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43marea_weight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m grid_m8[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm8\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m grid_m8[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_weighted_m8\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m grid_m8[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_weight\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#  Step 6: Merge M6, M7, and M8 into Grid\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#grid = grid.merge(m6_grid, left_index=True, right_index=True, how='left')\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#grid = grid.merge(grid_m7[['m7']], left_index=True, right_index=True, how='left')\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pandas/core/groupby/generic.py:1432\u001b[0m, in \u001b[0;36mDataFrameGroupBy.aggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1429\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m engine_kwargs\n\u001b[1;32m   1431\u001b[0m op \u001b[38;5;241m=\u001b[39m GroupByApply(\u001b[38;5;28mself\u001b[39m, func, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m-> 1432\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dict_like(func) \u001b[38;5;129;01mand\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1434\u001b[0m     \u001b[38;5;66;03m# GH #52849\u001b[39;00m\n\u001b[1;32m   1435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_index \u001b[38;5;129;01mand\u001b[39;00m is_list_like(func):\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pandas/core/apply.py:190\u001b[0m, in \u001b[0;36mApply.agg\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_dict_like(func):\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(func):\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# we require a list, but not a 'str'\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magg_list_like()\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pandas/core/apply.py:423\u001b[0m, in \u001b[0;36mApply.agg_dict_like\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21magg_dict_like\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m    416\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m    Compute aggregation in the case of a dict-like argument.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;124;03m    Result of aggregation.\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_or_apply_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43magg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pandas/core/apply.py:1608\u001b[0m, in \u001b[0;36mGroupByApply.agg_or_apply_dict_like\u001b[0;34m(self, op_name)\u001b[0m\n\u001b[1;32m   1603\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine\u001b[39m\u001b[38;5;124m\"\u001b[39m: engine, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: engine_kwargs})\n\u001b[1;32m   1605\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m com\u001b[38;5;241m.\u001b[39mtemp_setattr(\n\u001b[1;32m   1606\u001b[0m     obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_index\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, condition\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1607\u001b[0m ):\n\u001b[0;32m-> 1608\u001b[0m     result_index, result_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_dict_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1611\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results_dict_like(selected_obj, result_index, result_data)\n\u001b[1;32m   1612\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pandas/core/apply.py:462\u001b[0m, in \u001b[0;36mApply.compute_dict_like\u001b[0;34m(self, op_name, selected_obj, selection, kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m is_groupby \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(obj, (DataFrameGroupBy, SeriesGroupBy))\n\u001b[1;32m    461\u001b[0m func \u001b[38;5;241m=\u001b[39m cast(AggFuncTypeDict, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\n\u001b[0;32m--> 462\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_dictlike_arg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m is_non_unique_col \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    465\u001b[0m     selected_obj\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m selected_obj\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnunique() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(selected_obj\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    467\u001b[0m )\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m selected_obj\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;66;03m# key only used for output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/pandas/core/apply.py:663\u001b[0m, in \u001b[0;36mApply.normalize_dictlike_arg\u001b[0;34m(self, how, obj, func)\u001b[0m\n\u001b[1;32m    661\u001b[0m     cols \u001b[38;5;241m=\u001b[39m Index(\u001b[38;5;28mlist\u001b[39m(func\u001b[38;5;241m.\u001b[39mkeys()))\u001b[38;5;241m.\u001b[39mdifference(obj\u001b[38;5;241m.\u001b[39mcolumns, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    662\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cols) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 663\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m do not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    665\u001b[0m aggregator_types \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m    667\u001b[0m \u001b[38;5;66;03m# if we have a dict of any non-scalars\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;66;03m# eg. {'A' : ['mean']}, normalize all to\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;66;03m# be list-likes\u001b[39;00m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;66;03m# Cannot use func.values() because arg may be a Series\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Column(s) ['weighted_m8'] do not exist\""
     ]
    }
   ],
   "source": [
    "\n",
    "grid_m8 = block_grid_overlap.groupby('grid_id').agg(\n",
    "    total_weighted_m8=('weighted_m8', 'sum'),\n",
    "    total_weight=('area_weight', 'sum')\n",
    ")\n",
    "\n",
    "grid_m8['m8'] = grid_m8['total_weighted_m8'] / grid_m8['total_weight']\n",
    "\n",
    "#  Step 6: Merge M6, M7, and M8 into Grid\n",
    "#grid = grid.merge(m6_grid, left_index=True, right_index=True, how='left')\n",
    "#grid = grid.merge(grid_m7[['m7']], left_index=True, right_index=True, how='left')\n",
    "grid = grid.merge(grid_m8[['m8']], left_index=True, right_index=True, how='left')\n",
    "\n",
    "# Fill NaNs\n",
    "#grid['m6'] = grid['m6'].fillna(0)\n",
    "#grid['m7'] = grid['m7'].fillna(0)\n",
    "grid['m8'] = grid['m8'].fillna(0)\n",
    "\n",
    "#  Step 7: Save Output\n",
    "grid = grid.compute() \n",
    "#path = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{str(grid_size)}m_grid_{YOUR_NAME}_metrics_6_7_8.geoparquet'\n",
    "#grid.to_parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subdivisions2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
