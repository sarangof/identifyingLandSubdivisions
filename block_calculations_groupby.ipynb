{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_NAME = 'sara'\n",
    "\n",
    "AWS_PROFILE = 'cities'\n",
    "\n",
    "'''\n",
    "# List of cities to process\n",
    "cities = [\"Belo Horizonte\", \"Campinas\"]#, \"Bogota\", \"Nairobi\", \"Bamako\", \n",
    "        #\"Lagos\", \"Accra\", \"Abidjan\", \"Mogadishu\", \"Cape Town\", \n",
    "        #\"Maputo\", \"Luanda\"]\n",
    "\n",
    "test_cities = [\"Belo Horizonte\"]\n",
    "#cities = test_cities\n",
    "\n",
    "cities = [city.replace(' ', '_') for city in cities]\n",
    "\n",
    "search_buffer_files = fs.ls(SEARCH_BUFFER_PATH)\n",
    "\n",
    "cities \n",
    "\n",
    "number_of_cities = len(cities)\n",
    "\n",
    "print(f'City count: {number_of_cities}')\n",
    "'''\n",
    "grid_size = 200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = \"s3://wri-cities-sandbox/identifyingLandSubdivisions/data\"\n",
    "INPUT_PATH = f'{MAIN_PATH}/input'\n",
    "CITY_INFO_PATH = f'{INPUT_PATH}/city_info'\n",
    "EXTENTS_PATH = f'{CITY_INFO_PATH}/extents'\n",
    "BUILDINGS_PATH = f'{INPUT_PATH}/buildings'\n",
    "BLOCKS_PATH = f'{INPUT_PATH}/blocks'\n",
    "ROADS_PATH = f'{INPUT_PATH}/roads'\n",
    "INTERSECTIONS_PATH = f'{INPUT_PATH}/intersections'\n",
    "GRIDS_PATH = f'{INPUT_PATH}/city_info/grids'\n",
    "SEARCH_BUFFER_PATH = f'{INPUT_PATH}/city_info/search_buffers'\n",
    "OUTPUT_PATH = f'{MAIN_PATH}/output'\n",
    "OUTPUT_PATH_CSV = f'{OUTPUT_PATH}/csv'\n",
    "OUTPUT_PATH_RASTER = f'{OUTPUT_PATH}/raster'\n",
    "OUTPUT_PATH_PNG = f'{OUTPUT_PATH}/png'\n",
    "OUTPUT_PATH_RAW = f'{OUTPUT_PATH}/raw_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '0FNEF04NN0RR6EMB',\n",
       "  'HostId': 'XUJBIhW9tcP+HkKG7SdW0tfLjks/lXA2zZscUUKHrGh23hsNy/PG2xfG7RUhI9ltJfP/nR3UBmV4wpiVCnbUqsaZBIIiZi36f3NEZUOhdo4=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'XUJBIhW9tcP+HkKG7SdW0tfLjks/lXA2zZscUUKHrGh23hsNy/PG2xfG7RUhI9ltJfP/nR3UBmV4wpiVCnbUqsaZBIIiZi36f3NEZUOhdo4=',\n",
       "   'x-amz-request-id': '0FNEF04NN0RR6EMB',\n",
       "   'date': 'Thu, 27 Mar 2025 22:36:18 GMT',\n",
       "   'content-type': 'application/xml',\n",
       "   'transfer-encoding': 'chunked',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'Buckets': [{'Name': 'aft-sandbox-540362055257',\n",
       "   'CreationDate': datetime.datetime(2022, 9, 13, 15, 12, 20, tzinfo=tzutc())},\n",
       "  {'Name': 'amplify-citiesindicatorsapi-dev-10508-deployment',\n",
       "   'CreationDate': datetime.datetime(2023, 8, 30, 5, 5, 13, tzinfo=tzutc())},\n",
       "  {'Name': 'cities-dev-sandbox',\n",
       "   'CreationDate': datetime.datetime(2025, 2, 7, 23, 18, 12, tzinfo=tzutc())},\n",
       "  {'Name': 'cities-heat',\n",
       "   'CreationDate': datetime.datetime(2023, 6, 1, 13, 22, 1, tzinfo=tzutc())},\n",
       "  {'Name': 'era5-brazil',\n",
       "   'CreationDate': datetime.datetime(2025, 2, 15, 19, 51, 14, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-athena-us-west-2',\n",
       "   'CreationDate': datetime.datetime(2024, 1, 12, 18, 45, 11, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-climate-hazards',\n",
       "   'CreationDate': datetime.datetime(2024, 1, 3, 16, 57, 31, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-data-api',\n",
       "   'CreationDate': datetime.datetime(2024, 7, 16, 8, 53, 31, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-heat',\n",
       "   'CreationDate': datetime.datetime(2024, 3, 25, 15, 46, 55, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-indicators',\n",
       "   'CreationDate': datetime.datetime(2024, 5, 13, 15, 50, 58, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-sandbox',\n",
       "   'CreationDate': datetime.datetime(2024, 7, 27, 0, 51, 38, tzinfo=tzutc())}],\n",
       " 'Owner': {'DisplayName': 'aws-cities',\n",
       "  'ID': 'df12253943982d72f60594f06c2cacf9a1ee3a9e738c1649c9fb96e5127f1a5c'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check s3 connection using AWS_PROFILE=CitiesUserPermissionSet profile \n",
    "import boto3\n",
    "\n",
    "session = boto3.Session(profile_name=AWS_PROFILE)\n",
    "s3 = session.client('s3')\n",
    "\n",
    "# export CitiesUserPermissionSet profile to use in the next cells\n",
    "import os\n",
    "os.environ['AWS_PROFILE'] = AWS_PROFILE\n",
    "\n",
    "\n",
    "s3.list_buckets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-27 17:36:19,549][INFO    ][coiled] Fetching latest package priorities...\n",
      "[2025-03-27 17:36:19,551][INFO    ][coiled.package_sync] Resolving your local subdivisions2 Python environment...\n",
      "[2025-03-27 17:36:20,657][INFO    ][coiled.package_sync] Scanning 444 conda packages...\n",
      "[2025-03-27 17:36:20,665][INFO    ][coiled.package_sync] Scanning 259 python packages...\n",
      "[2025-03-27 17:36:21,800][INFO    ][coiled] Running pip check...\n",
      "[2025-03-27 17:36:23,374][INFO    ][coiled] Validating environment...\n",
      "[2025-03-27 17:36:25,382][INFO    ][coiled] Creating wheel for ~/Documents/Identifying Land Subdivisions/identifyingLandSubdivisions...\n",
      "[2025-03-27 17:36:25,632][WARNING ][coiled.package_sync] Package - libopenvino-intel-cpu-plugin, libopenvino-intel-cpu-plugin~=2025.0.0 has no install candidate for Python 3.12 linux-aarch64 on conda-forge\n",
      "[2025-03-27 17:36:25,634][INFO    ][coiled] Uploading coiled_local_identifyingLandSubdivisions...\n",
      "[2025-03-27 17:36:26,692][INFO    ][coiled] Requesting package sync build...\n",
      "[2025-03-27 17:36:27,511][INFO    ][coiled] Creating Cluster (name: ils-sara, https://cloud.coiled.io/clusters/812475?account=wri-cities-data ). This usually takes 1-2 minutes...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started a new Dask client on Coiled. Dashboard is available at https://cluster-obugl.dask.host/2TaYvPQ1zWzP4DwZ/status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-3353' coro=<Client._gather.<locals>.wait() done, defined at /Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/distributed/client.py:2394> exception=AllExit()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/distributed/client.py\", line 2403, in wait\n",
      "    raise AllExit()\n",
      "distributed.client.AllExit\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-3354' coro=<Client._gather.<locals>.wait() done, defined at /Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/distributed/client.py:2394> exception=AllExit()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarangof/miniconda3/envs/subdivisions2/lib/python3.12/site-packages/distributed/client.py\", line 2403, in wait\n",
      "    raise AllExit()\n",
      "distributed.client.AllExit\n"
     ]
    }
   ],
   "source": [
    "import coiled\n",
    "\n",
    "cluster = coiled.Cluster(\n",
    "    workspace=\"wri-cities-data\",\n",
    "    name=f'ils-{YOUR_NAME}',\n",
    "    region=\"us-west-2\",\n",
    "    arm=True,\n",
    "    worker_vm_types=\"r8g.xlarge\",\n",
    "    spot_policy=\"spot\",\n",
    "    n_workers=8,\n",
    "    package_sync_ignore=[\"pyspark\", \"pypandoc\"]\n",
    ")\n",
    "client = cluster.get_client()\n",
    "\n",
    "print(f\"Started a new Dask client on Coiled. Dashboard is available at {client.dashboard_link}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import s3fs\n",
    "import fsspec\n",
    "import traceback\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "fs = s3fs.S3FileSystem(anon=False)\n",
    "search_buffer_files = fs.ls(SEARCH_BUFFER_PATH)\n",
    "\n",
    "cities = [x.split('/')[-1] for x in search_buffer_files]\n",
    "len(cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstart_time = time.time()  \\n\\n#cities = [\\'Nairobi\\',\\'Belo_Horizonte\\']\\ncities = [\"Belo Horizonte\", \\'Nairobi\\'] #\"Campinas\", \"Bogota\", \"Nairobi\", \"Bamako\", \"Lagos\", \"Accra\", \"Abidjan\", \"Cape Town\", \"Luanda\"] #\"Maputo\",\"Mogadishu\", \\ncities = [city.replace(\\' \\', \\'_\\') for city in cities]\\n\\ntasks = [produce_blocks(city) for city in cities]\\nresults = compute(*tasks)\\n\\nend_time = time.time()  \\nelapsed_time = end_time - start_time\\n\\nprint(f\"Tasks completed in {elapsed_time:.2f} seconds.\")\\'\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "THIS IS PRE-PROCESSING\n",
    "'''\n",
    "\n",
    "import dask_geopandas as dgpd\n",
    "import pandas as pd\n",
    "from dask import delayed, compute, visualize\n",
    "import geopandas as gpd\n",
    "from dask.diagnostics import ProgressBar\n",
    "from citywide_calculation import get_utm_crs\n",
    "from metrics_calculation import calculate_minimum_distance_to_roads_option_B\n",
    "from shapely.geometry import MultiLineString, LineString, Point\n",
    "from shapely.ops import polygonize, nearest_points\n",
    "#from shapely.geometry import Polygon, LineString, Point, MultiPolygon, MultiLineString, GeometryCollection\n",
    "from scipy.optimize import fminbound, minimize\n",
    "from shapely.ops import unary_union, polygonize\n",
    "import geopandas as gpd\n",
    "from polylabel import polylabel\n",
    "from shapely.geometry import mapping\n",
    "\n",
    "\n",
    "@delayed\n",
    "def get_epsg(city_name):\n",
    "    search_buffer = f'{SEARCH_BUFFER_PATH}/{city_name}/{city_name}_search_buffer.geoparquet'\n",
    "    extent = dgpd.read_parquet(search_buffer)\n",
    "    geometry = extent.geometry[0].compute()\n",
    "    epsg = get_utm_crs(geometry)\n",
    "    print(f'{city_name} EPSG: {epsg}')\n",
    "    return epsg\n",
    "\n",
    "@delayed\n",
    "def load_dataset(path, epsg=None):\n",
    "    \"\"\"Load a single parquet dataset\"\"\"\n",
    "    dataset = dgpd.read_parquet(path, npartitions=2)\n",
    "    if epsg:\n",
    "        dataset = dataset.set_crs(\"EPSG:4326\", allow_override=True) \n",
    "        dataset = dataset.to_crs(epsg=epsg)\n",
    "    return dataset\n",
    "\n",
    "from shapely.geometry import mapping, Point\n",
    "from polylabel import polylabel\n",
    "\n",
    "def to_geojson_dict(geom):\n",
    "    \"\"\"\n",
    "    Convert a Shapely geometry to a GeoJSON-like dict with lists instead of tuples.\n",
    "    \"\"\"\n",
    "    geojson = mapping(geom)\n",
    "    def recursive_convert(obj):\n",
    "        if isinstance(obj, tuple):\n",
    "            return list(obj)\n",
    "        elif isinstance(obj, list):\n",
    "            return [recursive_convert(item) for item in obj]\n",
    "        elif isinstance(obj, dict):\n",
    "            return {k: recursive_convert(v) for k, v in obj.items()}\n",
    "        else:\n",
    "            return obj\n",
    "    return recursive_convert(geojson)\n",
    "\n",
    "def compute_largest_inscribed_circle(geom):\n",
    "    \"\"\"\n",
    "    Compute the largest inscribed circle for a given polygon or multipolygon.\n",
    "\n",
    "    Parameters:\n",
    "      geom (shapely.geometry): A Polygon or MultiPolygon.\n",
    "    \n",
    "    Returns:\n",
    "      tuple: (optimal_point, max_radius) where optimal_point is a shapely Point and max_radius is a float.\n",
    "    \"\"\"\n",
    "    if geom is None or geom.is_empty:\n",
    "        return None, None\n",
    "\n",
    "    if geom.geom_type == 'Polygon':\n",
    "        geojson_poly = to_geojson_dict(geom)\n",
    "        # Pass in the coordinates list instead of the entire dict.\n",
    "        optimal_coords = polylabel(geojson_poly[\"coordinates\"])\n",
    "        optimal = Point(optimal_coords)\n",
    "        radius = geom.boundary.distance(optimal)\n",
    "        return optimal, radius\n",
    "\n",
    "    elif geom.geom_type == 'MultiPolygon':\n",
    "        best_point = None\n",
    "        best_radius = 0\n",
    "        for poly in geom.geoms:\n",
    "            geojson_poly = to_geojson_dict(poly)\n",
    "            optimal_coords = polylabel(geojson_poly[\"coordinates\"])\n",
    "            candidate = Point(optimal_coords)\n",
    "            radius = poly.boundary.distance(candidate)\n",
    "            if radius > best_radius:\n",
    "                best_radius = radius\n",
    "                best_point = candidate\n",
    "        return best_point, best_radius\n",
    "\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "def add_inscribed_circle_info(blocks_gdf):\n",
    "    \"\"\"\n",
    "    Adds two new columns to a blocks GeoDataFrame: 'optimal_point' and 'max_radius'\n",
    "    which indicate the center and radius of the largest inscribed circle for each block.\n",
    "    Converts the optimal_point geometries to WKT strings for Parquet compatibility.\n",
    "    \n",
    "    Parameters:\n",
    "      blocks_gdf (GeoDataFrame): A GeoDataFrame with block polygons.\n",
    "      \n",
    "    Returns:\n",
    "      GeoDataFrame: The input GeoDataFrame with two new columns.\n",
    "    \"\"\"\n",
    "    # Apply the computation for each geometry\n",
    "    results = blocks_gdf.geometry.apply(lambda geom: compute_largest_inscribed_circle(geom))\n",
    "    \n",
    "    # Unpack the tuple results into two new columns\n",
    "    blocks_gdf[\"optimal_point\"] = results.apply(lambda x: x[0])\n",
    "    blocks_gdf[\"max_radius\"] = results.apply(lambda x: x[1])\n",
    "    \n",
    "    # Convert the 'optimal_point' column from Shapely objects to WKT strings\n",
    "    blocks_gdf[\"optimal_point\"] = blocks_gdf[\"optimal_point\"].apply(\n",
    "        lambda geom: geom.wkt if geom is not None else None\n",
    "    )\n",
    "    \n",
    "    return blocks_gdf\n",
    "\n",
    "\n",
    "def get_blocks(roads):\n",
    "    \"\"\"\n",
    "    Create urban blocks from a grid and road network.\n",
    "\n",
    "    Parameters:\n",
    "      grid (GeoDataFrame): A GeoDataFrame of grid polygons defining the city extent.\n",
    "      roads (GeoDataFrame): A GeoDataFrame of road line geometries.\n",
    "    \n",
    "    Returns:\n",
    "      GeoDataFrame: A GeoDataFrame of block polygons.\n",
    "    \"\"\"\n",
    "    # Merge all road geometries into a single geometry\n",
    "    roads_union = unary_union(roads.geometry)\n",
    "    \n",
    "    # Polygonize the road network to generate blocks.\n",
    "    # The polygonize function returns an iterator of Polygons.\n",
    "    blocks_polygons = list(polygonize(roads_union))\n",
    "    \n",
    "    # Create a GeoDataFrame for blocks\n",
    "    blocks_gdf = gpd.GeoDataFrame(geometry=blocks_polygons, crs=roads.crs)\n",
    "    \n",
    "    # Remove any empty geometries resulting from the intersection.\n",
    "    blocks_gdf = blocks_gdf[~blocks_gdf.is_empty]\n",
    "    \n",
    "    return blocks_gdf\n",
    "\n",
    "@delayed\n",
    "def produce_blocks(city_name):\n",
    "    # Construct file paths for the city\n",
    "    paths = {\n",
    "        'grid': f'{GRIDS_PATH}/{city_name}/{city_name}_{str(grid_size)}m_grid.geoparquet',\n",
    "        'buildings': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}.geoparquet',\n",
    "        'roads': f'{ROADS_PATH}/{city_name}/{city_name}_OSM_roads.geoparquet',\n",
    "        'intersections': f'{INTERSECTIONS_PATH}/{city_name}/{city_name}_OSM_intersections.geoparquet'\n",
    "    }\n",
    "    \n",
    "    epsg = get_epsg(city_name)\n",
    "    \n",
    "    roads = load_dataset(paths['roads'], epsg=epsg)\n",
    "    \n",
    "    blocks = get_blocks(roads.compute())\n",
    "\n",
    "    # Now add the inscribed circle information.\n",
    "    blocks = add_inscribed_circle_info(blocks)\n",
    "    \n",
    "    # Define the output path for the blocks geoparquet\n",
    "    path_blocks = f'{BLOCKS_PATH}/{city_name}/{city_name}_blocks_{YOUR_NAME}.geoparquet'\n",
    "\n",
    "    blocks = blocks.set_crs(epsg.compute())\n",
    "\n",
    "    # Convert the geometry column to WKT before saving\n",
    "    #blocks[\"geometry\"] = blocks[\"geometry\"].apply(lambda geom: geom.wkt if geom is not None else None)\n",
    "    \n",
    "    # Save the blocks dataset. \n",
    "    blocks.to_parquet(path_blocks)\n",
    "    \n",
    "    # Optionally, return the output path or any summary info.\n",
    "    return blocks\n",
    "\n",
    "\n",
    "import time\n",
    "'''\n",
    "start_time = time.time()  \n",
    "\n",
    "#cities = ['Nairobi','Belo_Horizonte']\n",
    "cities = [\"Belo Horizonte\", 'Nairobi'] #\"Campinas\", \"Bogota\", \"Nairobi\", \"Bamako\", \"Lagos\", \"Accra\", \"Abidjan\", \"Cape Town\", \"Luanda\"] #\"Maputo\",\"Mogadishu\", \n",
    "cities = [city.replace(' ', '_') for city in cities]\n",
    "\n",
    "tasks = [produce_blocks(city) for city in cities]\n",
    "results = compute(*tasks)\n",
    "\n",
    "end_time = time.time()  \n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Tasks completed in {elapsed_time:.2f} seconds.\")'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t, sem, entropy\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "Auxiliary functions for metric 6\n",
    "'''\n",
    "\n",
    "def calculate_standardized_kl_azimuth(buildings_df, bin_width_degrees=5):\n",
    "    azimuths = buildings_df['azimuth'].to_numpy()\n",
    "    num_bins = int(90 / bin_width_degrees)\n",
    "    histogram, _ = np.histogram(azimuths, bins=num_bins, range=(0, 90))\n",
    "    P = histogram / histogram.sum() if histogram.sum() > 0 else np.ones(num_bins) / num_bins\n",
    "    Q = np.ones(num_bins) / num_bins\n",
    "    kl_divergence = entropy(P, Q)\n",
    "    max_kl_divergence = np.log(num_bins)\n",
    "    return kl_divergence / max_kl_divergence\n",
    "\n",
    "def compute_azimuth_partition(df):\n",
    "    def azimuth(geom):\n",
    "        if geom is None or geom.is_empty:\n",
    "            return np.nan\n",
    "        oriented = geom.minimum_rotated_rectangle\n",
    "        coords = list(oriented.exterior.coords)\n",
    "        edge = LineString([coords[0], coords[1]])\n",
    "        dx, dy = edge.xy[0][1] - edge.xy[0][0], edge.xy[1][1] - edge.xy[1][0]\n",
    "        angle = np.degrees(np.arctan2(dy, dx)) % 180\n",
    "        return angle % 90\n",
    "\n",
    "    df = df.copy()\n",
    "    df['azimuth'] = df['geometry'].map(azimuth)\n",
    "    return df\n",
    "\n",
    "@delayed\n",
    "def compute_block_kl_metrics(buildings_blocks):\n",
    "    grouped = buildings_blocks.groupby('block_id')\n",
    "    kl_data = grouped.apply(lambda g: pd.Series({\n",
    "        'standardized_kl': calculate_standardized_kl_azimuth(g),\n",
    "        'n_buildings': len(g),\n",
    "    })).reset_index()\n",
    "    return kl_data\n",
    "\n",
    "def compute_block_grid_weights(blocks, grid):\n",
    "    \"\"\"\n",
    "    Computes the proportional overlap of blocks in each grid cell.\n",
    "    Returns a Dask DataFrame containing block_id, index_right (grid ID), and area_weight.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    #blocks = blocks.rename_axis(index='block_id').reset_index()\n",
    "    grid = grid.rename_axis(index='grid_id').reset_index()\n",
    "\n",
    "    def overlay_partition(blocks_df, grid_df):\n",
    "        \"\"\"Computes intersection between blocks and grid.\"\"\"\n",
    "        return gpd.overlay(blocks_df, grid_df, how='intersection')\n",
    "\n",
    "    #meta = blocks._meta.merge(grid._meta, how=\"outer\")\n",
    "\n",
    "    block_grid_overlap = blocks.map_partitions(overlay_partition, grid)#, meta=meta\n",
    "\n",
    "\n",
    "    # Step 2: Compute area for each block-grid overlap\n",
    "    block_grid_overlap = block_grid_overlap.assign(\n",
    "        overlap_area=block_grid_overlap.map_partitions(lambda df: df.geometry.area, meta=('overlap_area', 'f8'))\n",
    "    )\n",
    "\n",
    "    # Step 3: Compute the total area of each grid cell\n",
    "    grid_areas = grid.assign(grid_area=grid.map_partitions(lambda df: df.geometry.area, meta=('grid_area', 'f8')))\n",
    "\n",
    "\n",
    "    # Step 4: Merge grid cell areas into block-grid overlap\n",
    "    block_grid_overlap = block_grid_overlap.merge(grid_areas[['grid_id','grid_area']], left_on='grid_id', right_on='grid_id', how='left')\n",
    "\n",
    "    # Step 5: Compute area weight as the ratio of overlap to grid cell area\n",
    "    block_grid_overlap = block_grid_overlap.assign(\n",
    "        area_weight=block_grid_overlap['overlap_area'] / block_grid_overlap['grid_area']\n",
    "    )\n",
    "    block_grid_overlap = block_grid_overlap.map_partitions(\n",
    "        lambda df: df.assign(\n",
    "            area_weight=df['area_weight'] / df.groupby(df['grid_id'])['area_weight'].transform('sum')\n",
    "        ),\n",
    "        meta=block_grid_overlap._meta  # Preserve original structure\n",
    "    )\n",
    "\n",
    "    return block_grid_overlap[['block_id', 'optimal_point', 'max_radius', 'grid_id', 'geometry', 'overlap_area', 'grid_area', 'area_weight']]\n",
    "\n",
    "\n",
    "def aggregate_m6(kl_df, overlap_df):\n",
    "    df = overlap_df.merge(kl_df, on='block_id', how='left')\n",
    "    df = df.dropna(subset=['standardized_kl'])\n",
    "\n",
    "    # Compute weights\n",
    "    df['weight'] = df['area_weight'] * df['n_buildings']\n",
    "    df['weighted_kl'] = df['standardized_kl'] * df['weight']\n",
    "\n",
    "    # Aggregate directly at the GRID level\n",
    "    grid_aggregated = df.groupby('grid_id').agg(\n",
    "        total_weighted_kl=('weighted_kl', 'sum'),\n",
    "        total_weight=('weight', 'sum')\n",
    "    )\n",
    "\n",
    "    # Compute final KL divergence for each grid cell\n",
    "    grid_aggregated['m6'] = grid_aggregated['total_weighted_kl'] / grid_aggregated['total_weight']\n",
    "\n",
    "    return grid_aggregated[['m6']]\n",
    "\n",
    "\n",
    "\n",
    "def building_orientation_metrics(city_name):\n",
    "    paths = {\n",
    "        'grid': f'{GRIDS_PATH}/{city_name}/{city_name}_{str(grid_size)}m_grid.geoparquet',\n",
    "        'blocks': f'{BLOCKS_PATH}/{city_name}/{city_name}_blocks_{YOUR_NAME}.geoparquet',\n",
    "        'buildings_with_distances': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}_with_distances.geoparquet',\n",
    "    }\n",
    "\n",
    "    epsg = get_epsg(city_name).compute()\n",
    "    grid = load_dataset(paths['grid'], epsg=epsg)\n",
    "    blocks = load_dataset(paths['blocks'], epsg=epsg)\n",
    "    buildings = load_dataset(paths['buildings_with_distances'], epsg=epsg)\n",
    "\n",
    "    if 'geom' in grid.columns:\n",
    "        grid = grid.drop(columns=['geom'])\n",
    "\n",
    "    blocks['block_id'] = blocks.index\n",
    "\n",
    "    meta = buildings._meta.copy()\n",
    "    meta['azimuth'] = 'f8'\n",
    "    buildings = buildings.map_partitions(compute_azimuth_partition, meta=meta)\n",
    "\n",
    "    # Fix `sjoin` issues by computing before\n",
    "    buildings_blocks = dgpd.sjoin(buildings.compute(), blocks.compute(), predicate='intersects')\n",
    "    buildings_blocks = buildings_blocks[['block_id', 'geometry', 'azimuth']]\n",
    "\n",
    "    kl_df = compute_block_kl_metrics(buildings_blocks)\n",
    "\n",
    "    # Keep `block_grid_overlap` lazy\n",
    "    block_grid_overlap = compute_block_grid_weights(blocks, grid)\n",
    "\n",
    "    # Aggregate `m6`\n",
    "    m6_grid = aggregate_m6(kl_df.compute(), block_grid_overlap.compute())\n",
    "    grid = grid.merge(m6_grid, left_index=True, right_index=True, how='left')\n",
    "    grid['m6'] = grid['m6'].fillna(0)\n",
    "\n",
    "\n",
    "    path = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{str(grid_size)}m_grid_{YOUR_NAME}_metric_6.geoparquet'\n",
    "    grid.to_parquet(path)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport time\\n\\nstart_time = time.time()  # Start the timer\\n\\n#cities = [\\'Nairobi\\',\\'Belo_Horizonte\\']\\ncities = [\"Belo Horizonte\", \"Campinas\", \"Bogota\", \"Nairobi\", \"Bamako\", \"Lagos\", \"Accra\", \"Abidjan\", \"Cape Town\", \"Luanda\"] #\"Maputo\",\"Mogadishu\", \\ncities = [city.replace(\\' \\', \\'_\\') for city in cities]\\n\\ntasks = [block_metrics(city,YOUR_NAME,grid_size) for city in cities]\\nresults = compute(*tasks)\\n\\nend_time = time.time()  # End the timer\\nelapsed_time = end_time - start_time\\n\\nresults\\n\\nprint(f\"Tasks completed in {elapsed_time:.2f} seconds.\")\\'\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask_geopandas as dgpd\n",
    "import pandas as pd\n",
    "from dask import delayed, compute, visualize\n",
    "import geopandas as gpd\n",
    "from dask.diagnostics import ProgressBar\n",
    "from citywide_calculation import get_utm_crs\n",
    "from metrics_calculation import calculate_minimum_distance_to_roads_option_B\n",
    "from shapely.geometry import MultiLineString, LineString, Point\n",
    "from shapely.ops import polygonize, nearest_points\n",
    "#from shapely.geometry import Polygon, LineString, Point, MultiPolygon, MultiLineString, GeometryCollection\n",
    "from scipy.optimize import fminbound, minimize\n",
    "import shapely.wkt\n",
    "\n",
    "@delayed\n",
    "def get_epsg(city_name):\n",
    "    search_buffer = f'{SEARCH_BUFFER_PATH}/{city_name}/{city_name}_search_buffer.geoparquet'\n",
    "    extent = dgpd.read_parquet(search_buffer)\n",
    "    geometry = extent.geometry[0].compute()\n",
    "    epsg = get_utm_crs(geometry)\n",
    "    print(f'{city_name} EPSG: {epsg}')\n",
    "    return epsg\n",
    "\n",
    "\n",
    "def load_dataset(path, epsg=None):\n",
    "    dataset = dgpd.read_parquet(path, npartitions=4)\n",
    "    \n",
    "    # Only assign if the file has no CRS\n",
    "    if epsg:\n",
    "        if dataset.crs is None:\n",
    "            dataset = dataset.set_crs(\"EPSG:4326\")  # assume WGS84 if missing\n",
    "        dataset = dataset.to_crs(epsg)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "@delayed\n",
    "def block_metrics(city_name, YOUR_NAME, grid_size):\n",
    "    paths = {\n",
    "        'grid': f'{GRIDS_PATH}/{city_name}/{city_name}_{str(grid_size)}m_grid.geoparquet',\n",
    "        'buildings': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}.geoparquet',\n",
    "        'roads': f'{ROADS_PATH}/{city_name}/{city_name}_OSM_roads.geoparquet',\n",
    "        'intersections': f'{INTERSECTIONS_PATH}/{city_name}/{city_name}_OSM_intersections.geoparquet',\n",
    "        'blocks' : f'{BLOCKS_PATH}/{city_name}/{city_name}_blocks_{YOUR_NAME}.geoparquet'\n",
    "    }\n",
    "    # Get EPSG (you may still use a delayed get_epsg, but ensure you compute it if necessary)\n",
    "    epsg = get_epsg(city_name).compute()\n",
    "    \n",
    "    grid = load_dataset(paths['grid'], epsg=epsg)\n",
    "    blocks = load_dataset(paths['blocks'])\n",
    "\n",
    "    if 'geom' in grid.columns:\n",
    "        grid = grid.drop(columns='geom')\n",
    "    if 'geom' in blocks.columns:\n",
    "        blocks = blocks.drop(columns='geom')\n",
    "\n",
    "    # Ensure that the active geometry is set correctly\n",
    "    grid = grid.set_geometry(\"geometry\")\n",
    "    blocks = blocks.set_geometry(\"geometry\")\n",
    "\n",
    "    grid = grid.persist()\n",
    "    blocks = blocks.persist()\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    grid_sample = grid.head()\n",
    "    blocks_sample = blocks.head()\n",
    "\n",
    "    ax = grid_sample.plot(edgecolor='black', facecolor='none', figsize=(8, 8))\n",
    "    blocks_sample.plot(ax=ax, color='red', alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Perform the spatial join with dask-geopandas GeoDataFrames\n",
    "    blocks_grid_joined = dgpd.sjoin(blocks, grid, predicate='intersects')\n",
    "    average_block_maxradius = blocks_grid_joined.groupby('index_right')['max_radius'].mean().astype(float)\n",
    "    \n",
    "    grid['m7'] = grid.index.map(average_block_maxradius).fillna(0).astype(float)\n",
    "    \n",
    "    path = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{str(grid_size)}m_grid_{YOUR_NAME}.geoparquet'\n",
    "    \n",
    "    # Remove problematic column if present\n",
    "    if 'geom' in grid.columns:\n",
    "        grid = grid.drop(columns='geom')\n",
    "    \n",
    "    grid.to_parquet(path)\n",
    "    return path\n",
    "\n",
    "'''\n",
    "import time\n",
    "\n",
    "start_time = time.time()  # Start the timer\n",
    "\n",
    "#cities = ['Nairobi','Belo_Horizonte']\n",
    "cities = [\"Belo Horizonte\", \"Campinas\", \"Bogota\", \"Nairobi\", \"Bamako\", \"Lagos\", \"Accra\", \"Abidjan\", \"Cape Town\", \"Luanda\"] #\"Maputo\",\"Mogadishu\", \n",
    "cities = [city.replace(' ', '_') for city in cities]\n",
    "\n",
    "tasks = [block_metrics(city,YOUR_NAME,grid_size) for city in cities]\n",
    "results = compute(*tasks)\n",
    "\n",
    "end_time = time.time()  # End the timer\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "results\n",
    "\n",
    "print(f\"Tasks completed in {elapsed_time:.2f} seconds.\")'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks completed in 192.72 seconds.\n"
     ]
    }
   ],
   "source": [
    "@delayed\n",
    "def compute_m6_m7(city_name, YOUR_NAME, grid_size):\n",
    "    \"\"\"\n",
    "    Computes m6 (building orientation KL divergence) and \n",
    "    m7 (average block width) in a single function, ensuring \n",
    "    shared datasets are only loaded once and operations are correctly ordered.\n",
    "    \"\"\"\n",
    "\n",
    "    paths = {\n",
    "        'grid': f'{GRIDS_PATH}/{city_name}/{city_name}_{str(grid_size)}m_grid.geoparquet',\n",
    "        'blocks': f'{BLOCKS_PATH}/{city_name}/{city_name}_blocks_{YOUR_NAME}.geoparquet',\n",
    "        'buildings_with_distances': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}_with_distances.geoparquet',\n",
    "    }\n",
    "\n",
    "    # Step 1: Load common datasets\n",
    "    epsg = get_epsg(city_name).compute()\n",
    "    grid = load_dataset(paths['grid'], epsg=epsg)\n",
    "    blocks = load_dataset(paths['blocks'], epsg=epsg)\n",
    "    buildings = load_dataset(paths['buildings_with_distances'], epsg=epsg)\n",
    "\n",
    "    if 'geom' in grid.columns:\n",
    "        grid = grid.drop(columns=['geom'])\n",
    "\n",
    "    blocks['block_id'] = blocks.index\n",
    "\n",
    "    # Step 2: Compute M6 (Building Orientation KL Divergence)\n",
    "    meta = buildings._meta.copy()\n",
    "    meta['azimuth'] = 'f8'\n",
    "    buildings = buildings.map_partitions(compute_azimuth_partition, meta=meta)\n",
    "\n",
    "    buildings_blocks = dgpd.sjoin(buildings.compute(), blocks.compute(), predicate='intersects')\n",
    "    buildings_blocks = buildings_blocks[['block_id', 'geometry', 'azimuth']]\n",
    "\n",
    "    kl_df = compute_block_kl_metrics(buildings_blocks)\n",
    "    block_grid_overlap = compute_block_grid_weights(blocks, grid)\n",
    "\n",
    "    # Force computation of block-grid overlap **before** passing it to both `m6` and `m7`\n",
    "    block_grid_overlap = block_grid_overlap.compute()  \n",
    "\n",
    "    m6_grid = aggregate_m6(kl_df.compute(), block_grid_overlap)\n",
    "\n",
    "    # Compute area-weighted average max_radius per grid cell\n",
    "    block_grid_overlap['weighted_max_radius'] = (\n",
    "        block_grid_overlap['max_radius'] * block_grid_overlap['area_weight']\n",
    "    )\n",
    "\n",
    "    grid_m7 = block_grid_overlap.groupby('grid_id').agg(\n",
    "        total_weighted_max_radius=('weighted_max_radius', 'sum'),\n",
    "        total_weight=('area_weight', 'sum')\n",
    "    )\n",
    "\n",
    "    grid_m7['m7'] = grid_m7['total_weighted_max_radius'] / grid_m7['total_weight']\n",
    "\n",
    "    # Step 4: Merge M6 and M7 into Grid\n",
    "    grid = grid.merge(m6_grid, left_index=True, right_index=True, how='left')\n",
    "    grid = grid.merge(grid_m7[['m7']], left_index=True, right_index=True, how='left')\n",
    "\n",
    "    # Fill NaNs\n",
    "    grid['m6'] = grid['m6'].fillna(0)\n",
    "    grid['m7'] = grid['m7'].fillna(0)\n",
    "\n",
    "    # Step 5: Save Output\n",
    "    grid = grid.compute() \n",
    "    path = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{str(grid_size)}m_grid_{YOUR_NAME}_metrics_6_7.geoparquet'\n",
    "    grid.to_parquet(path)\n",
    "\n",
    "    return path\n",
    "\n",
    "\n",
    "import time\n",
    "from dask import compute\n",
    "\n",
    "start_time = time.time()  # Start the timer\n",
    "\n",
    "cities = ['Nairobi', 'Belo_Horizonte']\n",
    "cities = [city.replace(' ', '_') for city in cities]\n",
    "\n",
    "tasks = [compute_m6_m7(city, YOUR_NAME, grid_size) for city in cities]\n",
    "results = compute(*tasks)  \n",
    "\n",
    "end_time = time.time()  # End the timer\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Tasks completed in {elapsed_time:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subdivisions2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
