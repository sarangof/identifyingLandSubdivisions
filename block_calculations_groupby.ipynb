{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_NAME = 'sara'\n",
    "\n",
    "AWS_PROFILE = 'cities'\n",
    "\n",
    "'''\n",
    "# List of cities to process\n",
    "cities = [\"Belo Horizonte\", \"Campinas\"]#, \"Bogota\", \"Nairobi\", \"Bamako\", \n",
    "        #\"Lagos\", \"Accra\", \"Abidjan\", \"Mogadishu\", \"Cape Town\", \n",
    "        #\"Maputo\", \"Luanda\"]\n",
    "\n",
    "test_cities = [\"Belo Horizonte\"]\n",
    "#cities = test_cities\n",
    "\n",
    "cities = [city.replace(' ', '_') for city in cities]\n",
    "\n",
    "search_buffer_files = fs.ls(SEARCH_BUFFER_PATH)\n",
    "\n",
    "cities \n",
    "\n",
    "number_of_cities = len(cities)\n",
    "\n",
    "print(f'City count: {number_of_cities}')\n",
    "'''\n",
    "grid_size = 200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = \"s3://wri-cities-sandbox/identifyingLandSubdivisions/data\"\n",
    "INPUT_PATH = f'{MAIN_PATH}/input'\n",
    "CITY_INFO_PATH = f'{INPUT_PATH}/city_info'\n",
    "EXTENTS_PATH = f'{CITY_INFO_PATH}/extents'\n",
    "BUILDINGS_PATH = f'{INPUT_PATH}/buildings'\n",
    "BLOCKS_PATH = f'{INPUT_PATH}/blocks'\n",
    "ROADS_PATH = f'{INPUT_PATH}/roads'\n",
    "INTERSECTIONS_PATH = f'{INPUT_PATH}/intersections'\n",
    "GRIDS_PATH = f'{INPUT_PATH}/city_info/grids'\n",
    "SEARCH_BUFFER_PATH = f'{INPUT_PATH}/city_info/search_buffers'\n",
    "OUTPUT_PATH = f'{MAIN_PATH}/output'\n",
    "OUTPUT_PATH_CSV = f'{OUTPUT_PATH}/csv'\n",
    "OUTPUT_PATH_RASTER = f'{OUTPUT_PATH}/raster'\n",
    "OUTPUT_PATH_PNG = f'{OUTPUT_PATH}/png'\n",
    "OUTPUT_PATH_RAW = f'{OUTPUT_PATH}/raw_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'WB1BCRT3WKRBNC18',\n",
       "  'HostId': 'elmxKwiBXoK5CqSlLGUQrJwO7ZxC0N8DZsz4E+bnX/PuEkPv8cZmz1x7IRn1cBZP2FpJv+FxOPs=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'elmxKwiBXoK5CqSlLGUQrJwO7ZxC0N8DZsz4E+bnX/PuEkPv8cZmz1x7IRn1cBZP2FpJv+FxOPs=',\n",
       "   'x-amz-request-id': 'WB1BCRT3WKRBNC18',\n",
       "   'date': 'Tue, 01 Apr 2025 21:45:34 GMT',\n",
       "   'content-type': 'application/xml',\n",
       "   'transfer-encoding': 'chunked',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'Buckets': [{'Name': 'aft-sandbox-540362055257',\n",
       "   'CreationDate': datetime.datetime(2022, 9, 13, 15, 12, 20, tzinfo=tzutc())},\n",
       "  {'Name': 'amplify-citiesindicatorsapi-dev-10508-deployment',\n",
       "   'CreationDate': datetime.datetime(2023, 8, 30, 5, 5, 13, tzinfo=tzutc())},\n",
       "  {'Name': 'cities-dev-sandbox',\n",
       "   'CreationDate': datetime.datetime(2025, 2, 7, 23, 18, 12, tzinfo=tzutc())},\n",
       "  {'Name': 'cities-heat',\n",
       "   'CreationDate': datetime.datetime(2023, 6, 1, 13, 22, 1, tzinfo=tzutc())},\n",
       "  {'Name': 'era5-brazil',\n",
       "   'CreationDate': datetime.datetime(2025, 2, 15, 19, 51, 14, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-athena-us-west-2',\n",
       "   'CreationDate': datetime.datetime(2024, 1, 12, 18, 45, 11, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-climate-hazards',\n",
       "   'CreationDate': datetime.datetime(2024, 1, 3, 16, 57, 31, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-data-api',\n",
       "   'CreationDate': datetime.datetime(2024, 7, 16, 8, 53, 31, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-heat',\n",
       "   'CreationDate': datetime.datetime(2024, 3, 25, 15, 46, 55, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-indicators',\n",
       "   'CreationDate': datetime.datetime(2024, 5, 13, 15, 50, 58, tzinfo=tzutc())},\n",
       "  {'Name': 'wri-cities-sandbox',\n",
       "   'CreationDate': datetime.datetime(2024, 7, 27, 0, 51, 38, tzinfo=tzutc())}],\n",
       " 'Owner': {'DisplayName': 'aws-cities',\n",
       "  'ID': 'df12253943982d72f60594f06c2cacf9a1ee3a9e738c1649c9fb96e5127f1a5c'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check s3 connection using AWS_PROFILE=CitiesUserPermissionSet profile \n",
    "import boto3\n",
    "\n",
    "session = boto3.Session(profile_name=AWS_PROFILE)\n",
    "s3 = session.client('s3')\n",
    "\n",
    "# export CitiesUserPermissionSet profile to use in the next cells\n",
    "import os\n",
    "os.environ['AWS_PROFILE'] = AWS_PROFILE\n",
    "\n",
    "\n",
    "s3.list_buckets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-01 16:45:35,867][INFO    ][coiled] Fetching latest package priorities...\n",
      "[2025-04-01 16:45:35,869][INFO    ][coiled.package_sync] Resolving your local subdivisions2 Python environment...\n",
      "[2025-04-01 16:45:36,815][INFO    ][coiled.package_sync] Scanning 444 conda packages...\n",
      "[2025-04-01 16:45:36,825][INFO    ][coiled.package_sync] Scanning 259 python packages...\n",
      "[2025-04-01 16:45:37,724][INFO    ][coiled] Running pip check...\n",
      "[2025-04-01 16:45:39,290][INFO    ][coiled] Validating environment...\n",
      "[2025-04-01 16:45:41,833][INFO    ][coiled] Creating wheel for ~/Documents/Identifying Land Subdivisions/identifyingLandSubdivisions...\n",
      "[2025-04-01 16:45:42,032][WARNING ][coiled.package_sync] Package - libopenvino-intel-cpu-plugin, libopenvino-intel-cpu-plugin~=2025.0.0 has no install candidate for Python 3.12 linux-aarch64 on conda-forge\n",
      "[2025-04-01 16:45:42,033][INFO    ][coiled] Uploading coiled_local_identifyingLandSubdivisions...\n",
      "[2025-04-01 16:45:42,974][INFO    ][coiled] Requesting package sync build...\n",
      "[2025-04-01 16:45:43,847][INFO    ][coiled] Creating Cluster (name: ils-sara, https://cloud.coiled.io/clusters/818761?account=wri-cities-data ). This usually takes 1-2 minutes...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started a new Dask client on Coiled. Dashboard is available at https://cluster-qrnhy.dask.host/C1NczZBVgZrXV8jZ/status\n"
     ]
    }
   ],
   "source": [
    "import coiled\n",
    "\n",
    "cluster = coiled.Cluster(\n",
    "    workspace=\"wri-cities-data\",\n",
    "    name=f'ils-{YOUR_NAME}',\n",
    "    region=\"us-west-2\",\n",
    "    arm=True,\n",
    "    worker_vm_types=\"r8g.xlarge\",\n",
    "    spot_policy=\"spot\",\n",
    "    n_workers=4,\n",
    "    package_sync_ignore=[\"pyspark\", \"pypandoc\"]\n",
    ")\n",
    "client = cluster.get_client()\n",
    "\n",
    "print(f\"Started a new Dask client on Coiled. Dashboard is available at {client.dashboard_link}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import s3fs\n",
    "import fsspec\n",
    "import traceback\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "fs = s3fs.S3FileSystem(anon=False)\n",
    "search_buffer_files = fs.ls(SEARCH_BUFFER_PATH)\n",
    "\n",
    "cities = [x.split('/')[-1] for x in search_buffer_files]\n",
    "len(cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_geopandas as dgpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "from dask import delayed\n",
    "from shapely.geometry import LineString\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "THIS IS PRE-PROCESSING\n",
    "'''\n",
    "\n",
    "@delayed\n",
    "def get_epsg(city_name):\n",
    "    search_buffer = f'{SEARCH_BUFFER_PATH}/{city_name}/{city_name}_search_buffer.geoparquet'\n",
    "    extent = dgpd.read_parquet(search_buffer)\n",
    "    geometry = extent.geometry[0].compute()\n",
    "    epsg = get_utm_crs(geometry)\n",
    "    print(f'{city_name} EPSG: {epsg}')\n",
    "    return epsg\n",
    "\n",
    "def load_dataset(path, epsg=None):\n",
    "    dataset = dgpd.read_parquet(path, npartitions=4)\n",
    "    \n",
    "    # Only assign if the file has no CRS\n",
    "    if epsg:\n",
    "        if dataset.crs is None:\n",
    "            dataset = dataset.set_crs(\"EPSG:4326\")  # assume WGS84 if missing\n",
    "        dataset = dataset.to_crs(epsg)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstart_time = time.time()  \\n\\n#cities = [\\'Nairobi\\',\\'Belo_Horizonte\\']\\ncities = [\"Belo Horizonte\", \\'Nairobi\\'] #\"Campinas\", \"Bogota\", \"Nairobi\", \"Bamako\", \"Lagos\", \"Accra\", \"Abidjan\", \"Cape Town\", \"Luanda\"] #\"Maputo\",\"Mogadishu\", \\ncities = [city.replace(\\' \\', \\'_\\') for city in cities]\\n\\ntasks = [produce_blocks(city) for city in cities]\\nresults = compute(*tasks)\\n\\nend_time = time.time()  \\nelapsed_time = end_time - start_time\\n\\nprint(f\"Tasks completed in {elapsed_time:.2f} seconds.\")\\'\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "PRE-PROCESSING: CREATE BLOCKS\n",
    "'''\n",
    "\n",
    "\n",
    "def to_geojson_dict(geom):\n",
    "    \"\"\"\n",
    "    Convert a Shapely geometry to a GeoJSON-like dict with lists instead of tuples.\n",
    "    \"\"\"\n",
    "    geojson = mapping(geom)\n",
    "    def recursive_convert(obj):\n",
    "        if isinstance(obj, tuple):\n",
    "            return list(obj)\n",
    "        elif isinstance(obj, list):\n",
    "            return [recursive_convert(item) for item in obj]\n",
    "        elif isinstance(obj, dict):\n",
    "            return {k: recursive_convert(v) for k, v in obj.items()}\n",
    "        else:\n",
    "            return obj\n",
    "    return recursive_convert(geojson)\n",
    "\n",
    "def compute_largest_inscribed_circle(geom):\n",
    "    \"\"\"\n",
    "    Compute the largest inscribed circle for a given polygon or multipolygon.\n",
    "\n",
    "    Parameters:\n",
    "      geom (shapely.geometry): A Polygon or MultiPolygon.\n",
    "    \n",
    "    Returns:\n",
    "      tuple: (optimal_point, max_radius) where optimal_point is a shapely Point and max_radius is a float.\n",
    "    \"\"\"\n",
    "    if geom is None or geom.is_empty:\n",
    "        return None, None\n",
    "\n",
    "    if geom.geom_type == 'Polygon':\n",
    "        geojson_poly = to_geojson_dict(geom)\n",
    "        # Pass in the coordinates list instead of the entire dict.\n",
    "        optimal_coords = polylabel(geojson_poly[\"coordinates\"])\n",
    "        optimal = Point(optimal_coords)\n",
    "        radius = geom.boundary.distance(optimal)\n",
    "        return optimal, radius\n",
    "\n",
    "    elif geom.geom_type == 'MultiPolygon':\n",
    "        best_point = None\n",
    "        best_radius = 0\n",
    "        for poly in geom.geoms:\n",
    "            geojson_poly = to_geojson_dict(poly)\n",
    "            optimal_coords = polylabel(geojson_poly[\"coordinates\"])\n",
    "            candidate = Point(optimal_coords)\n",
    "            radius = poly.boundary.distance(candidate)\n",
    "            if radius > best_radius:\n",
    "                best_radius = radius\n",
    "                best_point = candidate\n",
    "        return best_point, best_radius\n",
    "\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "def add_inscribed_circle_info(blocks_gdf):\n",
    "    \"\"\"\n",
    "    Adds two new columns to a blocks GeoDataFrame: 'optimal_point' and 'max_radius'\n",
    "    which indicate the center and radius of the largest inscribed circle for each block.\n",
    "    Converts the optimal_point geometries to WKT strings for Parquet compatibility.\n",
    "    \n",
    "    Parameters:\n",
    "      blocks_gdf (GeoDataFrame): A GeoDataFrame with block polygons.\n",
    "      \n",
    "    Returns:\n",
    "      GeoDataFrame: The input GeoDataFrame with two new columns.\n",
    "    \"\"\"\n",
    "    # Apply the computation for each geometry\n",
    "    results = blocks_gdf.geometry.apply(lambda geom: compute_largest_inscribed_circle(geom))\n",
    "    \n",
    "    # Unpack the tuple results into two new columns\n",
    "    blocks_gdf[\"optimal_point\"] = results.apply(lambda x: x[0])\n",
    "    blocks_gdf[\"max_radius\"] = results.apply(lambda x: x[1])\n",
    "    \n",
    "    # Convert the 'optimal_point' column from Shapely objects to WKT strings\n",
    "    blocks_gdf[\"optimal_point\"] = blocks_gdf[\"optimal_point\"].apply(\n",
    "        lambda geom: geom.wkt if geom is not None else None\n",
    "    )\n",
    "    \n",
    "    return blocks_gdf\n",
    "\n",
    "\n",
    "def get_blocks(roads):\n",
    "    \"\"\"\n",
    "    Create urban blocks from a grid and road network.\n",
    "\n",
    "    Parameters:\n",
    "      grid (GeoDataFrame): A GeoDataFrame of grid polygons defining the city extent.\n",
    "      roads (GeoDataFrame): A GeoDataFrame of road line geometries.\n",
    "    \n",
    "    Returns:\n",
    "      GeoDataFrame: A GeoDataFrame of block polygons.\n",
    "    \"\"\"\n",
    "    # Merge all road geometries into a single geometry\n",
    "    roads_union = unary_union(roads.geometry)\n",
    "    \n",
    "    # Polygonize the road network to generate blocks.\n",
    "    # The polygonize function returns an iterator of Polygons.\n",
    "    blocks_polygons = list(polygonize(roads_union))\n",
    "    \n",
    "    # Create a GeoDataFrame for blocks\n",
    "    blocks_gdf = gpd.GeoDataFrame(geometry=blocks_polygons, crs=roads.crs)\n",
    "    \n",
    "    # Remove any empty geometries resulting from the intersection.\n",
    "    blocks_gdf = blocks_gdf[~blocks_gdf.is_empty]\n",
    "    \n",
    "    return blocks_gdf\n",
    "\n",
    "@delayed\n",
    "def produce_blocks(city_name):\n",
    "    # Construct file paths for the city\n",
    "    paths = {\n",
    "        'grid': f'{GRIDS_PATH}/{city_name}/{city_name}_{str(grid_size)}m_grid.geoparquet',\n",
    "        'buildings': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}.geoparquet',\n",
    "        'roads': f'{ROADS_PATH}/{city_name}/{city_name}_OSM_roads.geoparquet',\n",
    "        'intersections': f'{INTERSECTIONS_PATH}/{city_name}/{city_name}_OSM_intersections.geoparquet'\n",
    "    }\n",
    "    \n",
    "    epsg = get_epsg(city_name)\n",
    "    \n",
    "    roads = load_dataset(paths['roads'], epsg=epsg)\n",
    "    \n",
    "    blocks = get_blocks(roads.compute())\n",
    "\n",
    "    # Now add the inscribed circle information.\n",
    "    blocks = add_inscribed_circle_info(blocks)\n",
    "    \n",
    "    # Define the output path for the blocks geoparquet\n",
    "    path_blocks = f'{BLOCKS_PATH}/{city_name}/{city_name}_blocks_{YOUR_NAME}.geoparquet'\n",
    "\n",
    "    blocks = blocks.set_crs(epsg.compute())\n",
    "\n",
    "    # Convert the geometry column to WKT before saving\n",
    "    #blocks[\"geometry\"] = blocks[\"geometry\"].apply(lambda geom: geom.wkt if geom is not None else None)\n",
    "    \n",
    "    # Save the blocks dataset. \n",
    "    blocks.to_parquet(path_blocks)\n",
    "    \n",
    "    # Optionally, return the output path or any summary info.\n",
    "    return blocks\n",
    "\n",
    "\n",
    "import time\n",
    "'''\n",
    "start_time = time.time()  \n",
    "\n",
    "#cities = ['Nairobi','Belo_Horizonte']\n",
    "cities = [\"Belo Horizonte\", 'Nairobi'] #\"Campinas\", \"Bogota\", \"Nairobi\", \"Bamako\", \"Lagos\", \"Accra\", \"Abidjan\", \"Cape Town\", \"Luanda\"] #\"Maputo\",\"Mogadishu\", \n",
    "cities = [city.replace(' ', '_') for city in cities]\n",
    "\n",
    "tasks = [produce_blocks(city) for city in cities]\n",
    "results = compute(*tasks)\n",
    "\n",
    "end_time = time.time()  \n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Tasks completed in {elapsed_time:.2f} seconds.\")'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport time\\nfrom dask import compute\\n\\nstart_time = time.time()  # Start the timer\\n\\ncities = [\\'Nairobi\\']#,\\'Belo_Horizonte\\',\\'Medellin\\',\\'Bogota\\',\\'Campinas\\',\\'Luanda\\',\\'Lagos\\',\\'Cape_Town\\'\\ncities = [city.replace(\\' \\', \\'_\\') for city in cities]\\n\\ntasks = [pre_process_block_calculations(city_name, epsilon=0.001) for city_name in cities]\\nresults = compute(*tasks)  \\n\\nend_time = time.time()  # End the timer\\nelapsed_time = end_time - start_time\\n\\nprint(f\"Tasks completed in {elapsed_time:.2f} seconds.\")\\'\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "PRE-PROCESSING: CALCULATE BUILDING AZIMUTH\n",
    "'''\n",
    "\n",
    "\n",
    "def compute_azimuth_partition(df):\n",
    "    def azimuth(geom):\n",
    "        if geom is None or geom.is_empty:\n",
    "            return np.nan\n",
    "        oriented = geom.minimum_rotated_rectangle\n",
    "        coords = list(oriented.exterior.coords)\n",
    "        edge = LineString([coords[0], coords[1]])\n",
    "        dx, dy = edge.xy[0][1] - edge.xy[0][0], edge.xy[1][1] - edge.xy[1][0]\n",
    "        angle = np.degrees(np.arctan2(dy, dx)) % 180\n",
    "        return angle % 90\n",
    "\n",
    "    df = df.copy()\n",
    "    df['azimuth'] = df['geometry'].map(azimuth)\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_azimuths(city_name):\n",
    "    paths = {\n",
    "        'grid': f'{GRIDS_PATH}/{city_name}/{city_name}_{str(grid_size)}m_grid.geoparquet',\n",
    "        'blocks': f'{BLOCKS_PATH}/{city_name}/{city_name}_blocks_{YOUR_NAME}.geoparquet',\n",
    "        'buildings_with_distances': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}_with_distances.geoparquet',\n",
    "        'buildings_with_distances_azimuths': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}_with_distances_and_azimuths.geoparquet',\n",
    "        'buildings_to_blocks':f'{BLOCKS_PATH}/{city_name}/{city_name}_buildings_to_blocks_{YOUR_NAME}.geoparquet'\n",
    "    }\n",
    "    epsg = get_epsg(city_name).compute()\n",
    "    buildings = load_dataset(paths['buildings_with_distances'], epsg=epsg)\n",
    "    meta = buildings._meta.copy()\n",
    "    meta['azimuth'] = 'f8'\n",
    "    buildings = buildings.map_partitions(compute_azimuth_partition, meta=meta)\n",
    "    path = paths['buildings_with_distances_azimuths']\n",
    "    buildings.to_parquet(path)\n",
    "\n",
    "    return path\n",
    "\n",
    "\n",
    "@delayed\n",
    "def pre_process_block_calculations(city_name, epsilon):\n",
    "    calculate_azimuths(city_name)\n",
    "\n",
    "'''\n",
    "\n",
    "import time\n",
    "from dask import compute\n",
    "\n",
    "start_time = time.time()  # Start the timer\n",
    "\n",
    "cities = ['Nairobi']#,'Belo_Horizonte','Medellin','Bogota','Campinas','Luanda','Lagos','Cape_Town'\n",
    "cities = [city.replace(' ', '_') for city in cities]\n",
    "\n",
    "tasks = [pre_process_block_calculations(city_name, epsilon=0.001) for city_name in cities]\n",
    "results = compute(*tasks)  \n",
    "\n",
    "end_time = time.time()  # End the timer\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Tasks completed in {elapsed_time:.2f} seconds.\")'\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks completed in 95.63 seconds.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculate_standardized_kl_azimuth(buildings_df, bin_width_degrees=5):\n",
    "    azimuths = buildings_df['azimuth'].to_numpy()\n",
    "    num_bins = int(90 / bin_width_degrees)\n",
    "    histogram, _ = np.histogram(azimuths, bins=num_bins, range=(0, 90))\n",
    "    P = histogram / histogram.sum() if histogram.sum() > 0 else np.ones(num_bins) / num_bins\n",
    "    Q = np.ones(num_bins) / num_bins\n",
    "    kl_divergence = entropy(P, Q)\n",
    "    max_kl_divergence = np.log(num_bins)\n",
    "    return kl_divergence / max_kl_divergence\n",
    "\n",
    "\n",
    "\n",
    "@delayed\n",
    "def compute_block_kl_metrics(buildings_blocks):\n",
    "    grouped = buildings_blocks.groupby('block_id')\n",
    "    kl_data = grouped.apply(lambda g: pd.Series({\n",
    "        'standardized_kl': calculate_standardized_kl_azimuth(g),\n",
    "        'n_buildings': len(g),\n",
    "    })).reset_index()\n",
    "    return kl_data\n",
    "\n",
    "def compute_block_grid_weights(blocks, grid):\n",
    "    \"\"\"\n",
    "    Computes the proportional overlap of blocks in each grid cell.\n",
    "    Returns a Dask DataFrame containing block_id, index_right (grid ID), and area_weight.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    #blocks = blocks.rename_axis(index='block_id').reset_index()\n",
    "    grid = grid.rename_axis(index='grid_id').reset_index()\n",
    "\n",
    "    def overlay_partition(blocks_df, grid_df):\n",
    "        \"\"\"Computes intersection between blocks and grid.\"\"\"\n",
    "        return gpd.overlay(blocks_df, grid_df, how='intersection')\n",
    "\n",
    "    #meta = blocks._meta.merge(grid._meta, how=\"outer\")\n",
    "\n",
    "    block_grid_overlap = blocks.map_partitions(overlay_partition, grid)#, meta=meta\n",
    "\n",
    "\n",
    "    # Step 2: Compute area for each block-grid overlap\n",
    "    block_grid_overlap = block_grid_overlap.assign(\n",
    "        overlap_area=block_grid_overlap.map_partitions(lambda df: df.geometry.area, meta=('overlap_area', 'f8'))\n",
    "    )\n",
    "\n",
    "    # Step 3: Compute the total area of each grid cell\n",
    "    grid_areas = grid.assign(grid_area=grid.map_partitions(lambda df: df.geometry.area, meta=('grid_area', 'f8')))\n",
    "\n",
    "\n",
    "    # Step 4: Merge grid cell areas into block-grid overlap\n",
    "    block_grid_overlap = block_grid_overlap.merge(grid_areas[['grid_id','grid_area']], left_on='grid_id', right_on='grid_id', how='left')\n",
    "\n",
    "    # Step 5: Compute area weight as the ratio of overlap to grid cell area\n",
    "    block_grid_overlap = block_grid_overlap.assign(\n",
    "        area_weight=block_grid_overlap['overlap_area'] / block_grid_overlap['grid_area']\n",
    "    )\n",
    "    block_grid_overlap = block_grid_overlap.map_partitions(\n",
    "        lambda df: df.assign(\n",
    "            area_weight=df['area_weight'] / df.groupby(df['grid_id'])['area_weight'].transform('sum')\n",
    "        ),\n",
    "        meta=block_grid_overlap._meta  # Preserve original structure\n",
    "    )\n",
    "\n",
    "    return block_grid_overlap[['block_id', 'optimal_point', 'max_radius', 'grid_id', 'geometry', 'overlap_area', 'grid_area', 'area_weight']]\n",
    "\n",
    "\n",
    "def aggregate_m6(kl_df, overlap_df):\n",
    "    df = overlap_df.merge(kl_df, on='block_id', how='left')\n",
    "    df = df.dropna(subset=['standardized_kl'])\n",
    "\n",
    "    # Compute weights\n",
    "    df['weight'] = df['area_weight'] * df['n_buildings']\n",
    "    df['weighted_kl'] = df['standardized_kl'] * df['weight']\n",
    "\n",
    "    # Aggregate directly at the GRID level\n",
    "    grid_aggregated = df.groupby('grid_id').agg(\n",
    "        total_weighted_kl=('weighted_kl', 'sum'),\n",
    "        total_weight=('weight', 'sum')\n",
    "    )\n",
    "\n",
    "    # Compute final KL divergence for each grid cell\n",
    "    grid_aggregated['m6'] = grid_aggregated['total_weighted_kl'] / grid_aggregated['total_weight']\n",
    "\n",
    "    return grid_aggregated[['m6']]\n",
    "\n",
    "\n",
    "\n",
    "def get_internal_buffer_with_target_area(geom, target_area, tolerance=1e-6, max_iter=100):\n",
    "    \"\"\"\n",
    "    Iteratively finds an internal buffer that results in the target area.\n",
    "\n",
    "    Parameters:\n",
    "    - geom: Shapely Polygon geometry.\n",
    "    - target_area: Desired area for the internal buffer.\n",
    "    - tolerance: Error tolerance for area difference.\n",
    "    - max_iter: Maximum iterations to refine buffer.\n",
    "\n",
    "    Returns:\n",
    "    - Buffered geometry (Polygon) or the original geometry if buffering is not possible.\n",
    "    \"\"\"\n",
    "    if geom.is_empty or geom.area <= target_area:\n",
    "        return geom  # Return original if no valid buffer can be made\n",
    "\n",
    "    buffer_dist = -0.1 * (geom.area ** 0.5)  # Start with a fraction of block size\n",
    "    iteration = 0\n",
    "\n",
    "    while iteration < max_iter:\n",
    "        buffered_geom = geom.buffer(buffer_dist)\n",
    "\n",
    "        if buffered_geom.is_empty:\n",
    "            return geom  # If buffering fails, return original geometry\n",
    "\n",
    "        new_area = buffered_geom.area\n",
    "        area_diff = abs(new_area - target_area)\n",
    "\n",
    "        if area_diff < tolerance:\n",
    "            return buffered_geom  # Found a good enough buffer\n",
    "\n",
    "        # Adjust buffer distance using a binary search-like approach\n",
    "        if new_area > target_area:\n",
    "            buffer_dist *= 1.1  # Increase buffer distance\n",
    "        else:\n",
    "            buffer_dist *= 0.9  # Decrease buffer distance\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    return buffered_geom  # Return best-found buffer\n",
    "\n",
    "\n",
    "\n",
    "def clip_group(df_group, buffer_type):\n",
    "    # Use the buffer from the specified column.\n",
    "    buffer_geom = df_group[buffer_type].iloc[0]\n",
    "    # Use the original building footprint column for intersection.\n",
    "    clipped = np.vectorize(lambda geom: geom.intersection(buffer_geom))(df_group['geometry'].values)\n",
    "    return pd.DataFrame({\n",
    "        'building_id': df_group['building_id'],\n",
    "        'block_id': df_group['block_id'],\n",
    "        'clipped_geometry': clipped\n",
    "    }, index=df_group.index)\n",
    "\n",
    "\n",
    "def clip_buildings_by_buffer(buildings_blocks_df, buffer_type):\n",
    "    # Copy the input and reset the index.\n",
    "    gdf = buildings_blocks_df.copy().reset_index()\n",
    "    # If an 'id' column exists, rename it; otherwise, create 'building_id' from the index.\n",
    "    if 'id' in gdf.columns:\n",
    "        gdf = gdf.rename(columns={'id': 'building_id'})\n",
    "    else:\n",
    "        gdf['building_id'] = gdf.index\n",
    "\n",
    "    if gdf.crs is None or not gdf.crs.is_projected:\n",
    "        raise ValueError(\"GeoDataFrame must have a projected CRS for efficient clipping.\")\n",
    "\n",
    "    # Group by block_id and apply the clipping function.\n",
    "    clipped_series = gdf.groupby('block_id', group_keys=False).apply(clip_group, buffer_type)\n",
    "    \n",
    "    # Create a GeoDataFrame from the result.\n",
    "    clipped_geo = gpd.GeoDataFrame(clipped_series, geometry='clipped_geometry', crs=buildings_blocks_df.crs)\n",
    "    \n",
    "    # Merge the clipped geometries back into the original GeoDataFrame.\n",
    "    gdf = gdf.merge(clipped_geo[['building_id', 'block_id', 'clipped_geometry']], \n",
    "                    on=['building_id', 'block_id'], how='left')\n",
    "    \n",
    "    gdf_clipped = gpd.GeoDataFrame(gdf.copy(), geometry='clipped_geometry', crs=buildings_blocks_df.crs)\n",
    "    gdf_clipped['clipped_area'] = gdf_clipped['clipped_geometry'].area\n",
    "    gdf_clipped['buffer_area'] = gdf_clipped[buffer_type].area\n",
    "    \n",
    "    # Aggregate the areas by block.\n",
    "    clipped_building_area = gdf_clipped.groupby('block_id')['clipped_area'].sum()\n",
    "    total_buffer_area = gdf_clipped.groupby('block_id')['buffer_area'].sum()\n",
    "    ratio = clipped_building_area / total_buffer_area\n",
    "    return ratio\n",
    "\n",
    "\n",
    "@delayed\n",
    "def compute_m6_m7_m8(city_name, YOUR_NAME, grid_size):\n",
    "    \"\"\"\n",
    "    Computes:\n",
    "    - M6: KL divergence (building orientation)\n",
    "    - M7: Average block width\n",
    "    - M8: Building density ratio (inner vs. outer buffer)\n",
    "    \"\"\"\n",
    "\n",
    "    epsilon = 0.001\n",
    "    paths = {\n",
    "        'grid': f'{GRIDS_PATH}/{city_name}/{city_name}_{str(grid_size)}m_grid.geoparquet',\n",
    "        'blocks': f'{BLOCKS_PATH}/{city_name}/{city_name}_blocks_{YOUR_NAME}.geoparquet',\n",
    "        'buildings_with_distances': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}_with_distances.geoparquet',\n",
    "        'buildings_with_distances_azimuths': f'{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}_with_distances_and_azimuths.geoparquet',\n",
    "        'buildings_to_blocks':f'{BLOCKS_PATH}/{city_name}/{city_name}_buildings_to_blocks_{YOUR_NAME}.geoparquet'\n",
    "    }\n",
    "\n",
    "    epsg = get_epsg(city_name).compute()\n",
    "    grid = load_dataset(paths['grid'], epsg=epsg)\n",
    "    blocks = load_dataset(paths['blocks'], epsg=epsg).persist()\n",
    "    buildings = load_dataset(paths['buildings_with_distances_azimuths'], epsg=epsg).persist()\n",
    "    buildings['azimuth'] = buildings['azimuth'].map_partitions(pd.to_numeric, errors='coerce')\n",
    "\n",
    "\n",
    "    if 'geom' in grid.columns:\n",
    "        grid = grid.drop(columns=['geom'])\n",
    "    \n",
    "    blocks['block_id'] = blocks.index\n",
    "    blocks['epsilon_buffer'] = blocks['geometry'].buffer(-(1.- epsilon) * blocks['max_radius'])\n",
    "    blocks['width_buffer'] = blocks['geometry'].buffer(-0.2 * blocks['max_radius'])\n",
    "\n",
    "    buildings_blocks = dgpd.sjoin(buildings, blocks, predicate='intersects').persist() #,how='right'\n",
    "    buildings_blocks = buildings_blocks[['block_id', 'geometry', 'epsilon_buffer','width_buffer','azimuth']]\n",
    "    buildings_blocks = buildings_blocks.set_index('block_id').repartition(npartitions=4)\n",
    "\n",
    "    block_grid_overlap = compute_block_grid_weights(blocks, grid)\n",
    "    block_grid_overlap = block_grid_overlap.compute()\n",
    "\n",
    "    # Metric 6\n",
    "    kl_df = compute_block_kl_metrics(buildings_blocks)\n",
    "    m6_grid = aggregate_m6(kl_df.compute(), block_grid_overlap)\n",
    " \n",
    "    # Metric 7\n",
    "    block_grid_overlap['weighted_max_radius'] = (\n",
    "        block_grid_overlap['max_radius'] * block_grid_overlap['area_weight']\n",
    "    )\n",
    "\n",
    "    grid_m7 = block_grid_overlap.groupby('grid_id').agg(\n",
    "        total_weighted_max_radius=('weighted_max_radius', 'sum'),\n",
    "        total_weight=('area_weight', 'sum')\n",
    "    )\n",
    "    grid_m7['m7'] = grid_m7['total_weighted_max_radius'] / grid_m7['total_weight']\n",
    "\n",
    "    # Metric 8\n",
    "    width_buffer_ratios = buildings_blocks.map_partitions(clip_buildings_by_buffer, buffer_type='width_buffer')\n",
    "    epsilon_buffer_ratios = buildings_blocks.map_partitions(clip_buildings_by_buffer, buffer_type='epsilon_buffer')\n",
    "    clipped_buildings_area_to_buffer_ratio = epsilon_buffer_ratios / width_buffer_ratios\n",
    "    clipped_buildings_area_to_buffer_ratio = clipped_buildings_area_to_buffer_ratio.replace([np.inf, -np.inf], np.nan).fillna(999)\n",
    "    ratio_df = clipped_buildings_area_to_buffer_ratio.to_frame(name='m8')\n",
    "    blocks_with_m8 = blocks.merge(ratio_df, left_on='block_id', right_index=True, how='left').compute()\n",
    "    block_grid_overlap = block_grid_overlap.merge(blocks_with_m8, how='left',left_on='block_id',right_index=True)\n",
    "    block_grid_overlap['weighted_m8'] = (\n",
    "        block_grid_overlap['m8'] * block_grid_overlap['area_weight']\n",
    "    )\n",
    "    grid_m8 = block_grid_overlap.groupby('grid_id').agg(\n",
    "        total_weighted_m8=('weighted_m8', 'sum'),\n",
    "        total_weight=('area_weight', 'sum')\n",
    "    )\n",
    "    grid_m8['m8'] = grid_m8['total_weighted_m8'] / grid_m8['total_weight']\n",
    "\n",
    "    # Merge all metrics\n",
    "    grid = grid.merge(m6_grid, left_index=True, right_index=True, how='left')\n",
    "    grid = grid.merge(grid_m7[['m7']], left_index=True, right_index=True, how='left')\n",
    "    grid = grid.merge(grid_m8[['m8']], left_index=True, right_index=True, how='left')\n",
    "\n",
    "    '''\n",
    "    # Fill NaNs\n",
    "    '''\n",
    "\n",
    "    grid['m6'] = grid['m6'].fillna(0)\n",
    "    grid['m7'] = grid['m7'].fillna(0)\n",
    "    grid['m8'] = grid['m8'].fillna(-999.)\n",
    "    \n",
    "    # Save Output\n",
    "    grid = grid.compute()  \n",
    "    path = f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{str(grid_size)}m_grid_{YOUR_NAME}_metrics_6_7_8.geoparquet'\n",
    "    grid.to_parquet(path)\n",
    "    \n",
    "    #path = f'{BLOCKS_PATH}/{city_name}/{city_name}_blocks_with_m8_{YOUR_NAME}.geoparquet'#f'{OUTPUT_PATH_RASTER}/{city_name}/{city_name}_{str(grid_size)}m_grid_{YOUR_NAME}_metrics_6_7_8.geoparquet'\n",
    "    #blocks_with_m8.to_parquet(path)\n",
    "    return  path\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "from dask import compute\n",
    "\n",
    "start_time = time.time()  # Start the timer\n",
    "\n",
    "cities = ['Nairobi']\n",
    "cities = [city.replace(' ', '_') for city in cities]\n",
    "\n",
    "tasks = [compute_m6_m7_m8(city_name, YOUR_NAME, grid_size) for city_name in cities]\n",
    "results = compute(*tasks)  \n",
    "\n",
    "end_time = time.time()  # End the timer\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Tasks completed in {elapsed_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subdivisions2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
