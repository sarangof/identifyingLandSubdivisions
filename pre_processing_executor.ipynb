{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ea1c429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- user config ----\n",
    "YOUR_NAME = 'sara'\n",
    "AWS_PROFILE = 'cities'\n",
    "\n",
    "# If you want to limit the run (debug):\n",
    "# LIMIT = 20\n",
    "LIMIT = None\n",
    "\n",
    "# Cities to ignore for now\n",
    "IGNORE_CITIES = [\n",
    "    'Ngo__Nigeria',\n",
    "    'Bugama__Nigeria',\n",
    "    'Mubi__Nigeria',\n",
    "    'San_Pedro_de_Macoris__Dominican_Republic',\n",
    "]\n",
    "\n",
    "# Dask tuning\n",
    "PARTITION_SIZE = 1   # 1 city per task (safer, better isolation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdba30b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "374d4150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- shared S3 paths (mirrors pre_processing.py) ----\n",
    "MAIN_PATH = 's3://wri-cities-sandbox/identifyingLandSubdivisions/data'\n",
    "INPUT_PATH = f'{MAIN_PATH}/input'\n",
    "CITY_INFO_PATH = f'{INPUT_PATH}/city_info'\n",
    "EXTENTS_PATH = f'{CITY_INFO_PATH}/extents'\n",
    "BUILDINGS_PATH = f'{INPUT_PATH}/buildings'\n",
    "BUILDINGS_DISTANCES_PATH = f'{INPUT_PATH}/buildings_with_distances'  # not used directly by current function\n",
    "ROADS_PATH = f'{INPUT_PATH}/roads'\n",
    "INTERSECTIONS_PATH = f'{INPUT_PATH}/intersections'\n",
    "NATURAL_FEATURES_PATH = f'{INPUT_PATH}/natural_features_and_railroads'\n",
    "GRIDS_PATH = f'{INPUT_PATH}/city_info/grids'\n",
    "SEARCH_BUFFER_PATH = f'{INPUT_PATH}/city_info/search_buffers'\n",
    "BLOCKS_PATH = f'{INPUT_PATH}/blocks'\n",
    "OUTPUT_PATH = f'{MAIN_PATH}/output'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdc19b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'GWWACT5M89F5X4NF',\n",
       "  'HostId': '3OQV2ulEq745mJroZawOFr1aNfIi1+gBvoqaNRWwgN5c+3oMGMo9wyhtIJguNnRJxIOB4bO0/VFZkkVJTtu1hidKujzlfVpTKHQlcr7LYmo=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': '3OQV2ulEq745mJroZawOFr1aNfIi1+gBvoqaNRWwgN5c+3oMGMo9wyhtIJguNnRJxIOB4bO0/VFZkkVJTtu1hidKujzlfVpTKHQlcr7LYmo=',\n",
       "   'x-amz-request-id': 'GWWACT5M89F5X4NF',\n",
       "   'date': 'Sat, 17 Jan 2026 23:10:51 GMT',\n",
       "   'content-type': 'application/xml',\n",
       "   'transfer-encoding': 'chunked',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'Buckets': [{'Name': 'aft-sandbox-540362055257',\n",
       "   'CreationDate': datetime.datetime(2022, 9, 13, 15, 12, 20, tzinfo=tzutc()),\n",
       "   'BucketArn': 'arn:aws:s3:::aft-sandbox-540362055257'},\n",
       "  {'Name': 'amplify-citiesindicatorsapi-dev-10508-deployment',\n",
       "   'CreationDate': datetime.datetime(2023, 8, 30, 5, 5, 13, tzinfo=tzutc()),\n",
       "   'BucketArn': 'arn:aws:s3:::amplify-citiesindicatorsapi-dev-10508-deployment'},\n",
       "  {'Name': 'cities-heat',\n",
       "   'CreationDate': datetime.datetime(2023, 6, 1, 13, 22, 1, tzinfo=tzutc()),\n",
       "   'BucketArn': 'arn:aws:s3:::cities-heat'},\n",
       "  {'Name': 'cities-test-sandbox',\n",
       "   'CreationDate': datetime.datetime(2025, 4, 18, 19, 10, 49, tzinfo=tzutc()),\n",
       "   'BucketArn': 'arn:aws:s3:::cities-test-sandbox'},\n",
       "  {'Name': 'do-not-delete-ssm-diagnosis-540362055257-us-east-1-hac7u',\n",
       "   'CreationDate': datetime.datetime(2025, 7, 7, 9, 3, 18, tzinfo=tzutc()),\n",
       "   'BucketArn': 'arn:aws:s3:::do-not-delete-ssm-diagnosis-540362055257-us-east-1-hac7u'},\n",
       "  {'Name': 'era5-brazil',\n",
       "   'CreationDate': datetime.datetime(2025, 2, 15, 19, 51, 14, tzinfo=tzutc()),\n",
       "   'BucketArn': 'arn:aws:s3:::era5-brazil'},\n",
       "  {'Name': 'wri-cities-athena-us-east-1',\n",
       "   'CreationDate': datetime.datetime(2025, 7, 17, 13, 37, 24, tzinfo=tzutc()),\n",
       "   'BucketArn': 'arn:aws:s3:::wri-cities-athena-us-east-1'},\n",
       "  {'Name': 'wri-cities-athena-us-west-2',\n",
       "   'CreationDate': datetime.datetime(2024, 1, 12, 18, 45, 11, tzinfo=tzutc()),\n",
       "   'BucketArn': 'arn:aws:s3:::wri-cities-athena-us-west-2'},\n",
       "  {'Name': 'wri-cities-aws',\n",
       "   'CreationDate': datetime.datetime(2025, 7, 15, 18, 56, 37, tzinfo=tzutc()),\n",
       "   'BucketArn': 'arn:aws:s3:::wri-cities-aws'},\n",
       "  {'Name': 'wri-cities-climate-hazards',\n",
       "   'CreationDate': datetime.datetime(2024, 1, 3, 16, 57, 31, tzinfo=tzutc()),\n",
       "   'BucketArn': 'arn:aws:s3:::wri-cities-climate-hazards'},\n",
       "  {'Name': 'wri-cities-data-api',\n",
       "   'CreationDate': datetime.datetime(2024, 7, 16, 8, 53, 31, tzinfo=tzutc()),\n",
       "   'BucketArn': 'arn:aws:s3:::wri-cities-data-api'},\n",
       "  {'Name': 'wri-cities-heat',\n",
       "   'CreationDate': datetime.datetime(2024, 3, 25, 15, 46, 55, tzinfo=tzutc()),\n",
       "   'BucketArn': 'arn:aws:s3:::wri-cities-heat'},\n",
       "  {'Name': 'wri-cities-indicators',\n",
       "   'CreationDate': datetime.datetime(2024, 5, 13, 15, 50, 58, tzinfo=tzutc()),\n",
       "   'BucketArn': 'arn:aws:s3:::wri-cities-indicators'},\n",
       "  {'Name': 'wri-cities-sandbox',\n",
       "   'CreationDate': datetime.datetime(2024, 7, 27, 0, 51, 38, tzinfo=tzutc()),\n",
       "   'BucketArn': 'arn:aws:s3:::wri-cities-sandbox'},\n",
       "  {'Name': 'wri-cities-tcm',\n",
       "   'CreationDate': datetime.datetime(2025, 8, 29, 19, 35, 51, tzinfo=tzutc()),\n",
       "   'BucketArn': 'arn:aws:s3:::wri-cities-tcm'}],\n",
       " 'Owner': {'ID': 'df12253943982d72f60594f06c2cacf9a1ee3a9e738c1649c9fb96e5127f1a5c'}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- check S3 connection using AWS_PROFILE ----\n",
    "import boto3, os\n",
    "\n",
    "session = boto3.Session(profile_name=AWS_PROFILE)\n",
    "s3 = session.client('s3')\n",
    "\n",
    "# Export profile so s3fs/cloudpathlib inherit it\n",
    "os.environ['AWS_PROFILE'] = AWS_PROFILE\n",
    "\n",
    "s3.list_buckets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cea343c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-17 18:10:50,811][INFO    ][coiled] Fetching latest package priorities...\n",
      "[2026-01-17 18:10:50,813][INFO    ][coiled.package_sync] Resolving your local subdivisions2 Python environment...\n",
      "[2026-01-17 18:10:51,232][INFO    ][coiled.package_sync] Scanning 446 conda packages...\n",
      "[2026-01-17 18:10:51,239][INFO    ][coiled.package_sync] Scanning 261 python packages...\n",
      "[2026-01-17 18:10:51,991][INFO    ][coiled.software_utils] No username or password found for https://conda.anaconda.org/conda-forge\n",
      "[2026-01-17 18:10:52,516][INFO    ][coiled] Running pip check...\n",
      "[2026-01-17 18:10:52,962][INFO    ][coiled] Validating environment...\n",
      "[2026-01-17 18:10:56,685][INFO    ][coiled] Creating wheel for ~/Documents/Identifying Land Subdivisions/identifyingLandSubdivisions...\n",
      "[2026-01-17 18:10:56,815][INFO    ][coiled] Creating wheel for /opt/spark-2.2.0/python...\n",
      "[2026-01-17 18:10:56,981][WARNING ][coiled.package_sync] Package - debugpy, debugpy~=1.8.16 has no install candidate for Python 3.12 linux-aarch64 on conda-forge\n",
      "[2026-01-17 18:10:56,982][INFO    ][coiled.package_sync] Package - aiobotocore, Pip check had the following issues that need resolving: \n",
      "aiobotocore 2.24.1 has requirement botocore<1.39.12,>=1.39.9, but you have botocore 1.40.16.\n",
      "[2026-01-17 18:10:56,982][INFO    ][coiled] Uploading coiled_local_identifyingLandSubdivisions...\n",
      "[2026-01-17 18:10:57,957][INFO    ][coiled] Uploading coiled_local_python...\n",
      "[2026-01-17 18:10:58,881][INFO    ][coiled] Requesting package sync build...\n",
      "[2026-01-17 18:10:59,727][INFO    ][coiled] Creating Cluster (name: ils-preproc-sara, https://cloud.coiled.io/clusters/1384572 ). This usually takes 1-2 minutes...\n",
      "/opt/anaconda3/envs/subdivisions2/lib/python3.12/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Dask client. Dashboard: https://cluster-vhdph.dask.host/YwcDGTBBc9-0vRBT/status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/subdivisions2/lib/python3.12/site-packages/distributed/client.py:1590: VersionMismatchWarning: Mismatched versions found\n",
      "\n",
      "+---------+--------+-----------+---------+\n",
      "| Package | Client | Scheduler | Workers |\n",
      "+---------+--------+-----------+---------+\n",
      "| lz4     | 4.4.4  | 4.4.5     | 4.4.5   |\n",
      "+---------+--------+-----------+---------+\n",
      "  warnings.warn(version_module.VersionMismatchWarning(msg[0][\"warning\"]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/subdivisions2/lib/python3.12/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "/opt/anaconda3/envs/subdivisions2/lib/python3.12/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "/opt/anaconda3/envs/subdivisions2/lib/python3.12/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "/opt/anaconda3/envs/subdivisions2/lib/python3.12/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "/opt/anaconda3/envs/subdivisions2/lib/python3.12/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "/opt/anaconda3/envs/subdivisions2/lib/python3.12/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "2026-01-17 21:42:05,061 - distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client\n",
      "/opt/anaconda3/envs/subdivisions2/lib/python3.12/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "# ---- start Coiled cluster (adjust n_workers / instance type as needed) ----\n",
    "import coiled\n",
    "\n",
    "cluster = coiled.Cluster(\n",
    "    workspace='wri-cities-data',\n",
    "    name=f'ils-preproc-{YOUR_NAME}',\n",
    "    region='us-west-2',\n",
    "    arm=True,\n",
    "    worker_vm_types='r8g.xlarge',\n",
    "    spot_policy='spot',\n",
    "    n_workers=8,\n",
    "    package_sync_ignore=['pyspark', 'pypandoc'],\n",
    "    worker_options={'nthreads': 1},\n",
    ")\n",
    "client = cluster.get_client()\n",
    "print(f'Started Dask client. Dashboard: {client.dashboard_link}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f4e04d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roads city dirs: 1234\n",
      "cities to run: 1234\n"
     ]
    }
   ],
   "source": [
    "# ---- build city list from S3 (roads folders) ----\n",
    "import s3fs\n",
    "\n",
    "fs = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "road_city_dirs = fs.ls(ROADS_PATH)\n",
    "all_cities = sorted([p.split('/')[-1] for p in road_city_dirs])\n",
    "all_cities = [c for c in all_cities if c and c not in ['.DS_Store']]\n",
    "\n",
    "cities = [c for c in all_cities if c not in set(IGNORE_CITIES)]\n",
    "\n",
    "print('roads city dirs:', len(all_cities))\n",
    "print('cities to run:', len(cities))\n",
    "\n",
    "if LIMIT:\n",
    "    cities = cities[:LIMIT]\n",
    "    print('LIMIT applied ->', len(cities))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff881fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nn/3mdkp6sx1n3d955f3wqgdbb00000gn/T/ipykernel_85841/1831175662.py:11: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  RUN_ID = datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')\n"
     ]
    }
   ],
   "source": [
    "# ---- logging + runner (inspired by gather_data_executor) ----\n",
    "import os, time, socket, traceback\n",
    "from datetime import datetime, timezone\n",
    "from cloudpathlib import S3Path\n",
    "\n",
    "import dask\n",
    "from dask import compute\n",
    "\n",
    "from pre_processing import calculate_building_distances_to_roads, produce_azimuths\n",
    "\n",
    "RUN_ID = datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')\n",
    "LOGS_S3_DIR = f\"{OUTPUT_PATH}/logs/pre_processing/{RUN_ID}\"\n",
    "SUMMARY_S3_PATH = f\"{LOGS_S3_DIR}/summary.csv\"\n",
    "LOCAL_LOG_DIR = f\"/tmp/pre_processing_logs/{RUN_ID}\"\n",
    "\n",
    "\n",
    "def utc_now():\n",
    "    return datetime.now(timezone.utc).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "\n",
    "def append_log(path, msg):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, 'a', encoding='utf-8') as f:\n",
    "        f.write(msg)\n",
    "\n",
    "\n",
    "def s3_exists(uri: str) -> bool:\n",
    "    # cheap existence check for s3://...\n",
    "    try:\n",
    "        return fs.exists(uri.replace('s3://', ''))\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def required_inputs_exist(city_name: str):\n",
    "    # For building distances: needs buildings + roads\n",
    "    buildings_path = f\"{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}.geoparquet\"\n",
    "    roads_path = f\"{ROADS_PATH}/{city_name}/{city_name}_OSM_roads.geoparquet\"\n",
    "\n",
    "    # For azimuths: unknown; assume it will handle its own reads, but roads+buildings are usually required\n",
    "    ok_buildings = s3_exists(buildings_path)\n",
    "    ok_roads = s3_exists(roads_path)\n",
    "\n",
    "    missing = []\n",
    "    if not ok_buildings:\n",
    "        missing.append('buildings')\n",
    "    if not ok_roads:\n",
    "        missing.append('roads')\n",
    "\n",
    "    return (len(missing) == 0), missing, {'buildings': buildings_path, 'roads': roads_path}\n",
    "\n",
    "\n",
    "def dist_output_path(city_name: str):\n",
    "    # matches calculate_building_distances_to_roads() naming\n",
    "    p = f\"{BUILDINGS_PATH}/{city_name}/Overture_building_{city_name}.geoparquet\"\n",
    "    return p.replace('.geoparquet', '_with_distances.geoparquet')\n",
    "\n",
    "\n",
    "def run_city_preproc(city_name: str):\n",
    "    host = socket.gethostname()\n",
    "    t0 = time.time()\n",
    "    local_log = f\"{LOCAL_LOG_DIR}/{city_name}.log\"\n",
    "\n",
    "    ok, missing, paths = required_inputs_exist(city_name)\n",
    "    if not ok:\n",
    "        return {\n",
    "            'city': city_name,\n",
    "            'status': 'missing_inputs',\n",
    "            'missing': ','.join(missing),\n",
    "            'ts_start': utc_now(),\n",
    "            'ts_end': utc_now(),\n",
    "            'secs': 0.0,\n",
    "            'host': host,\n",
    "            'distances_path': '',\n",
    "            'azimuths_result': '',\n",
    "            'log_s3': '',\n",
    "        }\n",
    "\n",
    "    # --- skip distances if already exists ---\n",
    "    out_dist = dist_output_path(city_name)\n",
    "    do_dist = not s3_exists(out_dist)\n",
    "\n",
    "    append_log(\n",
    "        local_log,\n",
    "        f\"[{utc_now()}] city={city_name} host={host} start\\n\"\n",
    "    )\n",
    "\n",
    "    append_log(\n",
    "        local_log,\n",
    "        f\"[{utc_now()}] inputs: {paths}\\n\"\n",
    "    )\n",
    "\n",
    "    append_log(\n",
    "        local_log,\n",
    "        f\"[{utc_now()}] distances_out: {out_dist} (will_run={do_dist})\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "    status = 'ok'\n",
    "    err = ''\n",
    "    distances_res = ''\n",
    "    azimuths_res = ''\n",
    "\n",
    "    try:\n",
    "        tasks = []\n",
    "        if do_dist:\n",
    "            tasks.append(calculate_building_distances_to_roads(city_name))\n",
    "        else:\n",
    "            distances_res = out_dist\n",
    "\n",
    "        # Always run azimuths (we don't know a reliable output path to skip safely)\n",
    "        tasks.append(produce_azimuths(city_name, YOUR_NAME))\n",
    "\n",
    "        # --- Step 1: distances (must finish before azimuths reads) ---\n",
    "        if do_dist:\n",
    "            distances_res = compute(calculate_building_distances_to_roads(city_name))[0]\n",
    "\n",
    "            # hard check: ensure file exists on S3 before moving on\n",
    "            if not s3_exists(out_dist):\n",
    "                raise FileNotFoundError(f\"distances output not found after compute: {out_dist}\")\n",
    "        else:\n",
    "            # if we skipped, still ensure it truly exists (otherwise azimuths will fail)\n",
    "            if not s3_exists(out_dist):\n",
    "                raise FileNotFoundError(f\"distances file expected but missing: {out_dist}\")\n",
    "            distances_res = out_dist\n",
    "\n",
    "        # --- Step 2: azimuths (reads buildings_with_distances) ---\n",
    "        azimuths_res = compute(produce_azimuths(city_name, YOUR_NAME))[0]\n",
    "\n",
    "\n",
    "        append_log(\n",
    "            local_log,\n",
    "            f\"[{utc_now()}] distances_res={distances_res}\\n\"\n",
    "        )\n",
    "\n",
    "        append_log(\n",
    "            local_log,\n",
    "            f\"[{utc_now()}] azimuths_res={azimuths_res}\\n\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        status = 'error'\n",
    "        err = repr(e)\n",
    "        append_log(\n",
    "            local_log,\n",
    "            f\"[{utc_now()}] ERROR: {err}\\n\"\n",
    "        )\n",
    "\n",
    "        append_log(\n",
    "            local_log,\n",
    "            traceback.format_exc() + \"\\n\"\n",
    "        )\n",
    "\n",
    "    # upload log\n",
    "    log_s3 = f\"{LOGS_S3_DIR}/city_logs/{city_name}.log\"\n",
    "    try:\n",
    "        S3Path(log_s3).parent.mkdir(parents=True, exist_ok=True)\n",
    "        S3Path(log_s3).upload_from(local_log)\n",
    "    except Exception as e:\n",
    "        append_log(local_log, f\"[{utc_now()}] WARN: failed to upload log to s3: {repr(e)}\")\n",
    "\n",
    "    secs = time.time() - t0\n",
    "    rec = {\n",
    "        'city': city_name,\n",
    "        'status': status,\n",
    "        'error': err,\n",
    "        'missing': '',\n",
    "        'ts_start': '',\n",
    "        'ts_end': utc_now(),\n",
    "        'secs': round(secs, 3),\n",
    "        'host': host,\n",
    "        'distances_path': str(distances_res),\n",
    "        'azimuths_result': str(azimuths_res),\n",
    "        'log_s3': log_s3,\n",
    "        'distances_skipped': (not do_dist),\n",
    "    }\n",
    "    return rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c79e5b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status\n",
      "ok    1234\n",
      "Name: count, dtype: int64\n",
      "✅ Summary written to: s3://wri-cities-sandbox/identifyingLandSubdivisions/data/output/logs/pre_processing/20260117T231251Z/summary.csv\n"
     ]
    }
   ],
   "source": [
    "# ---- execute across cities with dask.bag ----\n",
    "import dask.bag as db\n",
    "import pandas as pd\n",
    "\n",
    "bag = db.from_sequence(cities, partition_size=PARTITION_SIZE)\n",
    "records = bag.map(run_city_preproc).compute()\n",
    "\n",
    "summary = pd.DataFrame(records)\n",
    "print(summary['status'].value_counts(dropna=False))\n",
    "\n",
    "local_summary = f\"/tmp/summary_pre_processing_{RUN_ID}.csv\"\n",
    "summary.to_csv(local_summary, index=False, sep=';')\n",
    "\n",
    "S3Path(LOGS_S3_DIR).mkdir(parents=True, exist_ok=True)\n",
    "S3Path(SUMMARY_S3_PATH).upload_from(local_summary)\n",
    "\n",
    "print('✅ Summary written to:', SUMMARY_S3_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a2960ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n problematic: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>status</th>\n",
       "      <th>error</th>\n",
       "      <th>missing</th>\n",
       "      <th>ts_start</th>\n",
       "      <th>ts_end</th>\n",
       "      <th>secs</th>\n",
       "      <th>host</th>\n",
       "      <th>distances_path</th>\n",
       "      <th>azimuths_result</th>\n",
       "      <th>log_s3</th>\n",
       "      <th>distances_skipped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [city, status, error, missing, ts_start, ts_end, secs, host, distances_path, azimuths_result, log_s3, distances_skipped]\n",
       "Index: []"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- quick peek at failures / missing ----\n",
    "errs = summary[summary['status'].isin(['error', 'missing_inputs'])].copy()\n",
    "print('n problematic:', len(errs))\n",
    "errs.head(25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bb47fc",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- City list is built from existing **ROADS** folders on S3 and skips the 4 ignore cities.\n",
    "- **Building distances** are skipped if the `*_with_distances.geoparquet` already exists.\n",
    "- **Azimuths** are always run (no reliable output path to skip without seeing `calculate_azimuths` implementation).\n",
    "- Logs + summary CSV are uploaded to: `s3://.../output/logs/pre_processing/<RUN_ID>/`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subdivisions2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
